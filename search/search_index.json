{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Google Cloud Documentation","text":""},{"location":"test/","title":"Generative AI beginner's guide","text":"<p>This beginner's guide introduces you to the core technologies of generative AI and explains how they fit together to power chatbots and applications. Generative AI (also known as genAI or gen AI) is a field of machine learning (ML) that develops and uses ML models for generating new content.</p> <p>Generative AI models are often called large language models (LLMs) because of their large size and ability to understand and generate natural language. However, depending on the data that the models are trained on, these models can understand and generate content from multiple modalities, including text, images, videos, and audio. Models that work with multiple modalities of data are called multimodal models.</p> <p>Google provides the Gemini family of generative AI models designed for multimodal use cases, capable of processing information from multiple modalities, including images, videos, and text.</p>"},{"location":"test/#core-components-of-the-generative-ai-workflow","title":"\ud83d\udcda Core components of the generative AI workflow","text":"<p>For generative AI models to generate useful content in real-world applications, they rely on several key capabilities working together. These include the ability to be customized for new tasks, access external information to provide relevant responses, and filter harmful content to ensure safety.</p> <p>The following diagram shows how these different capabilities work together in a typical generative AI workflow.</p> <pre><code>flowchart LR\n    subgraph \"Generative AI Workflow\"\n        A[1\\. Prompt] --&gt; B[2\\. Foundation Model]\n        B --&gt; C[3\\. Model Customization]\n        C --&gt; D[4\\. Access External Information]\n        D --&gt; E[5\\. Citation Check]\n        E --&gt; F[6\\. Responsible AI &amp; Safety]\n        F --&gt; G[7\\. Response]\n    end</code></pre> 1. Prompt <p></p> <p>The generative AI workflow typically starts with a prompt. A prompt is a natural language request sent to a generative AI model to elicit a response. Depending on the model, a prompt can contain text, images, videos, audio, documents, and other modalities, or even a combination of multiple modalities (multimodal).</p> <p>Creating a prompt to get the desired response from the model is a practice called prompt design. While prompt design is a process of trial and error, there are principles and strategies you can use to guide the model toward the desired behavior. Vertex AI Studio offers a prompt management tool to help you manage your prompts.</p> 2. Foundation models <p></p> <p>Prompts are sent to a generative AI model for response generation. Vertex AI has a variety of generative AI foundation models that are accessible through a managed API, including the following:</p> <ul> <li>Gemini API: For advanced reasoning, multiturn chat, code generation, and multimodal prompts.</li> <li>Imagen API: For image generation, image editing, and visual captioning.</li> <li>MedLM: For medical question answering and summarization. (Deprecated)</li> </ul> <p>The models differ in size, modality, and cost. You can explore Google models, as well as open models and models from Google partners, in Model Garden.</p> 3. Model customization <p></p> <p>You can customize the default behavior of Google's foundation models so that they consistently generate the desired results without requiring complex prompts. This customization process is called model tuning. Model tuning helps you reduce the cost and latency of your requests by allowing you to simplify your prompts.</p> <p>Vertex AI also offers model evaluation tools to help you evaluate the performance of your tuned model. After your tuned model is production-ready, you can deploy it to an endpoint and monitor its performance, similar to standard MLOps workflows.</p> 4. Access external information <p></p> <p>Vertex AI offers multiple ways to give the model access to external APIs and real-time information.</p> Feature Description Grounding Connects model responses to a source of truth, such as your own data or web search, helping to reduce hallucinations. RAG Connects models to external knowledge sources, such as documents and databases, to generate more accurate and informative responses. Function calling Lets the model interact with external APIs to get real-time information and perform real-world tasks. 5. Citation check <p></p> <p>After the response is generated, Vertex AI checks whether citations need to be included. If a significant amount of the text in the response comes from a particular source, that source is added to the citation metadata in the response.</p> 6. Responsible AI and safety <p></p> <p>The final layer of checks that the prompt and response go through is the safety filters. Vertex AI checks both the prompt and the response for content that belongs to a safety category. If the threshold is exceeded for one or more categories, the response is blocked, and Vertex AI returns a fallback response.</p> 7. Response <p></p> <p>If the prompt and response pass the safety filter checks, the response is returned. Typically, the response is returned all at once. However, with Vertex AI you can also receive responses progressively as they are generated by enabling streaming.</p>"},{"location":"test/#get-started","title":"\u2699\ufe0f Get started","text":"<p>To get started with generative AI on Vertex AI, you can use the Vertex AI SDK for programmatic access or experiment directly in the Google Cloud console with Vertex AI Studio. Choose the path that best suits your needs.</p> Use the Vertex AI SDKUse the Google Cloud console <ul> <li>Generate text using the Vertex AI Gemini API: Use the SDK to send requests to the Vertex AI Gemini API.</li> </ul> <ul> <li>Send prompts to Gemini using the Vertex AI Studio Prompt Gallery: Test prompts in the console with no setup required.</li> <li>Generate an image and verify its watermark using Imagen: Create a watermarked image using Imagen on Vertex AI in the console.</li> </ul>"},{"location":"api_reference/generate_content/","title":"<code>generate_content()</code> API","text":"<p>The <code>generate_content()</code> method in the Google Gen AI SDK provides a flexible interface for interacting with generative models, enabling the creation of diverse content based on various inputs. This API is a fundamental component for building applications that leverage the power of large language models, supporting both standard text generation and advanced multimodal capabilities.</p>"},{"location":"api_reference/generate_content/#choosing-a-method","title":"\u2699\ufe0f Choosing a Method","text":"<p>The <code>generate_content</code> API is available in four variations depending on your needs for execution mode (synchronous vs. asynchronous) and response handling (unary vs. streaming). Use the tables below to select the best combination for your application.</p> <p>Execution Mode: Synchronous vs. Asynchronous</p> Feature Synchronous (Sync) Asynchronous (Async) Execution Blocks the program's execution until the operation is complete. Does not block; returns an awaitable object that resolves when the operation is complete. Use Case Ideal for simple scripts, sequential tasks, or when the result is needed immediately before proceeding. Best for I/O-bound applications like web servers or GUIs, and for running multiple API calls concurrently. Syntax A standard function call. Requires <code>async</code> and <code>await</code> keywords and an <code>asyncio</code> event loop. <p>Response Handling: Unary vs. Streaming</p> Feature Unary (Standard) Streaming Response The full response is returned in a single object after the model finishes generating. The response is returned in chunks as they are generated by the model. Use Case Suitable for tasks where the full context is needed at once (e.g., summarization, classification, short answers). Excellent for real-time applications (e.g., chatbots), generating long-form content, or displaying a progress indicator to the user. Latency Higher time-to-first-token, as you must wait for the entire response. Lower time-to-first-token, as you start receiving data almost immediately."},{"location":"api_reference/generate_content/#overview","title":"\ud83d\udcda Overview","text":"<p>At its core, the <code>generate_content()</code> method sends a request to a generative model with a set of inputs (<code>contents</code>) and optional configuration, receiving a generated response. The API is designed to accommodate different use cases by offering synchronous and asynchronous execution, as well as streaming responses for real-time interactions. It integrates with both the Vertex AI API and the Gemini API, providing a unified interface while handling underlying service-specific details.</p>"},{"location":"api_reference/generate_content/#key-concepts","title":"\ud83d\udcda Key Concepts","text":"<ul> <li>Generative Models: The API interacts with large language models, such as the Gemini family of models, which are capable of understanding and generating human-like text, code, images, and other modalities.</li> <li>Multimodal Input: Models like Gemini support prompts that combine different types of data. You can include text, images, or file references (via URI) in a single request to generate contextually rich responses.</li> <li>Content and Parts: Input and output are structured using <code>Content</code> objects, which are composed of one or more <code>Part</code> objects. A <code>Part</code> can hold text (<code>text</code>), inline data (like base64-encoded images, <code>inline_data</code>), or references to file data (<code>file_data</code>).</li> <li>Configuration (<code>GenerateContentConfig</code>): The <code>config</code> parameter allows you to fine-tune the model's behavior. This includes controlling randomness (<code>temperature</code>, <code>top_p</code>), managing response length (<code>max_output_tokens</code>), implementing safety measures (<code>safety_settings</code>), and enabling function calling (<code>tools</code>).</li> <li>Streaming: For scenarios where receiving the model's response incrementally is beneficial (e.g., chatbots), the <code>stream=True</code> parameter provides a streaming interface, yielding chunks of the response as they are generated.</li> <li>Automatic Function Calling (AFC): The SDK includes experimental support for Automatic Function Calling, where the model can suggest function calls based on the user's prompt and provided tool definitions.</li> </ul>"},{"location":"api_reference/generate_content/#api-reference","title":"\ud83d\udd17 API Reference","text":"<p>The <code>generate_content()</code> method is available in both synchronous (<code>google.genai.models.GenerativeModel</code>) and asynchronous (<code>google.genai.models.AsyncGenerativeModel</code>) versions.</p> SynchronousAsynchronousSynchronous StreamAsynchronous Stream <p>Makes an API request and waits for the full response.</p> <pre><code>generate_content(\n    contents: Union[types.ContentListUnion, types.ContentListUnionDict],\n    *,\n    stream: bool = False,\n    generation_config: Optional[types.GenerationConfigOrDict] = None,\n    safety_settings: Optional[types.SafetySettingOptions] = None,\n    tools: Optional[types.ToolListOrDict] = None,\n    tool_config: Optional[types.ToolConfigOrDict] = None,\n    request_options: Optional[types.RequestOptions] = None,\n) -&gt; Union[types.GenerateContentResponse, Iterable[types.GenerateContentResponse]]\n</code></pre> <p>An asynchronous version of <code>generate_content()</code>. Requires <code>await</code>.</p> <pre><code>async generate_content(\n    contents: Union[types.ContentListUnion, types.ContentListUnionDict],\n    *,\n    stream: bool = False,\n    generation_config: Optional[types.GenerationConfigOrDict] = None,\n    safety_settings: Optional[types.SafetySettingOptions] = None,\n    tools: Optional[types.ToolListOrDict] = None,\n    tool_config: Optional[types.ToolConfigOrDict] = None,\n    request_options: Optional[types.RequestOptions] = None,\n) -&gt; Union[types.GenerateContentResponse, AsyncIterable[types.GenerateContentResponse]]\n</code></pre> <p>Makes an API request and yields the response in chunks. Set <code>stream=True</code>.</p> <pre><code>generate_content(\n    contents: Union[types.ContentListUnion, types.ContentListUnionDict],\n    *,\n    stream: True,\n    # ... other parameters\n) -&gt; Iterable[types.GenerateContentResponse]\n</code></pre> <p>An asynchronous version of <code>generate_content_stream()</code>. Requires <code>await</code> and <code>async for</code>. Set <code>stream=True</code>.</p> <pre><code>async generate_content(\n    contents: Union[types.ContentListUnion, types.ContentListUnionDict],\n    *,\n    stream: True,\n    # ... other parameters\n) -&gt; AsyncIterable[types.GenerateContentResponse]\n</code></pre> <p>Parameters</p> <ul> <li><code>contents</code> (<code>Union[types.ContentListUnion, types.ContentListUnionDict]</code>): The input provided to the model. This can be a single string, a list of strings, a <code>types.Content</code> object, or a list of <code>types.Content</code> objects.</li> <li><code>stream</code> (<code>bool</code>): If <code>True</code>, the response is returned as an iterable of chunks as they are generated. If <code>False</code> (default), the full response is returned at once.</li> <li><code>generation_config</code> (<code>Optional[types.GenerationConfigOrDict]</code>): Configuration for the generation process.<ul> <li><code>temperature</code> (<code>Optional[float]</code>): Controls the randomness of the output (0.0 to 1.0).</li> <li><code>top_p</code> (<code>Optional[float]</code>): Nucleus sampling parameter.</li> <li><code>top_k</code> (<code>Optional[int]</code>): Top-k sampling parameter.</li> <li><code>candidate_count</code> (<code>Optional[int]</code>): Number of response candidates to generate.</li> <li><code>max_output_tokens</code> (<code>Optional[int]</code>): Maximum tokens in the generated response.</li> <li><code>stop_sequences</code> (<code>Optional[list[str]]</code>): Strings that halt generation.</li> <li><code>response_mime_type</code> (<code>Optional[str]</code>): Specifies the desired output format (e.g., <code>'application/json'</code>).</li> </ul> </li> <li><code>safety_settings</code> (<code>Optional[list[types.SafetySetting]]</code>): Configures safety filtering thresholds.</li> <li><code>tools</code> (<code>Optional[list[types.Tool]]</code>): Defines tools the model can use for function calling.</li> <li><code>tool_config</code> (<code>Optional[types.ToolConfig]</code>): Configures tool usage behavior.</li> </ul> <p>Return Type</p> <ul> <li><code>types.GenerateContentResponse</code>: If <code>stream=False</code>, a single response object.</li> <li><code>Iterable[types.GenerateContentResponse]</code>: If <code>stream=True</code> in a sync call, an iterable of response chunks.</li> <li><code>AsyncIterable[types.GenerateContentResponse]</code>: If <code>stream=True</code> in an async call, an async iterable of response chunks.</li> </ul> <p><code>GenerateContentResponse</code> Attributes</p> <ul> <li><code>candidates</code> (<code>list[types.Candidate]</code>): A list of potential responses from the model. Each <code>Candidate</code> includes the generated <code>content</code>, <code>finish_reason</code>, and <code>safety_ratings</code>.</li> <li><code>text</code> (<code>str</code>): A convenience property to access the text content of the first <code>Part</code> in the first <code>Candidate</code>.</li> <li><code>prompt_feedback</code> (<code>Optional[types.PromptFeedback]</code>): Information about the input prompt, such as safety blocking reasons.</li> <li><code>usage_metadata</code> (<code>Optional[types.UsageMetadata]</code>): Details on the token count for the request and response.</li> </ul> <p>Potential Errors</p> <ul> <li><code>ValueError</code>: Raised for invalid input values or configurations.</li> <li><code>google.api_core.exceptions.GoogleAPICallError</code>: Raised for issues with the backend API call, such as permission errors or invalid arguments.</li> <li><code>BlockedPromptException</code> / <code>FinishReason.SAFETY</code>: Indicates the prompt or response was blocked by safety settings.</li> </ul>"},{"location":"api_reference/generate_content/#code-samples","title":"\ud83d\udcbb Code Samples","text":"<p>Before running the examples, ensure you have configured the SDK. For the Gemini API, set your <code>GOOGLE_API_KEY</code>. For Vertex AI, initialize the client with your project and location.</p> <pre><code>import google.generativeai as genai\nimport os\n\n# For Gemini API (requires GOOGLE_API_KEY environment variable)\n# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\ngenai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n\n# For Vertex AI\n# import vertexai\n# vertexai.init(project=\"your-project-id\", location=\"us-central1\")\n</code></pre> Introductory Example: Text-to-Text Generation (Synchronous) <p>Generate a simple text response from a text prompt using a unary call.</p> <pre><code>model = genai.GenerativeModel('gemini-1.5-flash')\nresponse = model.generate_content(\"Tell me a short story about a brave knight.\")\n\nprint(\"Introductory Example (Sync):\")\nprint(response.text)\n</code></pre> Intermediate Example: Multimodal Input (Text + Image) <p>Describe the content of an image. This example uses an image file, but you can also provide image data via URIs (for Vertex AI) or raw bytes.</p> <pre><code>import PIL.Image\n\nimg = PIL.Image.open('scones.jpeg') # Make sure you have an image file named 'scones.jpeg'\nmodel = genai.GenerativeModel('gemini-1.5-flash')\n\nresponse = model.generate_content([\"What is shown in this image?\", img])\nprint(\"Intermediate Example (Multimodal Sync):\")\nprint(response.text)\n</code></pre> Advanced Example: Text Generation with Configuration (Asynchronous) <p>Generate text with specific parameters like <code>temperature</code> and <code>max_output_tokens</code> using the asynchronous method.</p> <pre><code>import asyncio\n\nasync def advanced_example_async():\n    model = genai.GenerativeModel('gemini-1.5-flash')\n    response = await model.generate_content_async(\n        'Write a poem about the ocean.',\n        generation_config=genai.types.GenerationConfig(\n            temperature=0.8,\n            max_output_tokens=100\n        )\n    )\n    print(\"Advanced Example (Async with Config):\")\n    print(response.text)\n\n# To run this, use: asyncio.run(advanced_example_async())\n</code></pre> Streaming Example: Synchronous Streaming <p>Receive the model's response in chunks as it's being generated.</p> <pre><code>model = genai.GenerativeModel('gemini-1.5-flash')\nresponse_stream = model.generate_content(\n    'Explain the concept of quantum entanglement in simple terms.',\n    stream=True\n)\n\nprint(\"Streaming Example (Sync):\")\nfor chunk in response_stream:\n    print(chunk.text, end='', flush=True)\nprint() # Newline after streaming\n</code></pre> Streaming Example: Asynchronous Streaming <p>Receive the model's response in chunks using the asynchronous streaming method.</p> <pre><code>import asyncio\n\nasync def streaming_example_async():\n    model = genai.GenerativeModel('gemini-1.5-flash')\n    response_stream = await model.generate_content_async(\n        'Explain the concept of black holes in simple terms.',\n        stream=True\n    )\n\n    print(\"Streaming Example (Async):\")\n    async for chunk in response_stream:\n        print(chunk.text, end='', flush=True)\n    print() # Newline after streaming\n\n# To run this, use: asyncio.run(streaming_example_async())\n</code></pre>"},{"location":"api_reference/generate_content/#use-cases","title":"\ud83d\udcda Use Cases","text":"<ul> <li>Text Generation: Create articles, stories, poems, code, scripts, and emails.</li> <li>Summarization: Condense long documents or articles into concise summaries.</li> <li>Question Answering: Extract and generate answers based on provided context.</li> <li>Translation: Translate text between languages.</li> <li>Chatbots and Conversational AI: Build interactive agents that respond naturally, using streaming for a responsive user experience.</li> <li>Data Extraction: Extract information from unstructured text and format it into structured data like JSON using <code>response_mime_type</code>.</li> <li>Image Captioning and Analysis: Understand the content of images and generate descriptive captions or answer questions about them.</li> <li>Function Calling: Enable the model to interact with external tools or APIs to retrieve real-time information or perform actions.</li> </ul>"},{"location":"gemini/migrate_to_gemini2/","title":"Migrate your application to Gemini 2 with the Gemini API in Vertex AI","text":"<p>This guide shows how to migrate generative AI applications from Gemini 1.x and PaLM models to Gemini 2 models.</p>"},{"location":"gemini/migrate_to_gemini2/#why-migrate-to-gemini-2","title":"\ud83d\udcda Why migrate to Gemini 2?","text":"<p>Gemini 2 delivers significant performance improvements over Gemini 1.x and PaLM models, along with new capabilities. Additionally, each model version has its own version support and availability timeline.</p> <p>Upgrading most generative AI applications to Gemini 2 shouldn't require significant reengineering of prompts or code. But some applications require prompt changes, and these changes are difficult to predict without running a prompt through Gemini 2 first. Therefore, Gemini 2 testing is recommended before migration.</p> <p>Significant code changes are only needed for certain breaking changes, or to use new Gemini 2 capabilities.</p>"},{"location":"gemini/migrate_to_gemini2/#which-gemini-2-model-should-i-migrate-to","title":"\ud83d\udcda Which Gemini 2 model should I migrate to?","text":"<p>As you choose a Gemini 2 model to migrate to, you'll want to consider the features that your application requires, as well as the cost of those features.</p> <p>For an overview of Gemini 2 model features, see Gemini 2. For an overview of all Google models, see Google models.</p> <p>For a comparison of Gemini 1.x and Gemini 2 models, see the following table.</p> Feature Gemini 1.5 Pro Gemini 1.5 Flash Gemini 2.0 Flash Gemini 2.0 Flash-Lite Gemini 2.5 Pro Gemini 2.5 Flash Input modalities text, documents, image, video, audio text, documents, image, video, audio text, documents, image, video, audio text, documents, image, video, audio text, documents, image, video, audio text, documents, image, video, audio Output modalities text text text text text text Context window, total token limit 2,097,152 1,048,576 1,048,576 1,048,576 1,048,576 1,048,576 Output context length 8,192 8,192 8,192 8,192 64,192 64,192 Grounding with Search Yes Yes Yes No Yes Yes Function calling Yes Yes Yes Yes Yes Yes Code execution No No Yes No Yes Yes Context caching Yes Yes Yes No Yes Yes Batch prediction Yes Yes Yes Yes Yes Yes Live API No No No No No No Latency Most capable in 1.5 family Fastest in 1.5 family Fast + good cost efficiency Fast + most cost efficient Slower than Flash, but good cost efficiency Fast + most cost efficient Fine-tuning Yes Yes Yes Yes Yes Yes Recommended SDK Vertex AI SDK Vertex AI SDK Gen AI SDK Gen AI SDK Gen AI SDK Gen AI SDK Pricing units Character Character Token Token Token Token"},{"location":"gemini/migrate_to_gemini2/#before-you-begin","title":"\u2699\ufe0f Before you begin","text":"<p>For a seamless Gemini 2 migration, we recommend that you address the following concerns before you begin the migration process.</p> <ul> <li>Model retirement awareness: Note the model version support and availability timelines for older Gemini models, and make sure your migration is completed before the model you're using is retired.</li> <li>InfoSec, governance, and regulatory approvals: Proactively request the approvals you need for Gemini 2 from your information security (InfoSec), risk, and compliance stakeholders. Make sure that you cover domain-specific risk and compliance constraints, especially in heavily regulated industries such as healthcare and financial services. Note that Gemini security controls differ among Gemini 2 models.</li> <li>Location availability: See the Generative AI on Google Cloud models and partner model availability documentation, and make sure your chosen Gemini 2 model is available in the regions where you need it, or consider switching to the global endpoint.</li> <li>Modality and tokenization-based pricing differences: Check Gemini 2 pricing for all the modalities (text, code, images, speech) in your application. For more information, see generative AI pricing page. Note that Gemini 2 text input and output is priced per token, while Gemini 1 text input and output is priced per character.</li> <li>Provisioned Throughput: If needed, purchase additional Provisioned Throughput for Gemini 2 or change existing Provisioned Throughput orders.</li> <li>Supervised fine-tuning: If your Gemini application uses supervised fine-tuning, submit a new tuning job with Gemini 2. We recommend that you start with the default tuning hyperparameters instead of reusing the hyperparameter values that you used with previous Gemini versions. The tuning service has been optimized for Gemini 2. Therefore, reusing previous hyperparameter values might not yield the best results.</li> <li>Regression testing: There are three main types of regression tests involved when upgrading to Gemini 2 models:<ul> <li>Code regression tests: Regression testing from a software engineering and DevOps perspective. This type of regression test is always required.</li> <li>Model performance regression tests: Regression testing from a data science or machine learning perspective. This means ensuring that the new Gemini 2 model provides outputs that are at least as high-quality as outputs from the current production model. Model performance regression tests are just model evaluations done as part of a change to a system or to the underlying model. Model performance regression testing further breaks down into:<ul> <li>Offline model performance testing: Assessing the quality of model outputs in a dedicated experimentation environment based on various model output quality metrics.</li> <li>Online model performance testing: Assessing the quality of model outputs in a live online deployment based on implicit or explicit user feedback.</li> </ul> </li> <li>Load testing: Assessing how the application handles high volumes of inference requests. This type of regression test is required for applications that require Provisioned Throughput.</li> </ul> </li> </ul>"},{"location":"gemini/migrate_to_gemini2/#migration-process","title":"\u2699\ufe0f Migration process","text":"<p>The following diagram shows the high-level workflow for migrating your application to Gemini 2.</p> <pre><code>flowchart LR\n    subgraph Preparation\n        A[1\\. Document evaluation requirements] --&gt; B[2\\. Perform code upgrades &amp; testing]\n    end\n    subgraph Evaluation &amp; Tuning\n        B --&gt; C[3\\. Conduct offline evaluation]\n        C --&gt; D{Performance adequate?}\n        D -- No --&gt; E[4\\. Assess results &amp; tune prompts/hyperparameters]\n        E --&gt; C\n    end\n    subgraph Deployment\n        D -- Yes --&gt; F[5\\. Perform load testing]\n        F --&gt; G[6\\. Conduct online evaluation]\n        G --&gt; H[7\\. Deploy to production]\n    end\n    H --&gt; I[End]</code></pre> Step 1: Document model evaluation and testing requirements <ol> <li>Prepare to repeat any relevant evaluations from when you originally built your application, along with any relevant evaluations you have done since then.</li> <li>If you feel your existing evaluations don't appropriately cover or measure the breadth of tasks that your application performs, you should design and prepare additional evaluations.</li> <li>If your application involves RAG, tool use, complex agentic workflows, or prompt chains, make sure that your existing evaluation data allows for assessing each component independently. If not, gather input-output examples for each component.</li> <li>If your application is especially high-impact, or if it's part of a larger user-facing real-time system, you should include online evaluation.</li> </ol> Step 2: Perform code upgrades and testing <p>Consider upgrading to the Google Gen AI SDK</p> <p>If your Gemini 1.x application uses the Vertex AI SDK, consider upgrading to the Gen AI SDK. New Gemini 2 capabilities are only available in the Gen AI SDK. However, there is no need to switch to the Gen AI SDK if your application only requires capabilities that are available in the Vertex AI SDK. If you're new to the Gen AI SDK, see the Getting started with Google Generative AI using the Gen AI SDK notebook.</p> Gen AI SDK (Recommended)Vertex AI SDK <p>We recommend that you migrate to the Gen AI SDK when upgrading to Gemini 2.0, as new features will only be added to this SDK. The setup process is different from the Vertex AI SDK. For more information, visit Google Gen AI SDK.</p> <p>Install</p> <pre><code>pip install --upgrade google-genai\n</code></pre> <p>To learn more, see the SDK reference documentation.</p> <p>Set environment variables and initialize</p> <p>Set environment variables to use the Gen AI SDK with Vertex AI. Replace <code>GOOGLE_CLOUD_PROJECT</code> with your Google Cloud project ID, and replace <code>GOOGLE_CLOUD_LOCATION</code> with the location of your Google Cloud project (for example, <code>us-central1</code>).</p> <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre> <pre><code>from google import genai\nfrom google.genai.types import HttpOptions\n\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash-preview-05-20\",\n    contents=\"How does AI work?\",\n)\nprint(response.text)\n# Example response:\n# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n#\n# Here's a simplified overview:\n# ...\n</code></pre> <p>If you reuse the Vertex AI SDK, the setup process is the same for the 1.0, 1.5, and 2.0 models. For more information, see Introduction to the Vertex AI SDK for Python.</p> <p>Note: The Vertex AI SDK does not support all features of Gemini 2.0. New features will only be added to the Gen AI SDK.</p> <p>Install the SDK</p> <pre><code>pip install --upgrade --quiet google-cloud-aiplatform\n</code></pre> <p>Initialize and use the SDK</p> <p>The following is a short code sample that uses the Vertex AI SDK for Python. Replace <code>PROJECT_ID</code> with your Google Cloud project ID, and replace <code>LOCATION</code> with the location of your Google Cloud project (for example, <code>us-central1</code>). Then, change the model ID to your target Gemini 2 model.</p> <pre><code>import vertexai\nfrom vertexai.generative_models import GenerativeModel\n\n# TODO(developer): Update and un-comment below line\n# PROJECT_ID = \"your-project-id\"\nvertexai.init(project=PROJECT_ID, location=\"us-central1\")\n\nmodel = GenerativeModel(\"gemini-2.0-flash-001\")\n\nresponse = model.generate_content(\n    \"What's a good name for a flower shop that specializes in selling bouquets of dried flowers?\"\n)\n\nprint(response.text)\n# Example response:\n# **Emphasizing the Dried Aspect:**\n# * Everlasting Blooms\n# * Dried &amp; Delightful\n# * The Petal Preserve\n# ...\n</code></pre> <p>Change your Gemini calls</p> <p>Change your prediction code to use Gemini 2. At a minimum, this means changing the specific model endpoint name to a Gemini 2 model where you load your model. The exact code change will differ depending on how you originally implemented your application, and especially whether you used the Gen AI SDK or the Vertex AI SDK.</p> <p>After you make your code changes, perform code regression testing and other software tests on your code to make sure that it runs. This test is only meant to assess whether the code functions correctly. It's not meant to assess the quality of model responses.</p> <p>Address breaking code changes</p> <ul> <li>Dynamic retrieval: Switch to using Grounding with Google Search. This feature requires using the Gen AI SDK; it's not supported by the Vertex AI SDK.</li> <li>Content filters: Note the default content filter settings, and change your code if it relies on a default that has changed.</li> <li><code>Top-K</code> token sampling parameter: Models after <code>gemini-1.0-pro-vision</code> don't support changing the <code>Top-K</code> parameter.</li> </ul> <p>Focus only on code changes in this step. You may need to make other changes, but wait until you start your evaluation, and then consider the following adjustment based on evaluation results:</p> <ul> <li>If you're switching from dynamic retrieval, you might need to experiment with system instructions to control when Google Search is used (for example, <code>\"Only generate queries for the Google Search tool if the user asks about sports. Don't generate queries for any other topic.\"</code>), but wait until you evaluate before making prompt changes.</li> <li>If you used the <code>Top-K</code> parameter, adjust other token sampling parameters, such as <code>Top-P</code>, to achieve similar results.</li> </ul> Step 3: Conduct offline evaluation <p>Repeat the evaluation that you did when you originally developed and launched your application, any further offline evaluation you did after launching, and any additional evaluation you identified in step 1. If you then feel that your evaluation doesn't fully capture the breadth and depth of your application, do further evaluation.</p> <p>If you don't have an automated way to run your offline evaluations, consider using the Gen AI evaluation service.</p> <p>If your application uses fine-tuning, perform offline evaluation before retuning your model with Gemini 2. Gemini 2's improved output quality may mean that your application no longer requires a fine-tuned model.</p> Step 4: Assess evaluation results and tune the Gemini 2 prompts and hyperparameters <p>If your offline evaluation shows a drop in performance with Gemini 2, iterate on your application as follows until Gemini performance matches the older model:</p> <ul> <li>Iteratively engineer your prompts to improve performance (\"Hill Climbing\"). If you are new to hill climbing, see the Vertex Gemini hill climbing online training. The Vertex AI prompt optimizer (example notebook) can help as well.</li> <li>If your application already relies on fine-tuning, try fine-tuning Gemini 2.</li> <li>If your application is impacted by Dynamic Retrieval and Top-K breaking changes, experiment with changing your prompt and token sampling parameters.</li> </ul> Step 5: Perform load testing <p>If your application requires a certain minimum throughput, perform load testing to make sure the Gemini 2 version of your application meets your throughput requirements.</p> <p>Load testing should happen before online evaluation, because online evaluation requires exposing Gemini 2 to production traffic. Use your existing load testing instrumentation to perform this step.</p> <p>If your application already meets throughput requirements, consider using Provisioned Throughput. You'll need additional short-term Provisioned Throughput to cover load testing while your existing Provisioned Throughput order continues to serve production traffic.</p> Step 6: Conduct online evaluation <p>Only proceed to online evaluation if your offline evaluation shows adequate Gemini output quality and your application requires online evaluation.</p> <p>Online evaluation is a special case of online testing. Try to use your organization's existing tools and procedures for online evaluation. For example:</p> <ul> <li>If your organization regularly conducts A/B tests, perform an A/B test that evaluates the current implementation of your application compared to the Gemini 2 version.</li> <li>If your organization regularly conducts canary deployments, be sure to do so with Gemini 2 and measure differences in user behavior.</li> </ul> <p>Online evaluation can also be done by building new feedback and measurement capabilities into your application. Different feedback and measurement capabilities are appropriate for different applications. For example:</p> <ul> <li>Adding thumbs-up and thumbs-down buttons next to model outputs and comparing thumbs-up versus thumbs-down rates between an older model and Gemini 2.</li> <li>Presenting users with the older model and Gemini 2 outputs side-by-side and asking for users to pick their favorite.</li> <li>Tracking how often users override or manually adjust older model versus Gemini 2 outputs.</li> </ul> <p>These kinds of feedback mechanisms often require running a Gemini 2 version of your application in parallel to your existing version. This parallel deployment is sometimes called \"shadow mode\" or \"blue-green deployment\".</p> <p>If online evaluation results differ significantly from offline evaluation results, your offline evaluation is not capturing key aspects of the live environment or user experience. Use the online evaluation findings to devise a new offline evaluation to cover the gap the online evaluation exposed, and then return to step 3.</p> <p>If you use Provisioned Throughput, you may need to purchase additional short-term Provisioned Throughput to continue to meet your throughput requirements for users subject to online evaluation.</p> Step 7: Deploy to production <p>Once your evaluation shows that Gemini 2 meets or exceeds performance of an older model, turn down the existing version of your application in favor of the Gemini 2 version. Follow your organization's existing procedures for production rollout.</p> <p>If you're using Provisioned Throughput, change your Provisioned Throughput order to your chosen Gemini 2 model. If you're rolling out your application incrementally, use short-term Provisioned Throughput to meet throughput requirements for two different Gemini models.</p>"},{"location":"gemini/migrate_to_gemini2/#improving-model-performance","title":"\ud83d\udcda Improving model performance","text":"<p>As you complete your migration, use the following tips to maximize Gemini 2 model performance:</p> <ul> <li>Inspect your system instructions, prompts, and few-shot learning examples for any inconsistencies, contradictions, or irrelevant instructions and examples.</li> <li>Test a more powerful model. For example, if you evaluated Gemini 2.0 Flash-Lite, try Gemini 2.0 Flash.</li> <li>Examine any automated evaluation results to make sure they match human judgment, especially results that use a judge model. Make sure your judge model instructions don't contain inconsistencies or ambiguities.</li> <li>One way to improve judge model instructions is to test the instructions with multiple humans in isolation and see if their judgments are consistent. If humans interpret the instructions differently and render different judgments, your judge model instructions are ambiguous.</li> <li>Fine-tune the Gemini 2 model.</li> <li>Examine evaluation outputs to look for patterns that show specific kinds of failures. Grouping together failures into different models, kinds, or categories gives you more targeted evaluation data, which makes it easier to adjust prompts to address these errors.</li> <li>Make sure you are independently evaluating different generative AI components.</li> <li>Experiment with adjusting token sampling parameters.</li> </ul>"},{"location":"gemini/migrate_to_gemini2/#getting-help","title":"\ud83d\udd17 Getting help","text":"<p>If you need help, Google Cloud offers support packages to meet your needs, such as 24/7 coverage, phone support, and access to a technical support manager. For more information, see Google Cloud Support.</p>"},{"location":"gemini/migrate_to_gemini2/#related-questions","title":"\ud83d\udcda Related Questions","text":"Why should I migrate my application from Gemini 1.x to Gemini 2? <p>Migrating to Gemini 2 provides significant performance improvements and access to new capabilities not found in Gemini 1.x or PaLM models. Additionally, each model version has a specific support and availability timeline, so migrating ensures your application remains on a supported version. While most migrations don't require major code changes, testing your prompts with Gemini 2 is recommended to ensure a smooth transition.</p> I'm using the Vertex AI SDK for my Gemini 1.x application. Do I have to switch to the new Gen AI SDK to use Gemini 2 models? <p>No, you are not required to switch. You can continue using the Vertex AI SDK with Gemini 2 models by updating the model name in your code. However, the documentation recommends considering an upgrade to the Gen AI SDK, as new Gemini 2 capabilities will only be made available through it. The Vertex AI SDK will not support all features of Gemini 2.0.</p> How do I choose the right Gemini 2 model for my application? <p>To select the best Gemini 2 model, you should evaluate your application's requirements against the features and costs of the available models. Key factors to consider include: *   Input/Output modalities (text, image, video, etc.) *   Context window size *   Special features like Grounding with Search, function calling, or code execution *   Latency and cost</p> <p>The documentation provides a feature comparison table to help you decide. For example, if you need the fastest response time and cost efficiency, a Flash model might be the best choice.</p> I'm using a fine-tuned Gemini 1.x model. Can I reuse my tuning settings for Gemini 2? <p>No, you should not reuse your old hyperparameter values. If your application requires fine-tuning, you must submit a new tuning job with the Gemini 2 model. It is recommended to start with the default tuning hyperparameters, as the tuning service has been optimized for Gemini 2, and reusing previous values may not yield the best results. Before you retune, you should perform an offline evaluation, as Gemini 2's improved base quality might mean that fine-tuning is no longer necessary for your application.</p> My application's performance dropped after migrating to Gemini 2. What can I do to improve it? <p>If you notice a drop in performance, you can take several steps to optimize your application: *   Tune Prompts: Iteratively refine your prompts to improve performance. You can use techniques like \"Hill Climbing\" or the Vertex AI prompt optimizer. *   Adjust Parameters: Experiment with token sampling parameters like <code>Top-P</code>, especially if you previously relied on the <code>Top-K</code> parameter, which is not supported in newer models. *   Try a More Powerful Model: If you are using a lighter model like Gemini 2.0 Flash-Lite, test a more capable model like Gemini 2.0 Flash. *   Fine-Tune: If prompt engineering is not sufficient, consider fine-tuning the Gemini 2 model. *   Analyze Failures: Examine your evaluation outputs to identify patterns in failures. This can help you create more targeted evaluation data and adjust prompts to address specific errors.</p>"},{"location":"gemini/migrate_to_gemini2/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Read the list of frequently asked questions.</li> <li>Migrate from the PaLM API to the Gemini API in Vertex AI.</li> </ul>"},{"location":"image/quickstart-image-generate-console/","title":"Quickstart: Generate and verify an image's watermark using Imagen text-to-image (Console)","text":"<p>This quickstart shows you how to use the Google Cloud console to generate an image with Imagen's text-to-image capabilities and then verify its digital watermark, which is powered by SynthID.</p> <p>Imagen on Vertex AI pricing is based on the feature you use. For more information, see Pricing.</p> Image generated using Imagen on Vertex AI from the prompt: \"portrait of a french bulldog at the beach, 85mm f/2.8\". <p>This tutorial follows this high-level workflow:</p> <pre><code>flowchart LR\n    A[Start] --&gt; B[1\\. Set up your Google Cloud project]\n    B --&gt; C[2\\. Generate and save an image]\n    C --&gt; D[3\\. Verify the image's watermark]\n    D --&gt; E[4\\. Clean up resources]\n    E --&gt; F[End]</code></pre>"},{"location":"image/quickstart-image-generate-console/#before-you-begin","title":"\u2699\ufe0f Before you begin","text":"<ol> <li> <p>Sign in to your Google Cloud account. If you're new to Google Cloud, create an account to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.</p> </li> <li> <p>In the Google Cloud console, on the project selector page, select or create a Google Cloud project.</p> <p>Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.</p> <p>Go to project selector</p> </li> <li> <p>Make sure that billing is enabled for your Google Cloud project.</p> </li> <li> <p>Enable the Vertex AI API.</p> <p>Enable the API</p> </li> </ol>"},{"location":"image/quickstart-image-generate-console/#generate-and-save-an-image","title":"\u2699\ufe0f Generate and save an image","text":"Step-by-step: Generate an image and save a local copy <ol> <li> <p>In the Google Cloud console, go to the Media page in Vertex AI Studio.</p> <p>Go to Vertex AI Studio</p> </li> <li> <p>In the Prompt field, enter a description of the image you want to generate. For this quickstart, use the following prompt:</p> <pre><code>portrait of a french bulldog at the beach, 85mm f/2.8\n</code></pre> </li> <li> <p>In the Parameters panel, configure the generation options:</p> <ul> <li>Model options: Select <code>Imagen 3</code>.</li> <li>Aspect ratio: Select <code>1:1</code>.</li> <li>Number of results: Change the value to <code>2</code>.</li> </ul> </li> <li> <p>Click play_arrowGenerate.</p> <p>Imagen generates images similar to the following:</p> <p> </p> </li> <li> <p>To save a local copy of an image, click one of the generated images to open the Image details window.</p> </li> <li> <p>In the Image details window, click Export.</p> </li> <li> <p>In the Export image dialog, click Export to download the image as a PNG file.</p> </li> </ol>"},{"location":"image/quickstart-image-generate-console/#verify-an-images-digital-watermark","title":"\u2699\ufe0f Verify an image's digital watermark","text":"Step-by-step: Verify the watermark of a saved image <p>After you generate and save a watermarked image, you can verify its digital watermark.</p> <ol> <li> <p>With a generated image selected, in the Image details window, click local_policeVerify. You are redirected to the watermark verification tool.</p> </li> <li> <p>Click Upload image.</p> </li> <li> <p>In the file selector, choose the generated image you saved locally in the previous section.</p> </li> </ol> <p>After the image is processed, the tool displays the watermark verification result. A detected watermark will look similar to the following:</p> <p> </p> <p>You have successfully used Imagen to generate an image and verify its digital watermark.</p>"},{"location":"image/quickstart-image-generate-console/#clean-up","title":"\u2699\ufe0f Clean up","text":"<p>To avoid incurring charges to your Google Cloud account for the resources used in this quickstart, delete the project that you created.</p> <p>Caution: Deleting a project has the following effects:</p> <ul> <li>Everything in the project is deleted. If you used an existing project for this quickstart, when you delete it, you also delete any other work you've done in the project.</li> <li>Custom project IDs are lost. When you created this project, you might have created a custom project ID that you want to use in the future. To preserve the URLs that use the project ID, such as an <code>appspot.com</code> URL, delete selected resources inside the project instead of deleting the whole project.</li> </ul> <ol> <li> <p>In the Google Cloud console, go to the Manage resources page.</p> <p>Go to Manage resources</p> </li> <li> <p>In the project list, select the project that you want to delete, and then click Delete.</p> </li> <li>In the dialog, type the project ID, and then click Shut down to delete the project.</li> </ol>"},{"location":"image/quickstart-image-generate-console/#related-questions","title":"\ud83d\udcda Related Questions","text":"What are the initial setup steps required in my Google Cloud project before I can use Imagen to generate images? <p>Before you can generate images, you need to perform the following steps in your Google Cloud project: 1.  Select or create a Google Cloud project. 2.  Make sure that billing is enabled for your project. 3.  Enable the Vertex AI API.</p> How do I generate an image using a text prompt in the Google Cloud console? <ol> <li>In the Google Cloud console, navigate to Vertex AI Studio &gt; Media.</li> <li>In the Prompt field, enter a description of the image you want to create.</li> <li>In the Parameters panel, you can configure options such as the model, aspect ratio, and the number of images to generate.</li> <li>Click Generate.</li> </ol> What is the purpose of the digital watermark, and how can I check if my generated image has one? <p>The digital watermark (SynthID) is used to identify an image as being generated by AI. To verify it, follow these steps in the Vertex AI Studio: 1.  Click on a generated image to open the Image details window. 2.  Save the image to your local machine by clicking Export. 3.  In the Image details window, click the Verify button. 4.  Click Upload image and select the image you just saved. The tool will then analyze the image and report whether a digital watermark is detected.</p> How can I avoid being charged for the resources I used while following this quickstart? <p>To avoid incurring charges to your Google Cloud account, you should delete the project you created for this quickstart. You can do this by going to the Manage resources page in the Google Cloud console, selecting the project, and clicking Delete. Be cautious, as this action deletes all resources associated with the project.</p> After generating multiple images, how do I save one to my computer? <ol> <li>From the generated results, click on the image you wish to save. This will open the Image details window.</li> <li>In the Image details window, click the Export button.</li> <li>In the dialog that appears, click Export again to save the image to your local machine.</li> </ol>"},{"location":"image/quickstart-image-generate-console/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Learn about all image generative AI features in the Imagen on Vertex AI overview.</li> <li>Read the usage guidelines for Imagen on Vertex AI.</li> <li>Explore more pretrained models in Model Garden.</li> <li>Learn about responsible AI best practices and Vertex AI's safety filters.</li> </ul>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/","title":"Migrate from the Gemini Developer API to the Gemini API in Vertex AI","text":"<p>If you are new to Gemini, the fastest way to get started is by using the quickstarts.</p> <p>As your generative AI solutions mature, you may need a platform for building and deploying them end-to-end. Google Cloud provides a comprehensive ecosystem of tools to enable developers to harness the power of generative AI, from initial app development to deployment, hosting, and managing complex data at scale.</p> <p>Google Cloud's Vertex AI platform offers a suite of MLOps tools that streamline the usage, deployment, and monitoring of AI models for efficiency and reliability. Additionally, integrations with databases, DevOps tools, logging, monitoring, and IAM provide a holistic approach to managing the entire generative AI lifecycle.</p> <p>Common use cases for Google Cloud offerings</p> <p>Here are some examples of common use cases that are well-suited for Google Cloud offerings:</p> <ul> <li>Productionize your apps and solutions: Products like Cloud Functions and Cloud Run let you deploy apps with enterprise-grade scale, security, and privacy. Find more details in the Security, Privacy, and Cloud Compliance on Google Cloud guide.</li> <li>Use Vertex AI for end-to-end MLOps: Leverage capabilities from tuning to vector similarity search and ML pipelines.</li> <li>Trigger your LLM with an event-driven architecture: Use Cloud Functions or Cloud Run.</li> <li>Monitor app usage: Integrate with Cloud Logging and BigQuery.</li> <li>Store data securely at scale: Use services like BigQuery, Cloud Storage, and Cloud SQL.</li> <li>Perform retrieval-augmented generation (RAG): Use data in the cloud with BigQuery or Cloud Storage.</li> <li>Create and schedule data pipelines: Schedule jobs using Cloud Scheduler.</li> <li>Apply LLMs to your data in the cloud: If you store data in Cloud Storage or BigQuery, you can prompt LLMs over that data to extract information, summarize, or ask questions about it.</li> <li>Leverage Google Cloud data governance: Use data governance/residency policies to manage your data lifecycle.</li> </ul>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#differences-between-the-gemini-developer-api-and-the-gemini-api-in-vertex-ai","title":"\ud83d\udcda Differences between the Gemini Developer API and the Gemini API in Vertex AI","text":"<p>The following table summarizes the main differences between the Gemini Developer API and the Vertex AI Gemini API to help you decide which option is right for your use case.</p> Features Gemini Developer API Vertex AI Gemini API Gemini models Gemini 1.5 Flash, Gemini 1.5 Pro Gemini 1.5 Flash, Gemini 1.5 Pro Sign up Google account Google Cloud account (with terms agreement and billing) Authentication API key Google Cloud service account User interface playground Google AI Studio Vertex AI Studio API &amp; SDK Server and mobile/web client SDKs: <ul><li>Server: Python, Node.js, Go, Dart, ABAP</li><li>Mobile/Web client: Android (Kotlin/Java), Swift, Web, Flutter</li></ul> Server and mobile/web client SDKs: <ul><li>Server: Python, Node.js, Go, Java, ABAP</li><li>Mobile/Web client (via Vertex AI in Firebase): Android (Kotlin/Java), Swift, Web, Flutter</li></ul> No-cost usage of API &amp; SDK Yes, where applicable $300 Google Cloud credit for new users Quota (requests per minute) Varies based on model and pricing plan (see detailed information) Varies based on model and region (see detailed information) Enterprise support No Customer encryption keyVirtual private cloudData residencyAccess transparencyScalable infrastructure for application hostingDatabases and data storage MLOps No Full MLOps on Vertex AI (examples: model evaluation, Model Monitoring, Model Registry)"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#migrate-to-gemini-api-in-vertex-ai","title":"\u2699\ufe0f Migrate to Gemini API in Vertex AI","text":"<p>This section shows how to migrate from the Gemini Developer API to the Gemini API in Vertex AI.</p> <pre><code>flowchart LR\n    A[Start: Set up Vertex AI] --&gt; B[Migrate Python Code]\n    B --&gt; C[Migrate Prompts]\n    C --&gt; D[Upload Training Data]\n    D --&gt; E[Finish: Delete old API Key]</code></pre> <p>Considerations when migrating</p> <ul> <li>You can use your existing Google Cloud project (the same one you used to generate your Gemini API key) or you can create a new Google Cloud project.</li> <li>Supported regions might differ between the Gemini Developer API and the Gemini API in Vertex AI. See the list of supported regions for generative AI on Google Cloud.</li> <li>Any models you created in Google AI Studio need to be retrained in Vertex AI.</li> </ul> Step 1: Start using Vertex AI Studio <p>The process you follow to migrate to the Gemini API in Vertex AI is different depending on whether you already have a Google Cloud account.</p> <p>Note: Google AI Studio and the Gemini Developer API are available only in specific regions and languages. If you aren't located in a supported region, you can't start using the Gemini API in Vertex AI.</p> <p>To learn how to migrate to the Gemini API in Vertex AI, click the tab that corresponds to your Google Cloud account status:</p> Already use Google CloudNew to Google Cloud <ol> <li>Sign in to Google AI Studio.</li> <li>At the bottom of the left navigation pane, click Build with Vertex AI on Google Cloud. The Try Vertex AI and Google Cloud for free page opens.</li> <li>Click Agree &amp; Continue. The Get Started with Vertex AI studio dialog appears.</li> <li>To enable the APIs required to run Vertex AI, click Agree &amp; Continue. The Vertex AI console appears.</li> </ol> <ol> <li>Sign in to Google AI Studio.</li> <li>At the bottom of the left navigation pane, click Build with Vertex AI on Google Cloud. The Create an account to get started with Google Cloud page opens.</li> <li>Click Agree &amp; Continue. The Let's confirm your identity page appears.</li> <li>Click Start Free. The Get Started with Vertex AI studio dialog appears.</li> <li>To enable the APIs required to run Vertex AI, click Agree &amp; Continue.</li> </ol> Step 2: Migrate your Python code <p>The following sections show code snippets to help you migrate your Python code to use the Gemini API in Vertex AI.</p> <p>Vertex AI Python SDK Setup</p> <p>On Vertex AI, you don't need an API key. Instead, Gemini on Vertex AI is managed using IAM access, which controls permission for a user, a group, or a service account to call the Gemini API through the Vertex AI SDK.</p> <p>While there are many ways to authenticate, the easiest method for authenticating in a development environment is to install the Google Cloud CLI and then use your user credentials to sign in to the CLI.</p> <p>To make inference calls to Vertex AI, you must also make sure that your user or service account has the Vertex AI User role.</p> Step 3: Migrate prompts to Vertex AI Studio <p>Your Google AI Studio prompt data is saved in a Google Drive folder. This section shows how to migrate your prompts to Vertex AI Studio.</p> <ol> <li>Open Google Drive.</li> <li>Navigate to the AI_Studio folder where the prompts are stored.     </li> <li>Download your prompts from Google Drive to a local directory.</li> </ol> <p>Note: Prompts downloaded from Google Drive are in the text (<code>txt</code>) format. Before you upload them to Vertex AI Studio, convert them to JSON files. To do this, change the file extension from <code>.txt</code> to <code>.json</code>.</p> <ol> <li>Open Vertex AI Studio in the Google Cloud console.</li> <li>In the Vertex AI menu, click Prompt management.</li> <li>Click Import prompt.</li> <li>In the Prompt file field, click Browse and select a prompt from your local directory. To upload prompts in bulk, you must manually combine your prompts into a single JSON file.</li> <li>Click Upload. The prompts are uploaded to the My Prompts tab.</li> </ol> Step 4: Upload training data to Vertex AI Studio <p>To migrate your training data to Vertex AI, you need to upload your data to a Cloud Storage bucket. For more information, see Introduction to tuning.</p> Step 5: Delete unused API Keys <p>If you no longer need to use your Gemini API key for the Gemini Developer API, follow security best practices and delete it.</p> <ol> <li>Open the Google Cloud API Credentials page.</li> <li>Find the API key that you want to delete and click the Actions icon.</li> <li>Select Delete API key.</li> <li>In the Delete credential modal, select Delete.</li> </ol> <p>Deleting an API key takes a few minutes to propagate. After propagation completes, any traffic using the deleted API key is rejected.</p> <p>Important: If you delete a key that's still used in production and need to recover it, see <code>gcloud beta services api-keys undelete</code>.</p>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#install-the-client","title":"Install the client","text":"Gemini Developer APIGemini API in Vertex AI <pre><code># To install the Python SDK, use this CLI command:\n# pip install google-generativeai\n\nimport google.generativeai as genai\nfrom google.generativeai import GenerativeModel\n\nAPI_KEY=\"API_KEY\"\ngenai.configure(api_key=API_KEY)\n</code></pre> <pre><code># To install the Python SDK, use this CLI command:\n# pip install google-cloud-aiplatform\n\nimport vertexai\nfrom vertexai.generative_models import GenerativeModel\n\nPROJECT_ID = \"PROJECT_ID\"\nLOCATION = \"LOCATION\"  # e.g. us-central1\nvertexai.init(project=PROJECT_ID, location=LOCATION)\n</code></pre>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#generate-text-from-a-text-prompt","title":"Generate text from a text prompt","text":"Gemini Developer APIGemini API in Vertex AI <pre><code>model = GenerativeModel(\"gemini-1.5-flash\")\n\nresponse = model.generate_content(\"The opposite of hot is\")\nprint(response.text) #  The opposite of hot is cold.\n</code></pre> <pre><code>model = GenerativeModel(\"gemini-1.5-flash-001\")\n\nresponse = model.generate_content(\"How does AI work?\")\nprint(response.text)\n# Example response:\n# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n#\n# Here's a simplified overview:\n# ...\n</code></pre>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#generate-text-from-text-and-image","title":"Generate text from text and image","text":"Gemini Developer APIGemini API in Vertex AI <pre><code>import PIL.Image\n\nmultimodal_model = GenerativeModel(\"gemini-1.5-flash\")\n\nimage = PIL.Image.open(\"image.jpg\")\n\nresponse = multimodal_model.generate_content([\"What is this picture?\", image])\nprint(response.text) # A cat is shown in this picture.\n</code></pre> <pre><code>from vertexai.generative_models import Part\n\nmultimodal_model = GenerativeModel(\"gemini-1.5-flash-001\")\n\nresponse = multimodal_model.generate_content(\n    [\n        \"What is shown in this image?\",\n        Part.from_uri(\n            \"gs://cloud-samples-data/generative-ai/image/scones.jpg\",\n            mime_type=\"image/jpeg\",\n        ),\n    ]\n)\nprint(response.text)\n# Example response:\n# The image shows a flat lay of blueberry scones arranged on parchment paper. There are ...\n</code></pre>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#generate-multi-turn-chat","title":"Generate multi-turn chat","text":"Gemini Developer APIGemini API in Vertex AI <pre><code>model = GenerativeModel(\"gemini-1.5-flash\")\n\nchat = model.start_chat()\n\nprint(chat.send_message(\"How are you?\").text)\nprint(chat.send_message(\"What can you do?\").text)\n</code></pre> <pre><code>model = GenerativeModel(\"gemini-1.5-flash-001\")\n\nchat = model.start_chat()\n\nresponse = chat.send_message(\"Hello\")\nprint(response.text)\n\nresponse = chat.send_message(\"Tell me a story.\")\nprint(response.text)\n# Example response:\n# Okay, here's a story for you:\n# ...\n</code></pre>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#related-questions","title":"\ud83d\udcda Related Questions","text":"What are the main advantages of migrating from the Gemini Developer API to the Gemini API in Vertex AI? <p>Migrating to the Gemini API in Vertex AI is beneficial as your generative AI solutions mature. Vertex AI provides a comprehensive platform for building and deploying applications end-to-end with enterprise-grade features. Key advantages include:</p> <ul> <li>MLOps Capabilities: Access to a full suite of MLOps tools for model evaluation, monitoring, and management.</li> <li>Enterprise Support: Features like customer-managed encryption keys, Virtual Private Cloud (VPC), data residency, and access transparency.</li> <li>Integration: Seamless integration with other Google Cloud services like Cloud Run for deployment, BigQuery and Cloud Storage for data management and Retrieval-Augmented Generation (RAG), and Cloud Logging for monitoring.</li> <li>Scalability and Security: Deploy applications with enterprise-grade scale, security, and privacy, leveraging Google Cloud's robust infrastructure.</li> </ul> How does authentication change when moving from the Gemini Developer API to the Vertex AI Gemini API? <p>Authentication methods are a key difference between the two APIs. The Gemini Developer API uses an API key for authentication. In contrast, the Gemini API in Vertex AI uses Google Cloud Identity and Access Management (IAM). Instead of an API key, you authenticate using a Google Cloud service account, and access is controlled by assigning roles, such as the \"Vertex AI User\" role, to a user, group, or service account.</p> I have prompts saved in Google AI Studio. How do I migrate them to Vertex AI Studio? <p>To migrate your prompts from Google AI Studio to Vertex AI Studio, follow these steps:</p> <ol> <li>Open Google Drive and navigate to the AI_Studio folder where your prompts are stored.</li> <li>Download the desired prompt files (which will be in <code>.txt</code> format) to your local machine.</li> <li>Convert the downloaded files to JSON format by changing the file extension from <code>.txt</code> to <code>.json</code>.</li> <li>In the Google Cloud console, go to Vertex AI Studio and select Prompt management.</li> <li>Click Import prompt, browse to your local JSON file, and upload it. The prompt will then appear in the My Prompts tab.</li> </ol> After migrating to Vertex AI, do I need to retrain the custom models I created in Google AI Studio? <p>Yes. According to the documentation, any models that you created and trained in Google AI Studio must be retrained in Vertex AI after you migrate.</p> I've successfully migrated my application to use the Vertex AI Gemini API. What should I do with my old Gemini API key? <p>As a security best practice, you should delete any API key that is no longer in use. You can delete your old Gemini API key from the Google Cloud API Credentials page. Find the key, click the Actions icon, and select Delete API key. If you accidentally delete a key that is still in use, it can be recovered for a limited time using the <code>gcloud beta services api-keys undelete</code> command.</p>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Try a quickstart tutorial using Vertex AI Studio or the Vertex AI API.</li> </ul>"},{"location":"migration/openai/Authenticate/","title":"Authenticate","text":"<p>To use the OpenAI-compatible features on Vertex AI, you must configure your application to use Google authentication and a Vertex AI endpoint. This allows you to leverage your existing OpenAI SDK-based code with Google's Gemini and other models available on Vertex AI.</p> <p>This document guides you through the authentication process.</p>"},{"location":"migration/openai/Authenticate/#before-you-begin","title":"\ud83d\udcda Before you begin","text":"<ol> <li> <p>Install SDKs: If you haven't already, install the necessary Python libraries.</p> <ul> <li>OpenAI SDK: Required for using the OpenAI client interface.</li> </ul> <pre><code>pip install openai\n</code></pre> <ul> <li>Google Auth Library: Required to generate Google Cloud authentication tokens.</li> </ul> <pre><code>pip install google-auth requests\n</code></pre> </li> <li> <p>Authenticate to Vertex AI: Set up Application Default Credentials (ADC) for your local development environment. For more information, see Set up authentication for a local development environment.</p> </li> <li> <p>Deploy a model (if necessary): Certain models in Model Garden and supported Hugging Face models must be deployed to a Vertex AI endpoint before they can serve requests. When calling these self-deployed models, you need to specify the endpoint ID. To list your existing Vertex AI endpoints, use the <code>gcloud ai endpoints list</code> command.</p> </li> </ol>"},{"location":"migration/openai/Authenticate/#choose-an-authentication-method","title":"\ud83d\udcda Choose an authentication method","text":"<p>You can configure your application to use Google credentials in two primary ways: modifying the client setup directly in your code or setting environment variables. Choose the method that best fits your development and deployment workflow.</p> Method Pros Cons Client setup Explicit configuration within your code. Good for applications where environment variables are not easily managed. Requires code modification to change credentials or endpoints. Environment variables Decouples configuration from code. Aligns with 12-factor app principles. Easy to switch between environments (dev, prod). Requires managing environment variables in your deployment environment."},{"location":"migration/openai/Authenticate/#set-up-authentication","title":"\u2699\ufe0f Set up authentication","text":"<p>The following workflow outlines the steps to authenticate your client.</p> <pre><code>flowchart LR\n    A[Start] --&gt; B[Install required libraries];\n    B --&gt; C[Set up Application Default Credentials];\n    C --&gt; D{Choose authentication method};\n    D -- Client Setup --&gt; E[Modify client in code with token &amp; endpoint];\n    D -- Environment Variables --&gt; F[Set OPENAI_API_KEY &amp; OPENAI_BASE_URL];\n    E --&gt; G[Initialize OpenAI client];\n    F --&gt; G;\n    G --&gt; H[Make API calls];\n    H --&gt; I[Finish];</code></pre> <p>Follow the steps in the tab that corresponds to your chosen authentication method.</p> Client setupEnvironment variables <p>You can programmatically generate a Google access token and configure the OpenAI client within your Python code.</p> <p>Before trying this sample, follow the Python setup instructions in the Vertex AI quickstart using client libraries. For more information, see the Vertex AI Python API reference documentation.</p> <p>The following code snippet demonstrates how to: 1.  Programmatically get an access token using the <code>google-auth</code> library. 2.  Set the <code>base_url</code> to the appropriate Vertex AI endpoint. 3.  Set the <code>api_key</code> to the generated access token.</p> <pre><code>import openai\n\nfrom google.auth import default\nimport google.auth.transport.requests\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n# Note: the credential lives for 1 hour by default (https://cloud.google.com/docs/authentication/token-types#at-lifetime); after expiration, it must be refreshed.\n\n##############################\n# Choose one of the following:\n##############################\n\n# If you are calling a Gemini model, set the ENDPOINT_ID variable to use openapi.\nENDPOINT_ID = \"openapi\"\n\n# If you are calling a self-deployed model from Model Garden, set the\n# ENDPOINT_ID variable and set the client's base URL to use your endpoint.\n# ENDPOINT_ID = \"YOUR_ENDPOINT_ID\"\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{ENDPOINT_ID}\",\n    api_key=credentials.token,\n)\n</code></pre> <p>By default, access tokens last for 1 hour. You can extend the life of your access token or see the section on automatically refreshing credentials.</p> <p>The OpenAI library can automatically read the <code>OPENAI_API_KEY</code> and <code>OPENAI_BASE_URL</code> environment variables to configure the client. This method requires you to have the Google Cloud CLI installed.</p> <p>1. Set common environment variables</p> <pre><code>export PROJECT_ID=PROJECT_ID\nexport LOCATION=LOCATION\nexport OPENAI_API_KEY=\"$(gcloud auth application-default print-access-token)\"\n</code></pre> <p>2. Set the base URL for your model type</p> <ul> <li>To call a Gemini model, set the <code>OPENAI_BASE_URL</code> to use the <code>openapi</code> endpoint:</li> </ul> <pre><code>export OPENAI_BASE_URL=\"https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/openapi\"\n</code></pre> <ul> <li>To call a self-deployed model from Model Garden, set the <code>ENDPOINT_ID</code> variable and use it in the <code>OPENAI_BASE_URL</code>:</li> </ul> <pre><code>export ENDPOINT_ID=YOUR_ENDPOINT_ID\nexport OPENAI_BASE_URL=\"https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/${ENDPOINT_ID}\"\n</code></pre> <p>3. Initialize the client</p> <p>After setting the environment variables, you can initialize the client without any arguments. It will automatically use the variables for configuration.</p> <pre><code>client = openai.OpenAI()\n</code></pre> <p>The Gemini Chat Completions API uses OAuth to authenticate with a short-lived access token. By default, access tokens last for 1 hour. You can extend the life of your access token or periodically re-run the <code>export OPENAI_API_KEY</code> command to refresh the token.</p>"},{"location":"migration/openai/Authenticate/#automatically-refresh-credentials","title":"\u2699\ufe0f Automatically refresh credentials","text":"<p>Because access tokens expire, your application must refresh them periodically. The following example shows a wrapper class for the OpenAI client that automatically refreshes the token whenever it has expired.</p> <p>This approach is useful for long-running applications to avoid authentication errors due to token expiration.</p> <pre><code>from typing import Any\n\nimport google.auth\nimport google.auth.transport.requests\nimport openai\n\n\nclass OpenAICredentialsRefresher:\n    def __init__(self, **kwargs: Any) -&gt; None:\n        # Set a placeholder key here\n        self.client = openai.OpenAI(**kwargs, api_key=\"PLACEHOLDER\")\n        self.creds, self.project = google.auth.default(\n            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n        )\n\n    def __getattr__(self, name: str) -&gt; Any:\n        if not self.creds.valid:\n            self.creds.refresh(google.auth.transport.requests.Request())\n\n            if not self.creds.valid:\n                raise RuntimeError(\"Unable to refresh auth\")\n\n            self.client.api_key = self.creds.token\n        return getattr(self.client, name)\n\n\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\nclient = OpenAICredentialsRefresher(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n)\n\nresponse = client.chat.completions.create(\n    model=\"google/gemini-2.0-flash-001\",\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n)\n\nprint(response)\n</code></pre>"},{"location":"migration/openai/Authenticate/#related-questions","title":"\ud83d\udcda Related Questions","text":"What's the simplest way to configure my application to use the OpenAI library with Vertex AI? <p>The simplest method is to use environment variables, which separates your configuration from your code.</p> <ol> <li>First, ensure you have installed the necessary libraries (<code>pip install openai google-auth requests</code>) and authenticated with gcloud (<code>gcloud auth application-default login</code>).</li> <li>Set the following environment variables in your terminal, replacing the placeholder values:     <pre><code>export PROJECT_ID=YOUR_PROJECT_ID\nexport LOCATION=YOUR_LOCATION\nexport OPENAI_API_KEY=\"$(gcloud auth application-default print-access-token)\"\n</code></pre></li> <li>Set the base URL environment variable. The URL depends on whether you are calling a Gemini model or a self-deployed model. For a Gemini model, use:     <pre><code>export OPENAI_BASE_URL=\"https://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/openapi\"\n</code></pre></li> <li>Finally, you can initialize the OpenAI client in your Python code without any extra configuration, as it will automatically use these environment variables.     <pre><code>import openai\nclient = openai.OpenAI()\n</code></pre></li> </ol> My script works initially but fails with an authentication error after about an hour. Why is this happening and how can I fix it? <p>This happens because the access token used for authentication is short-lived and expires after one hour by default. To fix this, you need to refresh the token periodically.</p> <p>The documentation provides a Python wrapper class called <code>OpenAICredentialsRefresher</code> that handles this automatically. This class intercepts calls to the OpenAI client, checks if the credential has expired, and refreshes it if necessary before proceeding with the API call.</p> <p>Here is how you can use it: <pre><code># (Assuming the OpenAICredentialsRefresher class is defined as in the documentation)\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\nclient = OpenAICredentialsRefresher(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n)\n\n# Now you can use the client as usual, and it will handle token refreshes.\nresponse = client.chat.completions.create(\n    model=\"google/gemini-2.0-flash-001\",\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n)\n</code></pre></p> What are the different ways to set up authentication, and what are the pros and cons of each? <p>There are two primary methods to configure authentication: Programmatic Client Setup and Environment Variables.</p> Method Pros Cons Best for Programmatic Client Setup Provides granular control within your application. Credentials can be managed and refreshed dynamically in your code. Requires more code to manage credentials. Applications that need to manage authentication state dynamically or handle multiple clients with different credentials. Environment Variables Offers a simple setup and separates configuration from your code, which aligns with 12-factor app principles. Less flexible for applications that require dynamic credential changes. Development environments, containerized deployments, and applications where configuration is managed externally. How do I call a custom model I've deployed to a Vertex AI endpoint instead of a standard Gemini model? <p>To call a self-deployed model, you need to specify your model's unique endpoint ID in the client's base URL instead of using the default <code>openapi</code> endpoint.</p> <p>You can find your endpoint ID by running the gcloud command: <code>gcloud ai endpoints list</code>.</p> <ul> <li>If using programmatic setup, update the <code>base_url</code> when initializing the client:     <pre><code># Replace \"YOUR_ENDPOINT_ID\" with your actual endpoint ID\nENDPOINT_ID = \"YOUR_ENDPOINT_ID\" \n\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{ENDPOINT_ID}\",\n    api_key=credentials.token,\n)\n</code></pre></li> <li>If using environment variables, set the <code>OPENAI_BASE_URL</code> variable to point to your endpoint:     <pre><code># Replace \"ENDPOINT_ID\" with your actual endpoint ID\nexport ENDPOINT=ENDPOINT_ID\nexport OPENAI_BASE_URL=\"https://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/${ENDPOINT}\"\n</code></pre></li> </ul> What are the prerequisites I need to complete before attempting to authenticate? <p>Before you configure authentication using either the programmatic or environment variable method, you must complete the following setup steps:</p> <ol> <li>Install the required Python libraries:     <pre><code>pip install openai google-auth requests\n</code></pre></li> <li>Set up Application Default Credentials (ADC) for your local environment. This allows your application to find and use your Google Cloud credentials automatically.     <pre><code>gcloud auth application-default login\n</code></pre></li> <li>Install the Google Cloud CLI if you haven't already. This is required for the <code>gcloud</code> command used in the ADC setup.</li> </ol>"},{"location":"migration/openai/Authenticate/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>See examples of calling the Chat Completions API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Inference API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Function Calling API with OpenAI-compatible syntax.</li> <li>Learn more about the Gemini API.</li> <li>Learn more about migrating from Azure OpenAI to the Gemini API.</li> </ul>"},{"location":"migration/openai/Examples/","title":"Examples","text":"<p>This page provides examples of how to use the Vertex AI Gemini API with OpenAI-compatible syntax. You can call the API in two primary ways:</p> <ul> <li>Call Gemini Models: Use a Google-managed endpoint to access Gemini models directly. This is the simplest way to get started.</li> <li>Call Self-Deployed Models: Use an endpoint for a model you have deployed yourself on Vertex AI, such as an open-source Gemma model. This gives you more control over the serving infrastructure.</li> </ul> <p>To see a complete, runnable example, open the \"Call Gemini with the OpenAI Library\" Jupyter notebook in your preferred environment:</p> <p>Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench user-managed notebooks | View on GitHub</p>"},{"location":"migration/openai/Examples/#call-gemini-models-with-the-chat-completions-api","title":"\ud83d\udcbb Call Gemini Models with the Chat Completions API","text":"<p>The following examples show how to send different types of requests to a Google-managed Gemini model endpoint.</p> <p>For Python samples, follow the setup instructions in the Vertex AI quickstart using client libraries and Set up authentication for a local development environment.</p> Non-streaming requestStreaming requestMultimodal request <p>A non-streaming request receives the entire generated response after the model has finished processing the prompt.</p> RESTPython <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\nhttps://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/openapi/chat/completions \\\n-d '{\n  \"model\": \"google/${MODEL_ID}\",\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Write a story about a magic backpack.\"\n  }]\n}'\n</code></pre> <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=\"google/gemini-2.0-flash-001\",\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n)\n\nprint(response)\n</code></pre> <p>A streaming request receives the response in chunks as it is being generated by the model. Set <code>\"stream\": true</code> to enable streaming.</p> RESTPython <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\nhttps://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/openapi/chat/completions \\\n-d '{\n  \"model\": \"google/${MODEL_ID}\",\n  \"stream\": true,\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Write a story about a magic backpack.\"\n  }]\n}'\n</code></pre> <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=\"google/gemini-2.0-flash-001\",\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n    stream=True,\n)\nfor chunk in response:\n    print(chunk)\n</code></pre> <p>The Chat Completions API supports multimodal input, including images, audio, and video.</p> REST (Image)REST (Audio)Python (Image) <p>Use <code>image_url</code> to pass in image data from a Cloud Storage bucket.</p> <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\n  https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/us-central1/endpoints/openapi/chat/completions \\\n  -d '{ \\\n    \"model\": \"google/gemini-2.0-flash-001\", \\\n    \"messages\": [{ \"role\": \"user\", \"content\": [ \\\n      { \"type\": \"text\", \"text\": \"Describe this image\" }, \\\n      { \"type\": \"image_url\", \"image_url\": \"gs://cloud-samples-data/generative-ai/image/scones.jpg\" }] }] }'\n</code></pre> <p>Use <code>input_audio</code> to pass in audio data from a Cloud Storage bucket.</p> <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\n  https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/us-central1/endpoints/openapi/chat/completions \\\n  -d '{ \\\n    \"model\": \"google/gemini-2.0-flash-001\", \\\n    \"messages\": [ \\\n      { \"role\": \"user\", \\\n        \"content\": [ \\\n          { \"type\": \"text\", \"text\": \"Describe this: \" }, \\\n          { \"type\": \"input_audio\", \"input_audio\": { \\\n            \"format\": \"audio/mp3\", \\\n            \"data\": \"gs://cloud-samples-data/generative-ai/audio/pixel.mp3\" } }] }] }'\n</code></pre> <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=\"google/gemini-2.0-flash-001\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Describe the following image:\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"gs://cloud-samples-data/generative-ai/image/scones.jpg\",\n                },\n            ],\n        }\n    ],\n)\n\nprint(response)\n</code></pre>"},{"location":"migration/openai/Examples/#call-a-self-deployed-model-with-the-chat-completions-api","title":"\ud83d\udcbb Call a Self-Deployed Model with the Chat Completions API","text":"<p>The following examples show how to send requests to a model that you have deployed to a Vertex AI endpoint.</p> <p>For Python samples, follow the setup instructions in the Vertex AI quickstart using client libraries and Set up authentication for a local development environment.</p> Non-streaming requestStreaming request <p>A non-streaming request receives the entire generated response after the model has finished processing the prompt.</p> RESTPython <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\nhttps://aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/global/endpoints/${ENDPOINT}/chat/completions \\\n-d '{\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Write a story about a magic backpack.\"\n  }]\n}'\n</code></pre> <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n# model_id = \"gemma-2-9b-it\"\n# endpoint_id = \"YOUR_ENDPOINT_ID\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{endpoint_id}\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=model_id,\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n)\nprint(response)\n</code></pre> <p>A streaming request receives the response in chunks as it is being generated by the model. Set <code>\"stream\": true</code> to enable streaming.</p> RESTPython <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\nhttps://aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/global/endpoints/${ENDPOINT}/chat/completions \\\n-d '{\n  \"stream\": true,\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Write a story about a magic backpack.\"\n  }]\n}'\n</code></pre> <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n# model_id = \"gemma-2-9b-it\"\n# endpoint_id = \"YOUR_ENDPOINT_ID\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{endpoint_id}\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=model_id,\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n    stream=True,\n)\nfor chunk in response:\n    print(chunk)\n</code></pre>"},{"location":"migration/openai/Examples/#advanced-usage","title":"\ud83d\udcbb Advanced Usage","text":"<p>This section provides examples for more advanced features.</p>"},{"location":"migration/openai/Examples/#use-response_format-for-structured-output","title":"Use <code>response_format</code> for structured output","text":"<p>You can use the <code>response_format</code> parameter with the Python SDK to get structured output that conforms to a Pydantic model.</p> <pre><code>from pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"google/gemini-2.5-flash-preview-04-17\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n    ],\n    response_format=CalendarEvent,\n)\n\nprint(completion.choices[0].message.parsed)\n</code></pre>"},{"location":"migration/openai/Examples/#use-extra_body-for-additional-parameters","title":"Use <code>extra_body</code> for additional parameters","text":"<p>You can use the <code>extra_body</code> field to pass Google-specific parameters that are not part of the standard OpenAI specification.</p> SDKREST <pre><code>client.chat.completions.create(\n  ...,\n  extra_body = {\n    'extra_body': { 'google': { ... } }\n  },\n)\n</code></pre> <p>Add <code>thought_tag_marker</code></p> <pre><code>{\n  ...,\n  \"extra_body\": {\n     \"google\": {\n       ...,\n       \"thought_tag_marker\": \"...\"\n     }\n   }\n}\n</code></pre> <p>Use <code>thinking_config</code></p> <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\n  https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/endpoints/openapi/chat/completions \\\n  -d '{ \\\n    \"model\": \"google/gemini-2.5-flash-preview-04-17\", \\\n    \"messages\": [ \\\n      { \"role\": \"user\", \\\n      \"content\": [ \\\n        { \"type\": \"text\", \\\n          \"text\": \"Are there any primes number of the form n*ceil(log(n))\" \\\n        }] }], \\\n    \"extra_body\": { \\\n      \"google\": { \\\n          \"thinking_config\": { \\\n          \"include_thoughts\": true, \"thinking_budget\": 10000 \\\n        }, \\\n        \"thought_tag_marker\": \"think\" } }, \\\n    \"stream\": true }'\n</code></pre>"},{"location":"migration/openai/Examples/#use-extra_content-for-per-message-parameters","title":"Use <code>extra_content</code> for per-message parameters","text":"<p>You can use the <code>extra_content</code> field in the REST API to pass Google-specific parameters at the message, content, or tool-call level.</p> <p><code>extra_content</code> with string <code>content</code></p> <pre><code>{\n  \"messages\": [\n    { \"role\": \"...\", \"content\": \"...\", \"extra_content\": { \"google\": { ... } } }\n  ]\n}\n</code></pre> <p>Per-message <code>extra_content</code></p> <pre><code>{\n  \"messages\": [\n    {\n      \"role\": \"...\",\n      \"content\": [\n        { \"type\": \"...\", ..., \"extra_content\": { \"google\": { ... } } }\n      ]\n    }\n  ]\n}\n</code></pre> <p>Per-tool call <code>extra_content</code></p> <pre><code>{\n  \"messages\": [\n    {\n      \"role\": \"...\",\n      \"tool_calls\": [\n        {\n          ...,\n          \"extra_content\": { \"google\": { ... } }\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"migration/openai/Examples/#related-questions","title":"\ud83d\udcda Related Questions","text":"I'm already using the OpenAI Python library. How can I adapt my code to call Google's Gemini models on Vertex AI? <p>You can use your existing knowledge of the OpenAI library with a few configuration changes. When initializing the client, you need to: 1.  Set the <code>base_url</code> to your Vertex AI endpoint: <code>https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi</code>. 2.  Pass a Google Cloud access token as the <code>api_key</code>.</p> <p>After the client is configured, you can use methods like <code>client.chat.completions.create()</code> as you normally would, specifying a Google model like <code>\"google/gemini-2.0-flash-001\"</code>.</p> Can I send images or audio files to the chat model with my text prompts? <p>Yes, the API supports multimodal requests. You can include images or audio in your request by structuring the <code>content</code> of your message as an array.</p> <ul> <li>For images: Include an object with <code>type: \"image_url\"</code> and an <code>image_url</code> pointing to a file in a Google Cloud Storage bucket (e.g., <code>\"gs://.../image.jpg\"</code>).</li> <li>For audio: Include an object with <code>type: \"input_audio\"</code> and an <code>input_audio</code> object specifying the format and the location of the audio file in a Google Cloud Storage bucket.</li> </ul> How does the API call differ between a Google-hosted Gemini model and a model I've deployed myself on a Vertex AI endpoint? <p>The primary difference is in the endpoint URL and how the model is specified.</p> <ul> <li>For a Google-hosted model: The <code>base_url</code> includes <code>/endpoints/openapi</code>, and you specify the model using a <code>google/</code> prefix, for example, <code>model=\"google/gemini-2.0-flash-001\"</code>.</li> <li>For a self-deployed model: The <code>base_url</code> points directly to your specific endpoint ID (<code>.../endpoints/{endpoint_id}</code>), and the <code>model</code> parameter is the ID of your deployed model (e.g., <code>model=\"gemma-2-9b-it\"</code>).</li> </ul> How can I force the model to return a response in a specific JSON structure? <p>You can get structured output by using the <code>response_format</code> parameter. The documentation provides a Python example where a Pydantic <code>BaseModel</code> is defined to represent the desired JSON schema. This model class is then passed to the <code>response_format</code> argument in the <code>client.beta.chat.completions.parse()</code> call to ensure the output conforms to that structure.</p> How do I use Google-specific features that aren't in the standard OpenAI API, like getting the model's intermediate thoughts? <p>You can pass Google-specific parameters using the <code>extra_body</code> field in your request. For example, to get intermediate thoughts from the model during generation, you can include a <code>thinking_config</code> object within a <code>google</code> dictionary inside <code>extra_body</code>. This allows you to access advanced, platform-specific capabilities.</p>"},{"location":"migration/openai/Examples/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>See examples of calling the Inference API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Function Calling API with OpenAI-compatible syntax.</li> <li>Learn more about the Gemini API.</li> <li>Learn more about migrating from Azure OpenAI to the Gemini API.</li> </ul>"},{"location":"migration/openai/Overview/","title":"Using OpenAI libraries with Vertex AI","text":"<p>The Chat Completions API is an OpenAI-compatible endpoint that makes it easier to use Gemini models on Vertex AI with existing tools that leverage OpenAI libraries for Python and REST.</p> <p>To see an example of using the Chat Completions API, you can run the \"Call Gemini with the OpenAI Library\" Jupyter notebook in your preferred environment:</p> <p>Open in Colab Open in Colab Enterprise Open in Vertex AI Workbench View on GitHub</p>"},{"location":"migration/openai/Overview/#choosing-an-sdk","title":"\ud83d\udcda Choosing an SDK","text":"<p>If you're already using the OpenAI libraries, this API offers a low-cost way to switch between OpenAI models and Vertex AI-hosted models to compare output, cost, and scalability without changing your existing code. If you are starting a new project, we recommend using the native Google Gen AI SDK.</p> SDK Best for Pros Cons OpenAI-compatible endpoint Migrating existing applications that use the OpenAI Python library or REST API. - Minimal code changes required.- Enables easy comparison between OpenAI and Gemini models. - Doesn't support all Gemini-native features.- May have slight differences in parameter handling. Google Gen AI SDK New projects or applications that require full access to Gemini features. - Full support for all Gemini features.- Optimized for Google Cloud infrastructure.- Idiomatic and actively developed. - Requires writing new code or refactoring existing OpenAI-based code. <p>For more information on the native SDK, see Use the Google Gen AI SDK.</p>"},{"location":"migration/openai/Overview/#supported-models","title":"\ud83d\udcda Supported models","text":"<p>The Chat Completions API supports both Gemini models and select self-deployed models from Model Garden.</p>"},{"location":"migration/openai/Overview/#gemini-models","title":"Gemini models","text":"<p>The following models provide support for the Chat Completions API:</p> <ul> <li>Gemini 2.5 Pro (Preview)</li> <li>Gemini 2.5 Flash (Preview)</li> <li>Gemini 2.0 Flash</li> <li>Gemini 2.0 Flash-Lite</li> </ul>"},{"location":"migration/openai/Overview/#self-deployed-models-from-model-garden","title":"Self-deployed models from Model Garden","text":"<p>The Hugging Face Text Generation Interface (HF TGI) and Vertex AI Model Garden prebuilt vLLM containers support the Chat Completions API. However, not every model deployed to these containers supports the API. The following are the most popular supported models by container:</p> HF TGIvLLM <ul> <li><code>gemma-2-9b-it</code></li> <li><code>gemma-2-27b-it</code></li> <li><code>Meta-Llama-3.1-8B-Instruct</code></li> <li><code>Meta-Llama-3-8B-Instruct</code></li> <li><code>Mistral-7B-Instruct-v0.3</code></li> <li><code>Mistral-Nemo-Instruct-2407</code></li> </ul> <ul> <li>Gemma</li> <li>Llama 2</li> <li>Llama 3</li> <li>Mistral-7B</li> <li>Mistral Nemo</li> </ul>"},{"location":"migration/openai/Overview/#supported-parameters","title":"\ud83d\udd17 Supported parameters","text":"<p>For Google models, the Chat Completions API supports the following OpenAI parameters. For a description of each parameter, see OpenAI's documentation on Creating chat completions. Parameter support for third-party models varies by model. Consult the model's documentation to see which parameters are supported.</p> <p>If you pass any unsupported parameter, it is ignored.</p>"},{"location":"migration/openai/Overview/#standard-parameters","title":"Standard parameters","text":"Parameter Description <code>messages</code> <ul><li><code>System message</code></li><li><code>User message</code>: The <code>text</code> and <code>image_url</code> types are supported. The <code>image_url</code> type supports images stored as a Cloud Storage URI or a base64 encoding in the form <code>\"data:&lt;MIME-TYPE&gt;;base64,&lt;BASE64-ENCODED-BYTES&gt;\"</code>. To learn how to create a Cloud Storage bucket and upload a file to it, see Discover object storage. The <code>detail</code> option is not supported.</li><li><code>Assistant message</code></li><li><code>Tool message</code></li><li><code>Function message</code>: This field is deprecated, but supported for backwards compatibility.</li></ul> <code>model</code> The model to use for the completion. <code>max_completion_tokens</code> Alias for <code>max_tokens</code>. <code>max_tokens</code> The maximum number of tokens to generate. <code>n</code> The number of completions to generate. <code>frequency_penalty</code> Penalizes new tokens based on their existing frequency in the text so far. <code>presence_penalty</code> Penalizes new tokens based on whether they appear in the text so far. <code>reasoning_effort</code> Configures how much time and how many tokens are used on a response.<ul><li><code>low</code>: 1024</li><li><code>medium</code>: 8192</li><li><code>high</code>: 24576</li></ul>As no thoughts are included in the response, only one of <code>reasoning_effort</code> or <code>extra_body.google.thinking_config</code> may be specified. <code>response_format</code> <ul><li><code>json_object</code>: Interpreted as passing \"application/json\" to the Gemini API.</li><li><code>json_schema</code>: Fully recursive schemas are not supported. <code>additional_properties</code> is supported.</li><li><code>text</code>: Interpreted as passing \"text/plain\" to the Gemini API.</li><li>Any other MIME type is passed as is to the model, such as passing \"application/json\" directly.</li></ul> <code>seed</code> Corresponds to <code>GenerationConfig.seed</code>. <code>stop</code> Sequences where the API will stop generating further tokens. <code>stream</code> If set, partial message deltas will be sent, like in ChatGPT. <code>temperature</code> Controls randomness. Lowering results in less random completions. <code>top_p</code> Controls diversity via nucleus sampling. <code>tools</code> <ul><li><code>type</code></li><li><code>function</code><ul><li><code>name</code></li><li><code>description</code></li><li><code>parameters</code>: Specify parameters by using the OpenAPI specification. This differs from the OpenAI parameters field, which is described as a JSON Schema object. To learn about keyword differences between OpenAPI and JSON Schema, see the OpenAPI guide.</li></ul></li></ul> <code>tool_choice</code> <ul><li><code>none</code></li><li><code>auto</code></li><li><code>required</code>: Corresponds to the mode <code>ANY</code> in the <code>FunctionCallingConfig</code>.</li><li><code>validated</code>: Corresponds to the mode <code>VALIDATED</code> in the <code>FunctionCallingConfig</code>. This is Google-specific.</li></ul> <code>web_search_options</code> Corresponds to the <code>GoogleSearch</code> tool. No sub-options are supported. <code>function_call</code> This field is deprecated, but supported for backwards compatibility. <code>functions</code> This field is deprecated, but supported for backwards compatibility."},{"location":"migration/openai/Overview/#multimodal-input-parameters","title":"Multimodal input parameters","text":"<p>The Chat Completions API supports select multimodal inputs. In general, the <code>data</code> parameter can be a URI or a combination of MIME type and base64 encoded bytes in the form <code>\"data:&lt;MIME-TYPE&gt;;base64,&lt;BASE64-ENCODED-BYTES&gt;\"</code>.</p> <p>For a full list of MIME types, see <code>GenerateContent</code>. For more information on OpenAI's base64 encoding, see their documentation.</p> Parameter Description <code>input_audio</code> <ul><li><code>data</code>: Any URI or valid blob format. We support all blob types, including image, audio, and video. Anything supported by <code>GenerateContent</code> is supported (HTTP, Cloud Storage, etc.).</li><li><code>format</code>: OpenAI supports both <code>wav</code> (audio/wav) and <code>mp3</code> (audio/mp3). Using Gemini, all valid MIME types are supported.</li></ul> <code>image_url</code> <ul><li><code>data</code>: Like <code>input_audio</code>, any URI or valid blob format is supported. Note that <code>image_url</code> as a URL will default to the <code>image/*</code> MIME-type and <code>image_url</code> as blob data can be used as any multimodal input.</li><li><code>detail</code>: Similar to media resolution, this determines the maximum tokens per image for the request. Note that while OpenAI's field is per-image, Gemini enforces the same detail across the request, and passing multiple detail types in one request will throw an error.</li></ul> <p>For usage, see our multimodal input examples.</p>"},{"location":"migration/openai/Overview/#gemini-specific-parameters","title":"Gemini-specific parameters","text":"<p>Several features supported by Gemini are not available in OpenAI models. These features can still be passed in as parameters, but must be contained within an <code>extra_content</code> or <code>extra_body</code> or they will be ignored.</p> <p><code>extra_body</code> features</p> Parameter Description <code>safety_settings</code> This corresponds to Gemini's <code>[SafetySetting](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/SafetySetting)</code>. <code>cached_content</code> This corresponds to Gemini's <code>[GenerateContentRequest](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/projects.locations.publishers.models/generateContent).cached_content</code>. <code>thinking_config</code> This corresponds to Gemini's <code>[GenerationConfig.ThinkingConfig](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/GenerationConfig#ThinkingConfig)</code>. <code>thought_tag_marker</code> Used to separate a model's thoughts from its responses for models with Thinking available. If not specified, no tags will be returned around the model's thoughts. If present, subsequent queries will strip the thought tags and mark the thoughts appropriately for context. This helps preserve the appropriate context for subsequent queries. <p><code>extra_part</code> features</p> <p><code>extra_part</code> lets you specify additional settings at a per-<code>Part</code> level.</p> Parameter Description <code>extra_content</code> A field for adding Gemini-specific content that shouldn't be ignored. <code>thought</code> This will explicitly mark if a field is a thought (and take precedence over <code>thought_tag_marker</code>). This should be used to specify whether a tool call is part of a thought or not."},{"location":"migration/openai/Overview/#related-questions","title":"\ud83d\udcda Related Questions","text":"I'm already using the OpenAI Python library. Why should I consider using it with Vertex AI, and is it difficult to switch? <p>Using the OpenAI library with Vertex AI's Chat Completions API provides a low-effort way to test and compare the performance, cost, and scalability of Google's Gemini models and select open models. If your application is already built with the OpenAI Python library or REST API, you can switch to using models on Vertex AI with minimal code changes. However, for new projects, using the Vertex AI SDK for Python is recommended for full access to all Vertex AI features.</p> Which models can I access through the OpenAI-compatible Chat Completions API? <p>The API supports both Google's Gemini models and select self-deployed models from Model Garden.</p> <ul> <li>Gemini models: Gemini 2.5 Pro, Gemini 2.5 Flash, Gemini 2.0 Flash, and Gemini 2.0 Flash-Lite.</li> <li>Self-deployed models: You can use models deployed with Hugging Face Text Generation Interface (HF TGI) or Vertex AI's prebuilt vLLM containers. This includes popular models like Gemma, Llama 3, and Mistral.</li> </ul> How can I use Gemini-specific features that aren't part of the standard OpenAI API? <p>You can access features unique to Gemini by passing them within an <code>extra_body</code> object in your API request. Supported Gemini-specific parameters include <code>safety_settings</code>, <code>cached_content</code>, and <code>thinking_config</code>.</p> My application needs the model to return a JSON object. How can I enforce this using the Chat Completions API? <p>You can specify the output format by using the <code>response_format</code> parameter. To get a JSON output, set it to <code>json_object</code>. This is interpreted as passing \"application/json\" to the Gemini API.</p> How do I send images to a model using the OpenAI library with Vertex AI? <p>You can send images by including them in the <code>messages</code> list. For a user message, use the <code>image_url</code> type. The image data can be provided as a Cloud Storage URI or as a base64 encoded string in the format <code>\"data:&lt;MIME-TYPE&gt;;base64,&lt;BASE64-ENCODED-BYTES&gt;\"</code>. Note that while the <code>image_url</code> parameter is used, the <code>detail</code> option within it is not supported.</p>"},{"location":"migration/openai/Overview/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Learn more about authentication and credentialing with the OpenAI-compatible syntax.</li> <li>See examples of calling the Chat Completions API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Inference API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Function Calling API with OpenAI-compatible syntax.</li> <li>Learn more about the Gemini API.</li> <li>Learn more about migrating from Azure OpenAI to the Gemini API.</li> </ul>"},{"location":"start/express-mode/overview/","title":"Vertex AI in express mode overview","text":"<p>Preview</p> <p>This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the Service Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see the launch stage descriptions.</p> <p>Vertex AI in express mode is the fastest way to start building generative AI applications on Google Cloud. Signing up in express mode is quick and easy, and it doesn't require entering any billing information. After you sign up, you can access and use Google Cloud APIs in just a few steps.</p> <p>To learn more about Vertex AI in express mode, see Google Cloud express mode FAQs.</p>"},{"location":"start/express-mode/overview/#understanding-vertex-ai-modes","title":"\ud83d\udcda Understanding Vertex AI Modes","text":"<p>Vertex AI in express mode provides a streamlined entry point to generative AI. As your needs grow, you can enable billing or graduate to the full Google Cloud experience. The following table compares the different modes.</p> Item Vertex AI express mode Vertex AI express mode with billing Vertex AI (Full Google Cloud) Time limit 90 days Unlimited Unlimited Available services Basic Generative AI on Vertex AI services. Expanded Vertex AI services and select Google Cloud services. All Google Cloud services, including Vertex AI. Data sources Google Drive <ul><li>Google Drive</li><li>Web files</li><li>YouTube video URLs</li></ul> All data sources available in Google Cloud. Quota See Available models and rate limits in express mode. See Rate limits. See Rate limits. Service level agreement (SLA) None Vertex AI SLA Vertex AI SLA API endpoint format Specify API key instead of project ID and location. For example: <code>https://aiplatform.googleapis.com/v1/publishers/google/models/{model}:streamGenerateContent?key={API_KEY}</code> Specify API key instead of project ID and location. For example: <code>https://aiplatform.googleapis.com/v1/publishers/google/models/{model}:streamGenerateContent?key={API_KEY}</code> Specify project ID and location. For example: <code>https://{location}-aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/publishers/google/models/{model}:streamGenerateContent</code>"},{"location":"start/express-mode/overview/#about-vertex-ai-in-express-mode","title":"\ud83d\udcda About Vertex AI in express mode","text":"<p>Upon completing your sign-up in express mode, you get access to the following:</p> <ul> <li>Core Vertex AI Studio features: You can test and customize prompts for different generative AI models in Vertex AI Studio in express mode, and get the corresponding code to use in your application.</li> <li>An API key: Use this key to authenticate your application's requests to the Vertex AI API.</li> <li>90 days to try Vertex AI: During your 90-day trial, you can use the Vertex AI APIs that support express mode for free up to their quotas.</li> </ul> <p>You can increase your quota limits at any time by enabling billing. After enabling billing, the 90-day limit is removed, your quotas are increased, and you only pay for what you use.</p>"},{"location":"start/express-mode/overview/#express-mode-eligibility","title":"\ud83d\udcda Express mode eligibility","text":"<p>Vertex AI in express mode is separate from, and not available through, the Google Cloud Free Program. If you are in the Google Cloud Free Program, see the other quickstarts in the Get Started section to start using Generative AI on Vertex AI.</p> <p>Vertex AI is available in express mode for developers that click the Try Vertex AI Studio free button and sign up using a <code>@gmail.com</code> Google Account. Accounts used previously to access Google Cloud are ineligible for express mode and are not shown the Try Vertex AI Studio free button. For example, if you used your Google Account to create a Google Cloud free trial account, you are not eligible to sign up in express mode with that same Google Account.</p>"},{"location":"start/express-mode/overview/#available-models-and-rate-limits-in-express-mode","title":"\ud83d\udcda Available models and rate limits in express mode","text":"<p>You can try out several models in express mode, including the Gemini 2.0 Flash models. The following table lists the models that are available in express mode, along with their rate limits:</p> Model category Available models Requests per minute Gemini <code>gemini-2.0-flash-001</code> 30 <code>gemini-2.0-flash-lite-001</code> 30 <code>gemini-2.5-pro-preview-05-06</code> 30 <code>gemini-2.5-flash-preview-04-17</code> 30 <code>gemini-2.5-flash-preview-05-20</code> 30 <p>For Gemini 2.0 models, the Multimodal Live API isn't available in the Console in express mode. To use the Multimodal Live API in express mode, use the Vertex AI API or the Google Gen AI SDK.</p>"},{"location":"start/express-mode/overview/#vertex-ai-in-express-mode-workflow","title":"\u2699\ufe0f Vertex AI in express mode workflow","text":"<p>You can start sending requests from your application to Vertex AI APIs in three steps.</p> <ol> <li> <p>Use Vertex AI Studio in express mode to quickly try Vertex AI features.</p> <p>For example, in the Google Cloud console in express mode, select Vertex AI &gt; Freeform and use the Freeform page to create and optimize multimodal prompts using a variety of Gemini models.</p> </li> <li> <p>Get the code for what you implemented with the UI.</p> <p>On the Freeform page, click code Get code. A panel opens showing code that programmatically sends the same requests that you implemented in the UI. You can get the code for a programming language or curl. You can use Google Colab to try the Python code.</p> </li> <li> <p>Use your API key to authenticate with the Vertex AI API.</p> <p>In the Google Cloud console in express mode, click menu Menu and select API Keys, and then copy your key into your code where it says <code>\"YOUR_API_KEY\"</code>.</p> </li> </ol> Python (Gen AI SDK)cURL <p>The Google Gen AI SDK for Python is available on PyPI and GitHub:</p> <ul> <li><code>google-genai</code> on PyPI</li> <li><code>python-genai</code> on GitHub</li> </ul> <p>To learn more, see the Python SDK reference (opens in a new tab).</p> <pre><code>from google import genai\n\n# TODO(developer): Update below line\nAPI_KEY = \"YOUR_API_KEY\"\n\nclient = genai.Client(vertexai=True, api_key=API_KEY)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash-preview-05-20\",\n    contents=\"Explain bubble sort to me.\",\n)\n\nprint(response.text)\n# Example response:\n# Bubble Sort is a simple sorting algorithm that repeatedly steps through the list\n</code></pre> <p>You can call the Vertex AI API using cURL. Replace <code>YOUR_API_KEY</code> with the key you copied from the console.</p> <pre><code>curl -X POST \\\n    -H \"Content-Type: application/json\" \\\n    \"https://aiplatform.googleapis.com/v1/publishers/google/models/gemini-2.5-flash-preview-05-20:generateContent?key=YOUR_API_KEY\" \\\n    -d '{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"Explain bubble sort to me.\"}]}]}'\n</code></pre>"},{"location":"start/express-mode/overview/#whats-different-in-express-mode","title":"\ud83d\udcda What's different in express mode","text":"<p>Vertex AI in express mode provides a subset of the features for Generative AI on Vertex AI. Therefore, some of the Vertex AI documentation is not relevant if you signed up in express mode. For details on the available API endpoints in express mode, see the Vertex AI in express mode REST API reference.</p> <p>In addition, customers in Google Cloud typically use organizations and projects to work with resources. When using Vertex AI in express mode, you don't need to worry about organizations or projects. You can still use the documentation, but ignore concepts and instructions that refer to organizations and projects. The location you selected when signing up is used throughout your experience.</p> <p>When calling REST API endpoints in express mode, you'll use the endpoint format for express mode and specify your API key.</p> Endpoint Type URL Format Standard endpoint URL <code>https://{location}-aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/publishers/google/models/{model}:streamGenerateContent</code> Endpoint URL in express mode <code>https://aiplatform.googleapis.com/v1/publishers/google/models/{model}:streamGenerateContent?key={API_KEY}</code>"},{"location":"start/express-mode/overview/#view-and-manage-api-keys","title":"\u2699\ufe0f View and manage API keys","text":"<p>To authenticate with Vertex AI API endpoints that support express mode, use the API key that was created for you during sign-up or any key that you've created in express mode.</p> <p>To learn more about the best practices for managing API keys, see Best practices for managing API keys.</p> <p>To view and manage your API keys, do the following:</p> <ol> <li> <p>Go to the Vertex AI Studio Overview page in express mode.</p> <p>Go to Vertex AI Studio</p> </li> <li> <p>In the Google Cloud console in express mode, click menu Menu.</p> </li> <li> <p>Select API Keys. The API Keys page opens and you can use it to manage your API keys.</p> </li> </ol>"},{"location":"start/express-mode/overview/#view-quotas","title":"\u2699\ufe0f View quotas","text":"<p>Your free use of Vertex AI in express mode is restricted by quotas. These quotas restrict the rate at which you can use Vertex AI in express mode at no cost.</p> <p>To view your current usage and quotas, do the following:</p> <ol> <li> <p>Go to the Vertex AI Studio Overview page in express mode.</p> <p>Go to Vertex AI Studio</p> </li> <li> <p>In the Google Cloud console in express mode, click menu Menu.</p> </li> <li> <p>Select Quotas.</p> </li> </ol>"},{"location":"start/express-mode/overview/#enable-and-manage-billing","title":"\u2699\ufe0f Enable and manage billing","text":"<p>You can increase your quotas and remove the 90-day limit by enabling billing. After enabling billing, you pay only for what you use. You can also save your prompts and access additional settings in the console that are grayed out when billing isn't enabled.</p> <p>To enable billing, do the following:</p> <ol> <li> <p>Go to the Vertex AI Studio Overview page in express mode.</p> <p>Go to Vertex AI Studio</p> </li> <li> <p>In the Google Cloud console in express mode, click menu Menu.</p> </li> <li> <p>Select Billing.</p> </li> </ol>"},{"location":"start/express-mode/overview/#start-using-all-google-cloud-capabilities-and-services","title":"\u2699\ufe0f Start using all Google Cloud capabilities and services","text":"<p>You can start using all the capabilities and services available in Google Cloud in your project by graduating from express mode.</p> <p>To graduate from express mode, do the following:</p> <ol> <li> <p>Go to the Vertex AI Studio Overview page in express mode.</p> <p>Go to Vertex AI Studio</p> </li> <li> <p>In the Google Cloud console in express mode, click menu Menu.</p> </li> <li> <p>Select Billing.</p> </li> <li> <p>In the Access all Google Cloud section, click Learn more and get started.</p> </li> </ol> <p>After you graduate from express mode, specify your project ID and location instead of your API key when you call the REST API endpoints. For example: <code>https://{location}-aiplatform.googleapis.com/v1/projects/{projectid}/locations/{location}/publishers/google/models/{model}:streamGenerateContent</code></p>"},{"location":"start/express-mode/overview/#related-questions","title":"\ud83d\udcda Related Questions","text":"What is Vertex AI in express mode and what are its main limitations? <p>Vertex AI in express mode is a way to quickly start building generative AI applications on Google Cloud for free, without needing to enter billing information. Upon signing up with a <code>@gmail.com</code> account, you get access to core Vertex AI Studio features, an API key, and a 90-day trial period.</p> <p>The main limitations compared to the full Vertex AI platform are: *   Time limit: 90 days (removed by enabling billing). *   Services: Access is limited to basic Generative AI on Vertex AI services. *   Quotas: Rate limits are lower (e.g., 30 requests per minute for Gemini models). *   SLA: There is no Service Level Agreement (SLA). *   Data sources: Limited to Google Drive.</p> Why can't I sign up for express mode? I don't see the 'Try Vertex AI Studio free' button. <p>You may not be eligible for Vertex AI in express mode. Eligibility is restricted to developers who sign up using a <code>@gmail.com</code> Google Account that has not been previously used to access Google Cloud. For example, if you have already used your Google Account for the Google Cloud Free Program or a free trial, you are ineligible for express mode with that same account and will not be shown the sign-up button.</p> How do I authenticate API calls in express mode? <p>In express mode, you authenticate using an API key that is automatically generated during sign-up. To use it:</p> <ol> <li>In the Google Cloud console for express mode, click the Menu.</li> <li>Select API Keys to view and copy your key.</li> <li>In your code, provide this key for authentication. For REST calls, you add it as a query parameter (<code>?key=YOUR_API_KEY</code>). For the Python SDK, you provide it when initializing the client.</li> </ol> What happens after my 90-day express mode trial ends? <p>The free trial for Vertex AI in express mode lasts for 90 days. To continue using the service beyond this period, you must enable billing. Enabling billing removes the 90-day limit, increases your usage quotas, and transitions your account to a pay-as-you-go model where you only pay for what you use. It also unlocks additional features that are unavailable in the free version.</p> I'm ready to use the full Google Cloud platform. How do I 'graduate' from express mode and how will my API calls change? <p>To graduate from express mode and access all Google Cloud services, navigate to the Menu in the express mode console, select Billing, and in the Access all Google Cloud section, click Learn more and get started.</p> <p>After graduating, your API calls must be updated. The primary change is the endpoint format and authentication method. Instead of using an API key, you will use your project ID and location for authentication.</p> <ul> <li>Express mode endpoint: <code>https://aiplatform.googleapis.com/v1/publishers/google/models/{model}:streamGenerateContent?key={API_KEY}</code></li> <li>Standard endpoint: <code>https://{location}-aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/publishers/google/models/{model}:streamGenerateContent</code></li> </ul>"},{"location":"start/express-mode/overview/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Try the Vertex AI Studio tutorial for Vertex AI in express mode.</li> <li>Try the API tutorial for Vertex AI in express mode.</li> <li>See the complete API reference for Vertex AI in express mode.</li> </ul>"},{"location":"start/express-mode/vertex-ai-express-mode-api-quickstart/","title":"Tutorial: Vertex AI API in express mode","text":"<p>Preview</p> <p>This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the Service Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see the launch stage descriptions.</p> <p>To see an example of Vertex AI in Express Mode, run the \"Getting started with Gemini using Vertex AI in Express Mode\" Jupyter notebook in one of the following environments:</p> <p>Open in Colab Open in Colab Enterprise Open in Vertex AI Workbench View on GitHub</p>"},{"location":"start/express-mode/vertex-ai-express-mode-api-quickstart/#how-to-the-vertex-ai-api-in-express-mode","title":"\u2699\ufe0f How to the Vertex AI API in express mode","text":"<p>Vertex AI in express mode lets you quickly try out core generative AI features that are available on Vertex AI. This tutorial shows you how to perform the following tasks by using the Vertex AI API in express mode:</p> Step 1: Install and initialize the Google Gen AI SDK for express mode <p>The Google Gen AI SDK lets you use Google generative AI models and features to build AI-powered applications. When using Vertex AI in express mode, you install and initialize the <code>google-genai</code> package to authenticate using your generated API key.</p> <p>Install the SDK</p> <p>To install the Google Gen AI SDK for express mode, run the following commands:</p> <pre><code># Developer TODO: If you're using Colab, uncomment the following lines:\n# from google.colab import auth\n# auth.authenticate_user()\n\n!pip install google-genai\n\n!pip install --force-reinstall -qq \"numpy&lt;2.0\"\n</code></pre> <p>If you're using Colab, ignore any dependency conflicts and restart the runtime after installation.</p> <p>Initialize the client</p> <p>Configure the API key for express mode and environment variables. For details on getting an API key, see Vertex AI in express mode overview.</p> <pre><code>from google import genai\nfrom google.genai import types\n\n# Developer TODO: Replace YOUR_API_KEY with your API key.\nAPI_KEY = \"YOUR_API_KEY\"\n\nclient = genai.Client(\n    vertexai=True, api_key=API_KEY\n)\n</code></pre> Step 2: Send a request to the Gemini for Google Cloud API <p>You can send requests to the Gemini for Google Cloud API in different ways depending on your needs. Streaming requests are useful for interactive applications, while non-streaming requests are simpler for offline processing. Function calling enables more complex, tool-based interactions.</p> Streaming requestNon-streaming requestFunction calling request <p>To send a streaming request, set <code>stream=True</code> and print the response in chunks. Streaming requests return the response as it's being generated, which can improve the user experience by reducing perceived latency.</p> <pre><code>from google import genai\nfrom google.genai import types\n\ndef generate():\n  client = genai.Client(vertexai=True, api_key=YOUR_API_KEY)\n\n  config=types.GenerateContentConfig(\n      temperature=0,\n      top_p=0.95,\n      top_k=20,\n      candidate_count=1,\n      seed=5,\n      max_output_tokens=100,\n      stop_sequences=[\"STOP!\"],\n      presence_penalty=0.0,\n      frequency_penalty=0.0,\n      safety_settings=[\n          types.SafetySetting(\n              category=\"HARM_CATEGORY_HATE_SPEECH\",\n              threshold=\"BLOCK_ONLY_HIGH\",\n          )\n      ],\n  )\n  for chunk in client.models.generate_content_stream(\n    model=\"gemini-2.0-flash-001\",\n    contents=\"Explain bubble sort to me\",\n    config=config,\n  ):\n    print(chunk.text)\n\ngenerate()\n</code></pre> <p>A non-streaming request returns the full response only after the entire generation process is complete. This is simpler to handle but might feel slower to an end-user. The following code sample defines a function that sends a non-streaming request to the <code>gemini-2.0-flash-001</code> model.</p> <pre><code>from google import genai\nfrom google.genai import types\n\ndef generate():\n  client = genai.Client(vertexai=True, api_key=YOUR_API_KEY)\n\n  config=types.GenerateContentConfig(\n      temperature=0,\n      top_p=0.95,\n      top_k=20,\n      candidate_count=1,\n      seed=5,\n      max_output_tokens=100,\n      stop_sequences=[\"STOP!\"],\n      presence_penalty=0.0,\n      frequency_penalty=0.0,\n      safety_settings=[\n          types.SafetySetting(\n              category=\"HARM_CATEGORY_HATE_SPEECH\",\n              threshold=\"BLOCK_ONLY_HIGH\",\n          )\n      ],\n  )\n  response = client.models.generate_content(\n    model=\"gemini-2.0-flash-001\",\n    contents=\"Explain bubble sort to me\",\n    config=config,\n  )\n  print(response.text)\n\ngenerate()\n</code></pre> <p>Function calling lets you define custom functions and provide them to the model. The model can then generate a structured data output that includes the name of a function to call and the arguments to use. You can then execute this function and return the result to the model to continue the conversation.</p> <p>The following code sample declares a function, passes it as a tool, and then receives a function call in the response.</p> <pre><code>function_response_parts = [\n    {\n        'function_response': {\n            'name': 'get_current_weather',\n            'response': {\n                'name': 'get_current_weather',\n                'content': {'weather': 'super nice'},\n            },\n        },\n    },\n]\nmanual_function_calling_contents = [\n    {'role': 'user', 'parts': [{'text': 'What is the weather in Boston?'}]},\n    {\n        'role': 'model',\n        'parts': [{\n            'function_call': {\n                'name': 'get_current_weather',\n                'args': {'location': 'Boston'},\n            }\n        }],\n    },\n    {'role': 'user', 'parts': function_response_parts},\n]\nfunction_declarations = [{\n    'name': 'get_current_weather',\n    'description': 'Get the current weather in a city',\n    'parameters': {\n        'type': 'OBJECT',\n        'properties': {\n            'location': {\n                'type': 'STRING',\n                'description': 'The location to get the weather for',\n            },\n            'unit': {\n                'type': 'STRING',\n                'enum': ['C', 'F'],\n            },\n        },\n    },\n}]\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash-001\",\n    contents=manual_function_calling_contents,\n    config=dict(tools=[{'function_declarations': function_declarations}]),\n)\nprint(response.text)\n</code></pre>"},{"location":"start/express-mode/vertex-ai-express-mode-api-quickstart/#clean-up","title":"\u2699\ufe0f Clean up","text":"<p>This tutorial does not create any Google Cloud resources, so no clean up is needed to avoid charges.</p>"},{"location":"start/express-mode/vertex-ai-express-mode-api-quickstart/#related-questions","title":"\ud83d\udcda Related Questions","text":"What is Vertex AI in express mode? <p>Vertex AI in express mode is a way to quickly try out core generative AI features available on Vertex AI using an API key for authentication. It is currently a Preview feature, which means it is subject to pre-general availability terms, may have limited support, and is provided \"as is\".</p> How do I get started with the Vertex AI API in express mode? <p>To get started, you need to install and initialize the Google Gen AI SDK.</p> <ol> <li>Install the SDK: Run <code>pip install google-genai</code>. If you are using Colab, you should ignore any dependency conflicts and restart the runtime after installation.</li> <li>Initialize the SDK: Import the library and configure the client with your API key.     <pre><code>from google import genai\n\n# Replace YOUR_API_KEY with your API key.\nAPI_KEY = \"YOUR_API_KEY\"\n\nclient = genai.Client(\n    vertexai=True, api_key=API_KEY\n)\n</code></pre></li> </ol> When should I use a streaming request versus a non-streaming request? <p>You should use a non-streaming request for tasks where the entire response is needed before processing can continue. This is the simplest request type.</p> <p>Use a streaming request for interactive applications to reduce perceived latency and display results incrementally as they are generated. To make a streaming request, set <code>stream=True</code> when calling the <code>generate_content_stream</code> method.</p> How can I connect the Gemini model to my own tools or external APIs? <p>You can use function calling to connect the model to external tools and APIs. This involves three main steps: 1.  Declare your function and its parameters and pass it to the model as a tool. 2.  The model will generate a function call with arguments based on the user's prompt. 3.  Your code executes the specified function with the provided arguments and sends the result back to the model to inform its final response.</p> Will I be charged for any Google Cloud resources if I follow this tutorial? <p>No. The documentation states that this tutorial does not create any Google Cloud resources, so no cleanup is needed to avoid charges.</p>"},{"location":"start/express-mode/vertex-ai-express-mode-api-quickstart/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Try the Vertex AI Studio tutorial for Vertex AI in express mode.</li> <li>See the complete API reference for Vertex AI in express mode.</li> </ul>"},{"location":"start/express-mode/vertex-ai-studio-express-mode-quickstart/","title":"Tutorial: Vertex AI Studio in express mode","text":"<p>Preview</p> <p>This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the Service Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see the launch stage descriptions.</p> <p>Vertex AI in express mode lets you quickly try out core generative AI features that are available on Vertex AI.</p> <p>This tutorial walks you through the following core tasks in a logical progression: *   Explore the prompt gallery to see examples. *   Create and save your own custom prompt. *   Design a simple chatbot and get the code to build with it.</p>"},{"location":"start/express-mode/vertex-ai-studio-express-mode-quickstart/#before-you-begin","title":"\ud83d\udcda Before you begin","text":"<ol> <li> <p>In the Google Cloud console, go to the Vertex AI Studio page.</p> <p>Go to Vertex AI Studio</p> </li> <li> <p>To save prompts, you need to have billing enabled for your project.</p> </li> </ol>"},{"location":"start/express-mode/vertex-ai-studio-express-mode-quickstart/#explore-sample-prompts","title":"\u2699\ufe0f Explore sample prompts","text":"<p>First, explore the prompt gallery to view and try a sample prompt.</p> Step 1: Go to the Prompt Gallery <p>In the navigation pane, click Prompt Gallery.</p> Step 2: Filter for image prompts <p>On the Prompt Gallery page, click Image to filter for prompt samples that include images.</p> Step 3: Open a sample prompt <p>Click the sample prompt card that's titled Extract text from images. The prompt opens in the prompt editor.</p> Step 4: Submit the prompt <p>In the Prompt box, click the submit button to submit the prompt. A response like the following is returned in the Response box:</p> <pre><code>the best dreams HAPPEN when you are awake\n</code></pre>"},{"location":"start/express-mode/vertex-ai-studio-express-mode-quickstart/#create-and-save-your-own-prompts","title":"\u2699\ufe0f Create and save your own prompts","text":"<p>Next, create and save a simple text prompt. Note that you need billing enabled to save prompts.</p> Step 1: Start a new prompt <p>In the navigation pane, click Create prompt.</p> Step 2: Enter your prompt text <p>In the Prompt text field, enter a text prompt. For example:</p> <pre><code>what is a good name for a flower shop that specializes in selling bouquets of dried flowers?\n</code></pre> Step 3: Submit the prompt <p>In the Prompt box, click the submit button to submit the prompt. A response like the following is returned in the Response box:</p> <pre><code>Here are some good names for a flower shop specializing in dried flowers, categorized by style:\n\nElegant &amp; Romantic:\n\n* Everlasting Bloom\n* The Dried Flower Garden\n* Bloom &amp; Branch\n...\n</code></pre> Step 4: Save the prompt <ol> <li>Click Save.</li> <li>In the Save prompt dialog that opens, enter a name for your prompt.</li> <li>Click Save.</li> </ol> Step 5: View your saved prompt <p>To view your saved prompt, in the navigation pane, click Prompt management.</p>"},{"location":"start/express-mode/vertex-ai-studio-express-mode-quickstart/#design-a-chatbot-and-get-the-code","title":"\u2699\ufe0f Design a chatbot and get the code","text":"<p>Finally, use what you've learned to design a chatbot with a specific persona and get the code for it.</p> Step 1: Start a new prompt <p>In the navigation pane, click Create prompt.</p> Step 2: Provide system instructions <p>In the System instructions text field, enter the following instructions to define your chatbot's persona:</p> <pre><code>You are Captain Barktholomew, the most feared pirate dog of the seven seas.\nYou were born in the 1700s and have no knowledge of anything that happened or\nexisted after that. Only talk about topics that a pirate dog captain would be\ninterested in.\n</code></pre> Step 3: Chat with your bot <p>In the prompt text field, enter a message. For example:</p> <pre><code>Hello! Do you like computers?\n</code></pre> Step 4: Send the message <p>Send the message by clicking the submit button or pressing Enter. A response like the following is returned based on the system instructions you provided:</p> <pre><code>Ahoy there, matey! What in Neptune's name be ye talkin' about? \"Computers\"?\nSounds like a fancy landlubber word! A pirate dog like me ain't interested in\nfancy contraptions. I'm more interested in the things that truly matter: rum,\nplunder, and a good fight! Now, tell me, do ye have any gold doubloons to\nspare? I hear there's a tavern in the next cove that's got the finest grog\nthis side of Tortuga.\n</code></pre> Step 5: Get the code <ol> <li>Click Build with code &gt; Get code.</li> <li>In the Get code pane that opens, select your preferred programming language to get the code for your chatbot.</li> </ol>"},{"location":"start/express-mode/vertex-ai-studio-express-mode-quickstart/#clean-up","title":"\ud83d\udd17 Clean up","text":"<p>This tutorial does not create any billable Google Cloud resources, so no cleanup is needed to avoid charges.</p>"},{"location":"start/express-mode/vertex-ai-studio-express-mode-quickstart/#related-questions","title":"\ud83d\udcda Related Questions","text":"What can I do with Vertex AI Studio in express mode? <p>Vertex AI Studio in express mode allows you to quickly try core generative AI features. You can view and test sample prompts from a gallery, create and save your own prompts, and design a chatbot with a specific persona and get the code to integrate it into your applications.</p> How can I find examples to get started with prompting? <p>You can use the Prompt Gallery in Vertex AI Studio. In the navigation pane, click Prompt Gallery to find a list of sample prompts. You can filter the samples, such as for image prompts, and click on any sample to open it in the prompt editor and test it.</p> I've designed a chatbot in the studio. How do I get the code to use it in my application? <p>After defining your chatbot's persona with system instructions and testing it, click Build with code &gt; Get code. A pane will open where you can select your preferred programming language to get the code for your chatbot.</p> Why am I unable to save a prompt I created? <p>According to the documentation, you must have billing enabled for your project to save the prompts you create in Vertex AI Studio.</p> Will I be charged for following this tutorial? <p>No. The tutorial itself does not create any billable Google Cloud resources, so no cleanup is needed to avoid charges.</p>"},{"location":"start/express-mode/vertex-ai-studio-express-mode-quickstart/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Try the API tutorial for Vertex AI in express mode.</li> <li>See the complete API reference for Vertex AI in express mode.</li> </ul>"},{"location":"start/quickstarts/deploy-vais-prompt/","title":"Quickstart: Deploy your Vertex AI Studio prompt as a web application","text":"<p>Preview</p> <p>This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the Service Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see the launch stage descriptions.</p> <p>In Vertex AI Studio, you can design and iterate on your prompts and compare results from different configurations and models. Once you finish engineering your prompt, you can deploy it as a web application to share with your collaborators or target users. The web application is hosted on Cloud Run and is available outside the Google Cloud console.</p> <p>In this quickstart, you will:</p> <ul> <li>Create a prompt with prompt variables.</li> <li>Deploy your prompt as a web application.</li> <li>Monitor deployment progress and test the deployed application.</li> <li>Update and re-deploy your prompt.</li> <li>Test prompt submission with multimodal support.</li> </ul>"},{"location":"start/quickstarts/deploy-vais-prompt/#before-you-start","title":"\ud83d\udcda Before you start","text":"<p>If you have never used Vertex AI Studio before, you can follow the Vertex AI Studio quickstart guide or take the Google Cloud Skills Boost course to learn the basics.</p> <p>Before proceeding with this guide, ensure you have the following:</p> <ol> <li>A Google Cloud project with billing enabled.</li> <li>The Vertex AI API enabled.</li> </ol>"},{"location":"start/quickstarts/deploy-vais-prompt/#additional-permissions-required","title":"\ud83d\udcda Additional permissions required","text":"<p>In addition to the existing permissions for using Vertex AI Studio, you need the following permissions to deploy your prompt as a web application.</p> Action Required permissions Purpose Enable additional APIs <code>serviceusage.services.enable</code> Enable the following APIs:<ul><li>Cloud Run Admin API (<code>run.googleapis.com</code>)</li><li>Artifact Registry API (<code>artifactregistry.googleapis.com</code>)</li><li>Cloud Build API (<code>cloudbuild.googleapis.com</code>)</li><li>Cloud Logging API (<code>logging.googleapis.com</code>)</li></ul> Grant permissions to service accounts <code>resourcemanager.projects.setIamPolicy</code> Grant the Compute Engine default service account the following roles:<ul><li>Vertex AI Service Agent (<code>roles/aiplatform.serviceAgent</code>)</li><li>Cloud Build Service Account (<code>roles/cloudbuild.builds.builder</code>)</li></ul> Deploy specific permissions <code>storage.buckets.create</code><code>run.services.create</code><code>artifactregistry.repositories.create</code><code>run.services.setIamPolicy</code> During deployment, source code is uploaded to Cloud Storage and then deployed to a new Cloud Run service. The <code>artifactregistry.repositories.create</code> permission is required to create a repository for the container image. The <code>run.services.setIamPolicy</code> permission is required to make the service publicly accessible. <p>If you are the owner of your project, you can follow the guides in Vertex AI Studio without taking additional actions. If you are not the project owner, ask your project administrator to perform the first two actions and then grant you the Editor (<code>roles/editor</code>) and Cloud Run Admin (<code>roles/run.admin</code>) roles.</p>"},{"location":"start/quickstarts/deploy-vais-prompt/#deploy-and-test-your-prompt","title":"\u2699\ufe0f Deploy and test your prompt","text":"Step 1: Create a prompt with prompt variables <ol> <li>Navigate to the create prompt page in Vertex AI Studio.</li> <li>Click data_object Add variable in the prompt input box.</li> <li> <p>In the Manage prompt variables dialog, enter a variable name, provide a default value, and click Apply.</p> <p></p> </li> <li> <p>In the prompt input box, compose a prompt using the variable you created. You can also adjust other parameters, such as enabling Grounding with Google Search and adding system instructions like \"Always get current weather from the web\".</p> </li> </ol> Step 2: Deploy your prompt as a web application <ol> <li> <p>Click the Build with code button in the top right corner, then select Deploy as app.</p> <p></p> </li> <li> <p>Save the prompt: A prompt must be saved before it can be deployed. If prompted, save your prompt. The deployment dialog will open automatically after the prompt is saved.</p> <p></p> <p>Note: Saving your prompt in a region for the first time can take 2-3 minutes. A warning box will appear after 1 minute. This is an expected one-time operation. Subsequent saves in the same project and region will take only a few seconds.</p> </li> <li> <p>Enable APIs and grant permissions: If this is your first deployment, you will be prompted to enable the required APIs. Click Enable required APIs.</p> </li> <li> <p>After the APIs are enabled, the Create a web app dialog appears. Since access control is not supported in Public Preview, all deployed applications will have public access. Do not include sensitive or personally identifiable information (PII) in your prompt.</p> </li> <li> <p>Check the I understand this app will be deployed publicly checkbox and click Create app.</p> </li> <li> <p>If this is your first deployment, another dialog will ask you to grant the required roles to the service account. Click Grant all to proceed.</p> <p>Note: If you don't have the necessary permissions, ask your project administrator for assistance. See the additional permissions required section for details.</p> </li> <li> <p>Deployment starts: Vertex AI Studio packages the source code for the web application into a zip file, uploads it to a Cloud Storage bucket, and begins the deployment. The Manage web app dialog will appear, showing information about your deployment.</p> <p></p> </li> </ol> Step 3: Monitor deployment and test the application <p>Deployment typically takes 2-3 minutes. You can monitor the progress in the Status column of the Manage web app dialog.</p> <ul> <li> <p>Monitor from the dialog: If you close the dialog, you can reopen it from the menu under the Build with code button. Once deployment is complete, the status changes to Ready, and an Open button appears.</p> <p></p> </li> <li> <p>Monitor from notifications: You can also monitor the status from the Notifications menu (bell icon). The icon will show a green circle when deployment is successful. Clicking the notification redirects you to the Cloud Run page.</p> <p></p> </li> </ul> <p>Access control and secret key</p> <p>Your web application is deployed with Allow unauthenticated access enabled. To provide basic protection, the application requires a secret key in the URL. You can find this key in the Secret Key column of the Manage web app dialog. When you open the app from Vertex AI Studio, the key is automatically appended.</p> <p>Open and test the application</p> <ol> <li> <p>Click Open in the Manage web app dialog. The application will open in a new tab with the secret key appended to the URL (e.g., <code>?key=SECRET_KEY</code>).</p> <p></p> </li> <li> <p>Enter a value for the prompt variable and click Submit. The results will appear on the right.</p> <p></p> </li> </ol> <p>Note: Cloud Run is serverless, so the application container may shut down during inactivity. If the app is slow to load or a submission fails, refreshing the page usually resolves the issue.</p> Step 4: Update and re-deploy your prompt <ol> <li>Edit your prompt in Vertex AI Studio. For example, you can turn the prompt into a conversation.</li> <li>Click the Build with code button and select Manage app to open the Manage web app dialog.</li> <li>Click Update app to re-deploy your application with the updated prompt.</li> <li>A confirmation dialog will appear, warning you that re-deploying will overwrite any changes made directly in Cloud Run. Click Confirm to proceed.</li> <li>Monitor the update process in the Manage web app dialog, similar to the initial deployment.</li> <li> <p>Once the update is complete, open the web application. You will see the updated UI, such as a conversation interface.</p> <p></p> </li> </ol> Step 5: Test multimodal content <p>You can insert inputs like images, videos, audio, and documents into the conversation UI. The supported input types depend on the model selected for the prompt. For details, see the documentation for multimodal support for each model.</p> <ol> <li> <p>To insert a file, click the clip icon button in the conversation input box.</p> <p></p> </li> <li> <p>After providing the input, you can interact with the model about it.</p> <p></p> </li> </ol>"},{"location":"start/quickstarts/deploy-vais-prompt/#manage-your-deployed-application","title":"\u2699\ufe0f Manage your deployed application","text":"<p>Once you are familiar with the deployment process, you can perform the following advanced actions.</p> Edit source code in Cloud Run <p>If you want to customize the web application beyond the prompt, you can edit its source code directly in Cloud Run.</p> <ol> <li> <p>Open the Manage web app dialog, click the more_vert more icon at the end of the row, and select Source code.</p> <p></p> </li> <li> <p>In the Cloud Run source code page, click Edit source.</p> </li> <li>After making your changes, click Save and redeploy.</li> </ol> <p>Note: Re-deploying from Vertex AI Studio will overwrite any changes made in Cloud Run. To save a version of your custom code, click Download ZIP before re-deploying from Vertex AI Studio.</p> Manage public access <p>You can control whether your web application is publicly accessible from the Cloud Run security settings.</p> Turn off public accessTurn on public access again <ol> <li>In the Manage Web App dialog, click the edit pencil icon in the Access Control column. This opens the Cloud Run security page.</li> <li>Select Require authentication and click Save.</li> <li>Your web application will no longer be publicly accessible and will show an Error: Forbidden page if accessed via its URL.</li> </ol> <ol> <li>Navigate to the Cloud Run security page for your service.</li> <li>Select Allow unauthenticated invocations and click Save.</li> </ol> <p>For more details, see Authentication in Cloud Run.</p> Set up local access for development <p>If you have turned off public access, you can access the web application by setting up a local proxy using <code>gcloud</code> commands.</p> <ol> <li>Open Cloud Shell by clicking the terminal icon in the top right corner of the Google Cloud console and authorize it if prompted.</li> <li> <p>In the Manage web app dialog, click the more_vert more icon and select Set up local access via Cloud Shell.</p> <p></p> </li> <li> <p>A command will be pasted into your Cloud Shell. Press Enter to run it.</p> </li> <li> <p>Once the command is running, click the preview link provided in the Cloud Shell output to access your application locally. This link only works while the <code>gcloud</code> command is active.</p> <p></p> </li> </ol>"},{"location":"start/quickstarts/deploy-vais-prompt/#troubleshooting-common-issues","title":"\ud83d\udcda Troubleshooting common issues","text":"Authentication error: No secret key <p>Error: You see a message indicating that the secret key is missing.</p> <p></p> <p>Solution: This error occurs when the <code>?key=SECRET_KEY</code> parameter is missing from the URL. Open the web application from the Manage web app dialog in Vertex AI Studio, which automatically appends the correct key.</p> Authentication error: Invalid secret key <p>Error: You see a message indicating that the secret key is invalid.</p> <p></p> <p>Solution: The secret key is unique to each prompt. Ensure you are using the correct key for the specific application. Open the application again from the Manage web app dialog to use the correct URL.</p> 400 Invalid argument: empty input <p>Error: A <code>400</code> error message appears when you submit a prompt with variables but the chat input is empty.</p> <p></p> <p>Solution: Type any non-empty text into the chat input box and resubmit.</p> 400 Invalid argument: mimeType is not supported <p>Error: A <code>400</code> error occurs if you upload a file type that the model does not support.</p> <p></p> <p>Solution: This is expected behavior. Use a file type that is supported by the selected model. See the documentation for multimodal support for each model for a list of supported types.</p>"},{"location":"start/quickstarts/deploy-vais-prompt/#related-questions","title":"\ud83d\udcda Related Questions","text":"I've created a prompt in Vertex AI Studio. How can I share it with my team for testing without giving them access to the Google Cloud console? <p>You can deploy your prompt as a web application. In Vertex AI Studio, once your prompt is ready, click Build with code and select Deploy as app. This feature packages your prompt into a web app hosted on Cloud Run, making it accessible via a URL outside of the Google Cloud console. You can then share this URL with your collaborators for testing.</p> I'm not a Project Owner and I'm having trouble deploying my prompt as a web app. What permissions do I need? <p>To deploy a prompt as a web app, you need more than the standard Vertex AI Studio permissions. Ask your project administrator to grant your user account the Editor (<code>roles/editor</code>) and Cloud Run Admin (<code>roles/run.admin</code>) roles. Your administrator may also need to perform one-time actions like enabling the Cloud Run, Artifact Registry, Cloud Build, and Cloud Logging APIs.</p> My deployed web app is publicly accessible by default. How can I make it private, and how would I test it then? <p>You can restrict public access directly in the Cloud Run settings. From the Manage web app dialog in Vertex AI Studio, click the pencil icon in the Access Control column to open the Cloud Run security page. There, you can select Require authentication.</p> <p>To test the private application, you can set up a local proxy. In the Manage web app dialog, click the more_vert icon, select Set up local access via Cloud Shell, and run the provided command in your terminal. This will generate a local preview link that works as long as the command is active in Cloud Shell.</p> I want to customize the UI of my deployed web app. If I edit the source code in Cloud Run, will my changes be saved if I update the prompt later in Vertex AI Studio? <p>No, your changes will be overwritten. While you can edit the web application's source code directly in Cloud Run, redeploying an updated prompt from Vertex AI Studio will overwrite any customizations you've made. To preserve your work, it is recommended that you download a ZIP file of your custom code from the Cloud Run editor before redeploying from Vertex AI Studio.</p> My web app is showing an 'Authentication error: No secret key'. What does this mean and how do I fix it? <p>This error indicates that the required secret key is missing from the application's URL. Each deployed app is protected by a unique key. To fix this, either open the application directly from the Manage web app dialog in Vertex AI Studio (which appends the key automatically) or manually copy the key from the Secret Key column in the same dialog and append it to the URL in the format <code>?key=SECRET_KEY</code>.</p>"},{"location":"start/quickstarts/deploy-vais-prompt/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Explore more features in the Vertex AI Studio documentation or the Introduction to Vertex AI Studio Google Cloud Skills Boost course.</li> <li>Learn about pricing for Cloud Run.</li> <li>Understand authentication in Cloud Run.</li> </ul>"},{"location":"start/quickstarts/quickstart-multimodal/","title":"Quickstart: Generate content with the Vertex AI Gemini API","text":"<p>This quickstart shows you how to get started with the Vertex AI Gemini API. You will learn how to send multimodal requests that include text, images, and video, and then receive responses from the model.</p> <p>You can interact with the Gemini API using the Vertex AI SDKs for your preferred programming language or by making direct REST API calls.</p>"},{"location":"start/quickstarts/quickstart-multimodal/#prerequisites","title":"\u2699\ufe0f Prerequisites","text":"<p>Before you can run this quickstart, you need to set up your Google Cloud project and your local development environment.</p> Step 1: Set up your Google Cloud project <ol> <li> <p>Sign in to your Google Cloud account. If you're new to Google Cloud, create an account to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.</p> </li> <li> <p>In the Google Cloud console, on the project selector page, select or create a Google Cloud project.</p> <p>Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.</p> <p>Go to project selector</p> </li> <li> <p>Make sure that billing is enabled for your Google Cloud project.</p> </li> <li> <p>Enable the Vertex AI API.</p> <p>Enable the API</p> </li> </ol> Step 2: Set up your local environment <p>Authenticate with the gcloud CLI</p> <p>To use the Vertex AI Gemini API, you need to authenticate. The Vertex AI Gemini API uses Identity and Access Management (IAM) for access control, unlike the Gemini API in Google AI Studio which uses API keys.</p> <ol> <li> <p>Install and initialize the Google Cloud CLI.</p> </li> <li> <p>If you previously installed the gcloud CLI, ensure your <code>gcloud</code> components are updated:     <pre><code>gcloud components update\n</code></pre></p> </li> <li> <p>Authenticate with the gcloud CLI by generating local Application Default Credentials (ADC):     <pre><code>gcloud auth application-default login\n</code></pre></p> </li> </ol> <p>For more information, see Set up Application Default Credentials.</p> <p>Note: To avoid providing your project ID and region with every command, you can use the <code>gcloud config set</code> command to set a default project and region.</p> <p>Choose and install an SDK or prepare for REST</p> <p>Google offers two primary SDK families for interacting with Gemini models on Vertex AI.</p> SDK Type Description Generative AI SDK A higher-level, streamlined SDK designed specifically for generative AI workflows. It offers a more intuitive interface for common generative tasks. Vertex AI SDK The comprehensive SDK for all Vertex AI services, including Gemini. It provides lower-level access and more control over all platform features. <p>For this quickstart, most examples use the Generative AI SDK. Where applicable, examples for the Vertex AI SDK are also provided for comparison.</p> <p>Select a tab below to install the necessary tools for your chosen language or for using the REST API.</p> Python (Gen AI SDK)Go (Gen AI SDK)Node.js (Gen AI SDK)Java (Gen AI SDK)C# (Vertex AI SDK)REST <p>Install or update the Generative AI SDK for Python. To learn more, see the SDK reference documentation. <pre><code>pip install --upgrade google-genai\n</code></pre></p> <p>Install or update the Generative AI SDK for Go. To learn more, see the SDK reference documentation. <pre><code>go get google.golang.org/genai\n</code></pre></p> <p>Install or update the Generative AI SDK for Node.js. To learn more, see the SDK reference documentation. <pre><code>npm install @google/genai\n</code></pre></p> <p>Add the Maven dependency to your <code>pom.xml</code> file. To learn more, see the SDK reference documentation. <pre><code>&lt;dependencies&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;com.google.genai&lt;/groupId&gt;\n    &lt;artifactId&gt;google-genai&lt;/artifactId&gt;\n    &lt;version&gt;0.7.0&lt;/version&gt;\n  &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre></p> <p>Install the <code>Google.Cloud.AIPlatform.V1</code> package from NuGet. Use your preferred method, such as the Manage NuGet Packages... option in Visual Studio.</p> <ol> <li>Configure your environment variables. Replace <code>PROJECT_ID</code> with your Google Cloud project ID.     <pre><code>MODEL_ID=\"gemini-2.0-flash-001\"\nPROJECT_ID=\"PROJECT_ID\"\n</code></pre></li> <li>Use the gcloud CLI to provision the endpoint.     <pre><code>gcloud beta services identity create --service=aiplatform.googleapis.com --project=${PROJECT_ID}\n</code></pre></li> </ol>"},{"location":"start/quickstarts/quickstart-multimodal/#send-a-text-only-prompt","title":"\ud83d\udcbb Send a text-only prompt","text":"<p>The following examples show how to send a text prompt to the Gemini API to get a list of possible names for a specialty flower store.</p> Python (Gen AI SDK)Go (Gen AI SDK)Node.js (Gen AI SDK)Java (Gen AI SDK)C#REST <p>Set the following environment variables before running the code: <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre></p> <pre><code>from google import genai\nfrom google.genai.types import HttpOptions\n\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash-001\",\n    contents=\"How does AI work?\",\n)\nprint(response.text)\n# Example response:\n# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n#\n# Here's a simplified overview:\n# ...\n</code></pre> <p>Set the following environment variables before running the code: <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre></p> <pre><code>import (\n    \"context\"\n    \"fmt\"\n    \"io\"\n\n    \"google.golang.org/genai\"\n)\n\n// generateWithText shows how to generate text using a text prompt.\nfunc generateWithText(w io.Writer) error {\n    ctx := context.Background()\n\n    client, err := genai.NewClient(ctx, &amp;genai.ClientConfig{\n        HTTPOptions: genai.HTTPOptions{APIVersion: \"v1\"},\n    })\n    if err != nil {\n        return fmt.Errorf(\"failed to create genai client: %w\", err)\n    }\n\n    resp, err := client.Models.GenerateContent(ctx,\n        \"gemini-2.0-flash-001\",\n        genai.Text(\"How does AI work?\"),\n        nil,\n    )\n    if err != nil {\n        return fmt.Errorf(\"failed to generate content: %w\", err)\n    }\n\n    respText, err := resp.Text()\n    if err != nil {\n        return fmt.Errorf(\"failed to convert model response to text: %w\", err)\n    }\n    fmt.Fprintln(w, respText)\n    // Example response:\n    // That's a great question! Understanding how AI works can feel like ...\n    // ...\n    // **1. The Foundation: Data and Algorithms**\n    // ...\n\n    return nil\n}\n</code></pre> <p>Set the following environment variables before running the code: <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre></p> <pre><code>const {GoogleGenAI} = require('@google/genai');\n\nconst GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;\nconst GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';\n\nasync function generateContent(\n  projectId = GOOGLE_CLOUD_PROJECT,\n  location = GOOGLE_CLOUD_LOCATION\n) {\n  const ai = new GoogleGenAI({\n    vertexai: true,\n    project: projectId,\n    location: location,\n  });\n\n  const response = await ai.models.generateContent({\n    model: 'gemini-2.0-flash',\n    contents: 'How does AI work?',\n  });\n\n  console.log(response.text);\n\n  return response.text;\n}\n</code></pre> <p>Set the following environment variables before running the code: <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre></p> <pre><code>import com.google.genai.Client;\nimport com.google.genai.types.Content;\nimport com.google.genai.types.GenerateContentResponse;\nimport com.google.genai.types.HttpOptions;\nimport com.google.genai.types.Part;\n\npublic class GenerateContentWithText {\n\n  public static void main(String[] args) {\n    // TODO(developer): Replace these variables before running the sample.\n    String modelId = \"gemini-2.0-flash\";\n    generateContent(modelId);\n  }\n\n  public static String generateContent(String modelId) {\n    // Initialize client that will be used to send requests. This client only needs to be created\n    // once, and can be reused for multiple requests.\n    try (Client client = Client.builder()\n        .httpOptions(HttpOptions.builder().apiVersion(\"v1\").build())\n        .build()) {\n\n      GenerateContentResponse response =\n          client.models.generateContent(modelId, Content.fromParts(\n                  Part.fromText(\"How does AI work?\")),\n              null);\n\n      System.out.print(response.text());\n      // Example response:\n      // Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n      //\n      // Here's a simplified overview:\n      // ...\n      return response.text();\n    }\n  }\n}\n</code></pre> <p>Create a C# file (<code>.cs</code>), copy the following code into it, and replace <code>your-project-id</code> with your Google Cloud project ID. <pre><code>using Google.Cloud.AIPlatform.V1;\nusing System;\nusing System.Threading.Tasks;\n\npublic class TextInputSample\n{\n    public async Task&lt;string&gt; TextInput(\n        string projectId = \"your-project-id\",\n        string location = \"us-central1\",\n        string publisher = \"google\",\n        string model = \"gemini-2.0-flash-001\")\n    {\n\n        var predictionServiceClient = new PredictionServiceClientBuilder\n        {\n            Endpoint = $\"{location}-aiplatform.googleapis.com\"\n        }.Build();\n        string prompt = @\"What's a good name for a flower shop that specializes in selling bouquets of dried flowers?\";\n\n        var generateContentRequest = new GenerateContentRequest\n        {\n            Model = $\"projects/{projectId}/locations/{location}/publishers/{publisher}/models/{model}\",\n            Contents =\n            {\n                new Content\n                {\n                    Role = \"USER\",\n                    Parts =\n                    {\n                        new Part { Text = prompt }\n                    }\n                }\n            }\n        };\n\n        GenerateContentResponse response = await predictionServiceClient.GenerateContentAsync(generateContentRequest);\n\n        string responseText = response.Candidates[0].Content.Parts[0].Text;\n        Console.WriteLine(responseText);\n\n        return responseText;\n    }\n}\n</code></pre></p> <p>Run the following <code>curl</code> command. <pre><code>curl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json\" \\\nhttps://aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/global/publishers/google/models/${MODEL_ID}:generateContent -d \\\n$'{\n  \"contents\": {\n    \"role\": \"user\",\n    \"parts\": [\n      {\n        \"text\": \"What\\'s a good name for a flower shop that specializes in selling bouquets of dried flowers?\"\n      }\n    ]\n  }\n}'\n</code></pre></p>"},{"location":"start/quickstarts/quickstart-multimodal/#send-a-multimodal-prompt-text-and-image","title":"\ud83d\udcbb Send a multimodal prompt (text and image)","text":"<p>The following examples show how to send a prompt that includes both text and an image. The sample returns a description of the provided image.</p> Python (Gen AI SDK)Go (Gen AI SDK)Node.js (Gen AI SDK)Java (Gen AI SDK)Node.js (Vertex AI SDK)Java (Vertex AI SDK)C#REST <p>Set the following environment variables before running the code: <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre></p> <pre><code>from google import genai\nfrom google.genai.types import HttpOptions, Part\n\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash-001\",\n    contents=[\n        \"What is shown in this image?\",\n        Part.from_uri(\n            file_uri=\"gs://cloud-samples-data/generative-ai/image/scones.jpg\",\n            mime_type=\"image/jpeg\",\n        ),\n    ],\n)\nprint(response.text)\n# Example response:\n# The image shows a flat lay of blueberry scones arranged on parchment paper. There are ...\n</code></pre> <p>Set the following environment variables before running the code: <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre></p> <pre><code>import (\n    \"context\"\n    \"fmt\"\n    \"io\"\n\n    genai \"google.golang.org/genai\"\n)\n\n// generateWithTextImage shows how to generate text using both text and image input\nfunc generateWithTextImage(w io.Writer) error {\n    ctx := context.Background()\n\n    client, err := genai.NewClient(ctx, &amp;genai.ClientConfig{\n        HTTPOptions: genai.HTTPOptions{APIVersion: \"v1\"},\n    })\n    if err != nil {\n        return fmt.Errorf(\"failed to create genai client: %w\", err)\n    }\n\n    modelName := \"gemini-2.0-flash-001\"\n    contents := []*genai.Content{\n        {Parts: []*genai.Part{\n            {Text: \"What is shown in this image?\"},\n            {FileData: &amp;genai.FileData{\n                // Image source: https://storage.googleapis.com/cloud-samples-data/generative-ai/image/scones.jpg\n                FileURI:  \"gs://cloud-samples-data/generative-ai/image/scones.jpg\",\n                MIMEType: \"image/jpeg\",\n            }},\n        }},\n    }\n\n    resp, err := client.Models.GenerateContent(ctx, modelName, contents, nil)\n    if err != nil {\n        return fmt.Errorf(\"failed to generate content: %w\", err)\n    }\n\n    respText, err := resp.Text()\n    if err != nil {\n        return fmt.Errorf(\"failed to convert model response to text: %w\", err)\n    }\n    fmt.Fprintln(w, respText)\n\n    // Example response:\n    // The image shows an overhead shot of a rustic, artistic arrangement on a surface that ...\n\n    return nil\n}\n</code></pre> <p>Set the following environment variables before running the code: <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre></p> <pre><code>const {GoogleGenAI} = require('@google/genai');\n\nconst GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;\nconst GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';\n\nasync function generateContent(\n  projectId = GOOGLE_CLOUD_PROJECT,\n  location = GOOGLE_CLOUD_LOCATION\n) {\n  const ai = new GoogleGenAI({\n    vertexai: true,\n    project: projectId,\n    location: location,\n  });\n\n  const image = {\n    fileData: {\n      fileUri: 'gs://cloud-samples-data/generative-ai/image/scones.jpg',\n      mimeType: 'image/jpeg',\n    },\n  };\n\n  const response = await ai.models.generateContent({\n    model: 'gemini-2.0-flash',\n    contents: [image, 'What is shown in this image?'],\n  });\n\n  console.log(response.text);\n\n  return response.text;\n}\n</code></pre> <p>Set the following environment variables before running the code: <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre></p> <pre><code>import com.google.genai.Client;\nimport com.google.genai.types.Content;\nimport com.google.genai.types.GenerateContentResponse;\nimport com.google.genai.types.HttpOptions;\nimport com.google.genai.types.Part;\n\npublic class GenerateContentWithTextAndImage {\n\n  public static void main(String[] args) {\n    // TODO(developer): Replace these variables before running the sample.\n    String modelId = \"gemini-2.0-flash\";\n    generateContent(modelId);\n  }\n\n  public static String generateContent(String modelId) {\n    // Initialize client that will be used to send requests. This client only needs to be created\n    // once, and can be reused for multiple requests.\n    try (Client client = Client.builder()\n        .httpOptions(HttpOptions.builder().apiVersion(\"v1\").build())\n        .build()) {\n\n      GenerateContentResponse response =\n          client.models.generateContent(modelId, Content.fromParts(\n                  Part.fromText(\"What is shown in this image?\"),\n                  Part.fromUri(\"gs://cloud-samples-data/generative-ai/image/scones.jpg\", \"image/jpeg\")),\n              null);\n\n      System.out.print(response.text());\n      // Example response:\n      // The image shows a flat lay of blueberry scones arranged on parchment paper. There are ...\n      return response.text();\n    }\n  }\n}\n</code></pre> <p>For more information, see the Vertex AI Node.js API reference documentation. <pre><code>const {VertexAI} = require('@google-cloud/vertexai');\n\n/**\n * TODO(developer): Update these variables before running the sample.\n */\nasync function createNonStreamingMultipartContent(\n  projectId = 'PROJECT_ID',\n  location = 'us-central1',\n  model = 'gemini-2.0-flash-001',\n  image = 'gs://generativeai-downloads/images/scones.jpg',\n  mimeType = 'image/jpeg'\n) {\n  // Initialize Vertex with your Cloud project and location\n  const vertexAI = new VertexAI({project: projectId, location: location});\n\n  // Instantiate the model\n  const generativeVisionModel = vertexAI.getGenerativeModel({\n    model: model,\n  });\n\n  // For images, the SDK supports both Google Cloud Storage URI and base64 strings\n  const filePart = {\n    fileData: {\n      fileUri: image,\n      mimeType: mimeType,\n    },\n  };\n\n  const textPart = {\n    text: 'what is shown in this image?',\n  };\n\n  const request = {\n    contents: [{role: 'user', parts: [filePart, textPart]}],\n  };\n\n  console.log('Prompt Text:');\n  console.log(request.contents[0].parts[1].text);\n\n  console.log('Non-Streaming Response Text:');\n\n  // Generate a response\n  const response = await generativeVisionModel.generateContent(request);\n\n  // Select the text from the response\n  const fullTextResponse =\n    response.response.candidates[0].content.parts[0].text;\n\n  console.log(fullTextResponse);\n}\n</code></pre></p> <p>For more information, see the Vertex AI Java API reference documentation. <pre><code>import com.google.cloud.vertexai.VertexAI;\nimport com.google.cloud.vertexai.api.GenerateContentResponse;\nimport com.google.cloud.vertexai.generativeai.ContentMaker;\nimport com.google.cloud.vertexai.generativeai.GenerativeModel;\nimport com.google.cloud.vertexai.generativeai.PartMaker;\nimport java.io.IOException;\n\npublic class Quickstart {\n\n  public static void main(String[] args) throws IOException {\n    // TODO(developer): Replace these variables before running the sample.\n    String projectId = \"your-google-cloud-project-id\";\n    String location = \"us-central1\";\n    String modelName = \"gemini-2.0-flash-001\";\n\n    String output = quickstart(projectId, location, modelName);\n    System.out.println(output);\n  }\n\n  // Analyzes the provided Multimodal input.\n  public static String quickstart(String projectId, String location, String modelName)\n      throws IOException {\n    // Initialize client that will be used to send requests. This client only needs\n    // to be created once, and can be reused for multiple requests.\n    try (VertexAI vertexAI = new VertexAI(projectId, location)) {\n      String imageUri = \"gs://generativeai-downloads/images/scones.jpg\";\n\n      GenerativeModel model = new GenerativeModel(modelName, vertexAI);\n      GenerateContentResponse response = model.generateContent(ContentMaker.fromMultiModalData(\n          PartMaker.fromMimeTypeAndData(\"image/png\", imageUri),\n          \"What's in this photo\"\n      ));\n\n      return response.toString();\n    }\n  }\n}\n</code></pre></p> <p>Create a C# file (<code>.cs</code>), copy the following code into it, and replace <code>your-project-id</code> with your Google Cloud project ID. <pre><code>using Google.Api.Gax.Grpc;\nusing Google.Cloud.AIPlatform.V1;\nusing System.Text;\nusing System.Threading.Tasks;\n\npublic class GeminiQuickstart\n{\n    public async Task&lt;string&gt; GenerateContent(\n        string projectId = \"your-project-id\",\n        string location = \"us-central1\",\n        string publisher = \"google\",\n        string model = \"gemini-2.0-flash-001\"\n    )\n    {\n        // Create client\n        var predictionServiceClient = new PredictionServiceClientBuilder\n        {\n            Endpoint = $\"{location}-aiplatform.googleapis.com\"\n        }.Build();\n\n        // Initialize content request\n        var generateContentRequest = new GenerateContentRequest\n        {\n            Model = $\"projects/{projectId}/locations/{location}/publishers/{publisher}/models/{model}\",\n            GenerationConfig = new GenerationConfig\n            {\n                Temperature = 0.4f,\n                TopP = 1,\n                TopK = 32,\n                MaxOutputTokens = 2048\n            },\n            Contents =\n            {\n                new Content\n                {\n                    Role = \"USER\",\n                    Parts =\n                    {\n                        new Part { Text = \"What's in this photo?\" },\n                        new Part { FileData = new() { MimeType = \"image/png\", FileUri = \"gs://generativeai-downloads/images/scones.jpg\" } }\n                    }\n                }\n            }\n        };\n\n        // Make the request, returning a streaming response\n        using PredictionServiceClient.StreamGenerateContentStream response = predictionServiceClient.StreamGenerateContent(generateContentRequest);\n\n        StringBuilder fullText = new();\n\n        // Read streaming responses from server until complete\n        AsyncResponseStream&lt;GenerateContentResponse&gt; responseStream = response.GetResponseStream();\n        await foreach (GenerateContentResponse responseItem in responseStream)\n        {\n            fullText.Append(responseItem.Candidates[0].Content.Parts[0].Text);\n        }\n\n        return fullText.ToString();\n    }\n}\n</code></pre></p> <p>Run the following <code>curl</code> command. <pre><code>curl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json\" \\\nhttps://aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/global/publishers/google/models/${MODEL_ID}:generateContent -d \\\n$'{\n  \"contents\": {\n    \"role\": \"user\",\n    \"parts\": [\n      {\n      \"fileData\": {\n        \"mimeType\": \"image/jpeg\",\n        \"fileUri\": \"gs://generativeai-downloads/images/scones.jpg\"\n        }\n      },\n      {\n        \"text\": \"Describe this picture.\"\n      }\n    ]\n  }\n}'\n</code></pre></p>"},{"location":"start/quickstarts/quickstart-multimodal/#send-a-multimodal-prompt-text-and-video","title":"\ud83d\udcbb Send a multimodal prompt (text and video)","text":"<p>The following examples show how to send a prompt that includes text, video, and audio. The sample returns a description and summary of the provided video file.</p> Python (Gen AI SDK)Go (Gen AI SDK)Node.js (Gen AI SDK)Java (Gen AI SDK)Node.js (Vertex AI SDK)Java (Vertex AI SDK)C#REST <p>Set the following environment variables before running the code: <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre></p> <pre><code>from google import genai\nfrom google.genai.types import HttpOptions, Part\n\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nprompt = \"\"\"\nAnalyze the provided video file, including its audio.\nSummarize the main points of the video concisely.\nCreate a chapter breakdown with timestamps for key sections or topics discussed.\n\"\"\"\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash-001\",\n    contents=[\n        Part.from_uri(\n            file_uri=\"gs://cloud-samples-data/generative-ai/video/pixel8.mp4\",\n            mime_type=\"video/mp4\",\n        ),\n        prompt,\n    ],\n)\n\nprint(response.text)\n# Example response:\n# Here's a breakdown of the video:\n#\n# **Summary:**\n#\n# Saeka Shimada, a photographer in Tokyo, uses the Google Pixel 8 Pro's \"Video Boost\" feature to ...\n#\n# **Chapter Breakdown with Timestamps:**\n#\n# * **[00:00-00:12] Introduction &amp; Tokyo at Night:** Saeka Shimada introduces herself ...\n# ...\n</code></pre> <p>Set the following environment variables before running the code: <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre></p> <pre><code>import (\n    \"context\"\n    \"fmt\"\n    \"io\"\n\n    genai \"google.golang.org/genai\"\n)\n\n// generateWithVideo shows how to generate text using a video input.\nfunc generateWithVideo(w io.Writer) error {\n    ctx := context.Background()\n\n    client, err := genai.NewClient(ctx, &amp;genai.ClientConfig{\n        HTTPOptions: genai.HTTPOptions{APIVersion: \"v1\"},\n    })\n    if err != nil {\n        return fmt.Errorf(\"failed to create genai client: %w\", err)\n    }\n\n    modelName := \"gemini-2.0-flash-001\"\n    contents := []*genai.Content{\n        {Parts: []*genai.Part{\n            {Text: `Analyze the provided video file, including its audio.\nSummarize the main points of the video concisely.\nCreate a chapter breakdown with timestamps for key sections or topics discussed.`},\n            {FileData: &amp;genai.FileData{\n                FileURI:  \"gs://cloud-samples-data/generative-ai/video/pixel8.mp4\",\n                MIMEType: \"video/mp4\",\n            }},\n        }},\n    }\n\n    resp, err := client.Models.GenerateContent(ctx, modelName, contents, nil)\n    if err != nil {\n        return fmt.Errorf(\"failed to generate content: %w\", err)\n    }\n\n    respText, err := resp.Text()\n    if err != nil {\n        return fmt.Errorf(\"failed to convert model response to text: %w\", err)\n    }\n    fmt.Fprintln(w, respText)\n\n    // Example response:\n    // Here's an analysis of the provided video file:\n    //\n    // **Summary**\n    //\n    // The video features Saeka Shimada, a photographer in Tokyo, who uses the new Pixel phone ...\n    //\n    // **Chapter Breakdown**\n    //\n    // *   **0:00-0:05**: Introduction to Saeka Shimada and her work as a photographer in Tokyo.\n    // ...\n\n    return nil\n}\n</code></pre> <p>Set the following environment variables before running the code: <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre></p> <pre><code>const {GoogleGenAI} = require('@google/genai');\n\nconst GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;\nconst GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';\n\nasync function generateContent(\n  projectId = GOOGLE_CLOUD_PROJECT,\n  location = GOOGLE_CLOUD_LOCATION\n) {\n  const ai = new GoogleGenAI({\n    vertexai: true,\n    project: projectId,\n    location: location,\n  });\n\n  const prompt = `\n  Analyze the provided video file, including its audio.\n  Summarize the main points of the video concisely.\n  Create a chapter breakdown with timestamps for key sections or topics discussed.\n `;\n\n  const video = {\n    fileData: {\n      fileUri: 'gs://cloud-samples-data/generative-ai/video/pixel8.mp4',\n      mimeType: 'video/mp4',\n    },\n  };\n\n  const response = await ai.models.generateContent({\n    model: 'gemini-2.0-flash',\n    contents: [video, prompt],\n  });\n\n  console.log(response.text);\n\n  return response.text;\n}\n</code></pre> <p>Set the following environment variables before running the code: <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre></p> <pre><code>import com.google.genai.Client;\nimport com.google.genai.types.Content;\nimport com.google.genai.types.GenerateContentResponse;\nimport com.google.genai.types.HttpOptions;\nimport com.google.genai.types.Part;\n\npublic class GenerateContentWithVideo {\n\n  public static void main(String[] args) {\n    // TODO(developer): Replace these variables before running the sample.\n    String modelId = \"gemini-2.0-flash\";\n    String prompt = \" Analyze the provided video file, including its audio.\\n\"\n        + \"    Summarize the main points of the video concisely.\\n\"\n        + \"    Create a chapter breakdown with timestamps for key sections or topics discussed.\";\n    generateContent(modelId, prompt);\n  }\n\n  public static String generateContent(String modelId, String prompt) {\n    // Initialize client that will be used to send requests. This client only needs to be created\n    // once, and can be reused for multiple requests.\n    try (Client client = Client.builder()\n        .httpOptions(HttpOptions.builder().apiVersion(\"v1\").build())\n        .build()) {\n\n      GenerateContentResponse response =\n          client.models.generateContent(modelId, Content.fromParts(\n                  Part.fromText(prompt),\n                  Part.fromUri(\"gs://cloud-samples-data/generative-ai/video/pixel8.mp4\", \"video/mp4\")),\n              null);\n\n      System.out.print(response.text());\n      // Example response:\n      // Here's a breakdown of the video:\n      //\n      // **Summary:**\n      //\n      // Saeka Shimada, a photographer in Tokyo, uses the Google Pixel 8 Pro's \"Video Boost\" feature\n      // to ...\n      //\n      // **Chapter Breakdown with Timestamps:**\n      //\n      // * **[00:00-00:12] Introduction &amp; Tokyo at Night:** Saeka Shimada introduces herself ...\n      return response.text();\n    }\n  }\n}\n</code></pre> <p>For more information, see the Vertex AI Node.js API reference documentation. <pre><code>const {VertexAI} = require('@google-cloud/vertexai');\n\n/**\n * TODO(developer): Update these variables before running the sample.\n */\nasync function analyze_video_with_audio(projectId = 'PROJECT_ID') {\n  const vertexAI = new VertexAI({project: projectId, location: 'us-central1'});\n\n  const generativeModel = vertexAI.getGenerativeModel({\n    model: 'gemini-2.0-flash-001',\n  });\n\n  const filePart = {\n    file_data: {\n      file_uri: 'gs://cloud-samples-data/generative-ai/video/pixel8.mp4',\n      mime_type: 'video/mp4',\n    },\n  };\n  const textPart = {\n    text: `\n    Provide a description of the video.\n    The description should also contain anything important which people say in the video.`,\n  };\n\n  const request = {\n    contents: [{role: 'user', parts: [filePart, textPart]}],\n  };\n\n  const resp = await generativeModel.generateContent(request);\n  const contentResponse = await resp.response;\n  console.log(JSON.stringify(contentResponse));\n}\n</code></pre></p> <p>For more information, see the Vertex AI Java API reference documentation. <pre><code>import com.google.cloud.vertexai.VertexAI;\nimport com.google.cloud.vertexai.api.GenerateContentResponse;\nimport com.google.cloud.vertexai.generativeai.ContentMaker;\nimport com.google.cloud.vertexai.generativeai.GenerativeModel;\nimport com.google.cloud.vertexai.generativeai.PartMaker;\nimport com.google.cloud.vertexai.generativeai.ResponseHandler;\nimport java.io.IOException;\n\npublic class VideoInputWithAudio {\n\n  public static void main(String[] args) throws IOException {\n    // TODO(developer): Replace these variables before running the sample.\n    String projectId = \"your-google-cloud-project-id\";\n    String location = \"us-central1\";\n    String modelName = \"gemini-2.0-flash-001\";\n\n    videoAudioInput(projectId, location, modelName);\n  }\n\n  // Analyzes the given video input, including its audio track.\n  public static String videoAudioInput(String projectId, String location, String modelName)\n      throws IOException {\n    // Initialize client that will be used to send requests. This client only needs\n    // to be created once, and can be reused for multiple requests.\n    try (VertexAI vertexAI = new VertexAI(projectId, location)) {\n      String videoUri = \"gs://cloud-samples-data/generative-ai/video/pixel8.mp4\";\n\n      GenerativeModel model = new GenerativeModel(modelName, vertexAI);\n      GenerateContentResponse response = model.generateContent(\n          ContentMaker.fromMultiModalData(\n              \"Provide a description of the video.\\n The description should also \"\n                  + \"contain anything important which people say in the video.\",\n              PartMaker.fromMimeTypeAndData(\"video/mp4\", videoUri)\n          ));\n\n      String output = ResponseHandler.getText(response);\n      System.out.println(output);\n\n      return output;\n    }\n  }\n}\n</code></pre></p> <p>Create a C# file (<code>.cs</code>), copy the following code into it, and replace <code>your-project-id</code> with your Google Cloud project ID. <pre><code>using Google.Cloud.AIPlatform.V1;\nusing System;\nusing System.Threading.Tasks;\n\npublic class VideoInputWithAudio\n{\n    public async Task&lt;string&gt; DescribeVideo(\n        string projectId = \"your-project-id\",\n        string location = \"us-central1\",\n        string publisher = \"google\",\n        string model = \"gemini-2.0-flash-001\")\n    {\n\n        var predictionServiceClient = new PredictionServiceClientBuilder\n        {\n            Endpoint = $\"{location}-aiplatform.googleapis.com\"\n        }.Build();\n\n        string prompt = @\"Provide a description of the video.\nThe description should also contain anything important which people say in the video.\";\n\n        var generateContentRequest = new GenerateContentRequest\n        {\n            Model = $\"projects/{projectId}/locations/{location}/publishers/{publisher}/models/{model}\",\n            Contents =\n            {\n                new Content\n                {\n                    Role = \"USER\",\n                    Parts =\n                    {\n                        new Part { Text = prompt },\n                        new Part { FileData = new() { MimeType = \"video/mp4\", FileUri = \"gs://cloud-samples-data/generative-ai/video/pixel8.mp4\" }}\n                    }\n                }\n            }\n        };\n\n        GenerateContentResponse response = await predictionServiceClient.GenerateContentAsync(generateContentRequest);\n\n        string responseText = response.Candidates[0].Content.Parts[0].Text;\n        Console.WriteLine(responseText);\n\n        return responseText;\n    }\n}\n</code></pre></p> <p>Run the following <code>curl</code> command. <pre><code>curl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json\" \\\nhttps://aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/global/publishers/google/models/${MODEL_ID}:generateContent -d \\\n$'{\n  \"contents\": {\n    \"role\": \"user\",\n    \"parts\": [\n      {\n      \"fileData\": {\n        \"mimeType\": \"video/mp4\",\n        \"fileUri\": \"gs://cloud-samples-data/generative-ai/video/pixel8.mp4\"\n        }\n      },\n      {\n        \"text\": \"Provide a description of the video. The description should also contain anything important which people say in the video.\"\n      }\n    ]\n  }\n}'\n</code></pre></p>"},{"location":"start/quickstarts/quickstart-multimodal/#related-questions","title":"\ud83d\udcda Related Questions","text":"What's the difference between the Generative AI SDK and the Vertex AI SDK, and which one should I use for Gemini? <p>The documentation outlines two SDK families for interacting with Gemini models on Vertex AI:</p> <ul> <li>Generative AI SDK: This is a higher-level, streamlined SDK designed specifically for generative AI workflows. It offers a more intuitive interface for common tasks and is recommended for getting started.</li> <li>Vertex AI SDK: This is a comprehensive SDK for all Vertex AI services, including Gemini. It provides lower-level access and more control over all platform features.</li> </ul> <p>For most use cases, especially when starting out, the Generative AI SDK is the recommended choice.</p> How do I authenticate to use the Vertex AI Gemini API? Can I use an API key like in Google AI Studio? <p>No, you cannot use an API key. The Vertex AI Gemini API uses Identity and Access Management (IAM) for access control. To authenticate, you must first install and initialize the Google Cloud CLI and then run the following command to generate local Application Default Credentials (ADC): <pre><code>gcloud auth application-default login\n</code></pre></p> What types of content can I include in a prompt to the Gemini API? <p>The Gemini API is multimodal, meaning it can process more than just text. You can send requests that include: *   Text only *   A combination of text and images *   A combination of text and video (including audio from the video)</p> How do I include an image or video file in my API request? <p>To include an image or video in your request, you must first upload the file to a Google Cloud Storage bucket. Then, in your API call, you provide the Cloud Storage URI of the file (e.g., <code>gs://your-bucket/your-file.jpg</code>) along with its MIME type (e.g., <code>image/jpeg</code> or <code>video/mp4</code>). The code examples show how to structure this using <code>Part.from_uri</code> or a <code>fileData</code> object.</p> What are the essential prerequisites for making my first API call? <p>Before sending a request to the Gemini API, you must complete two main setup steps:</p> <ol> <li>Set up your Google Cloud project: This involves creating or selecting a project, ensuring billing is enabled, and enabling the Vertex AI API.</li> <li>Set up your local environment: You need to install the Google Cloud CLI, authenticate to generate Application Default Credentials, and install the appropriate SDK for your programming language (e.g., <code>pip install --upgrade google-genai</code> for the Python Generative AI SDK).</li> </ol>"},{"location":"start/quickstarts/quickstart-multimodal/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Learn more about the Vertex AI Gemini API.</li> <li>Explore the Google Gen AI SDK reference.</li> <li>Learn about calling Vertex AI models by using the OpenAI library.</li> </ul>"},{"location":"start/quickstarts/quickstart/","title":"Quickstart: Send text prompts to Gemini using Vertex AI Studio","text":"<p>You can use Vertex AI Studio to design, test, and manage prompts for Google's Gemini large language models (LLMs). Vertex AI Studio also supports certain third-party models that are offered on Vertex AI as models as a service (MaaS), such as Anthropic's Claude models and Meta's Llama models.</p> <p>By providing a unified UI, Vertex AI Studio makes it easy to experiment with different models and prompts to discover the best combination for your use case before integrating it into your application.</p> <p>Note</p> <p>On your initial use of third-party models, Vertex AI prompts you to accept the third-party's terms and conditions. You must do this once for each third-party provider to start using their models.</p> <p>In this quickstart, you use sample prompts from the generative AI prompt gallery to test the Gemini model and view the code required to replicate the prompt in your own applications.</p>"},{"location":"start/quickstarts/quickstart/#before-you-begin","title":"\u2699\ufe0f Before you begin","text":"<p>This quickstart requires you to complete the following steps to set up a Google Cloud project and enable the Vertex AI API.</p> <ol> <li> <p>Sign in to your Google Cloud account. If you're new to Google Cloud, create an account to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.</p> </li> <li> <p>In the Google Cloud console, on the project selector page, select or create a Google Cloud project.</p> <p>Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.</p> <p>Go to project selector</p> </li> <li> <p>Make sure that billing is enabled for your Google Cloud project.</p> </li> <li> <p>Enable the Vertex AI API.</p> <p>Enable the API</p> </li> </ol>"},{"location":"start/quickstarts/quickstart/#about-sample-prompts-in-vertex-ai-studio","title":"\ud83d\udcda About sample prompts in Vertex AI Studio","text":"<p>A prompt is a natural language request submitted to a language model that generates a response. Prompts can contain questions, instructions, contextual information, few-shot examples, and partial input for the model to complete. After the model receives a prompt, depending on the type of model used, it can generate text, embeddings, code, images, videos, music, and more.</p> <p>The sample prompts in the Vertex AI Studio prompt gallery are pre-designed to help demonstrate model capabilities. Each prompt is preconfigured with specified model and parameter values so you can open the sample prompt and click Submit to generate a response.</p>"},{"location":"start/quickstarts/quickstart/#test-a-summarization-prompt","title":"\u2699\ufe0f Test a summarization prompt","text":"<p>Send a summarization text prompt to the Vertex AI Gemini API. A summarization task extracts the most important information from text. You can provide information in the prompt to help the model create a summary, or ask the model to create a summary on its own.</p> Step 1: Go to the Prompt Gallery <p>Go to the Prompt gallery page from the Vertex AI section in the Google Cloud console.</p> <p>Go to prompt gallery</p> Step 2: Select the summarization task <p>In the Tasks drop-down menu, select Summarize.</p> Step 3: Open the sample prompt <p>Open the Audio summarization card. This sample prompt includes an audio file and requests a summary of the file contents in a bulleted list.</p> <p></p> Step 4: Review the model settings <p>In the settings panel, notice that the model's default value is set to Gemini-2.0-flash-001. You can choose a different Gemini model by clicking Switch model.</p> <p></p> Step 5: Submit the prompt <p>Click Submit to generate the summary. The output is displayed in the response.</p> <p></p> Step 6: View the code <p>To view the Vertex AI API code used to generate the transcript summary, click Build with code &gt; Get code.</p> <p>In the Get code panel, you can choose your preferred language to get the sample code for the prompt, or you can open the Python code in a Colab Enterprise notebook.</p>"},{"location":"start/quickstarts/quickstart/#test-a-code-generation-prompt","title":"\u2699\ufe0f Test a code generation prompt","text":"<p>Send a code generation prompt to the Vertex AI Gemini API. A code generation task generates code using a natural language description.</p> Step 1: Go to the Prompt Gallery <p>Go to the Prompt gallery page from the Vertex AI section in the Google Cloud console.</p> <p>Go to prompt gallery</p> Step 2: Select the code generation task <p>In the Tasks drop-down menu, select Code.</p> Step 3: Open the sample prompt <p>Open the Generate code from comments card. This sample prompt includes a system instruction that tells the model how to respond and some incomplete Java methods.</p> <p></p> Step 4: Review the model settings <p>In the settings panel, notice that the model's default value is set to Gemini-2.0-flash-001. You can choose a different Gemini model by clicking Switch model.</p> <p></p> Step 5: Submit the prompt <p>To complete each method by generating code in the areas marked <code>&lt;WRITE CODE HERE&gt;</code>, click Submit. The output is displayed in the response.</p> Step 6: View the code <p>To view the Vertex AI API code used to generate the code, click Build with code &gt; Get code.</p> <p>In the Get code panel, you can choose your preferred language to get the sample code for the prompt, or you can open the Python code in a Colab Enterprise notebook.</p>"},{"location":"start/quickstarts/quickstart/#related-questions","title":"\ud83d\udcda Related Questions","text":"What do I need to do before I can start using Vertex AI Studio to test prompts? <p>Before you can use Vertex AI Studio, you must set up your Google Cloud environment by completing these four steps: 1.  Sign in to your Google Cloud account. 2.  Select or create a Google Cloud project. 3.  Make sure billing is enabled for your project. 4.  Enable the Vertex AI API.</p> I've created a prompt in the Vertex AI Studio UI. How can I get the API code to use it in my application? <p>After you submit a prompt and receive a response in Vertex AI Studio, click Build with code and then select Get code. A panel will appear where you can choose your preferred language to get the sample code for the prompt you just ran. You can also open the Python code directly in a Colab Enterprise notebook.</p> Can I use models other than Google's Gemini in Vertex AI Studio? <p>Yes, Vertex AI Studio supports certain third-party models that are offered as a service, such as Anthropic's Claude models and Meta's Llama models. Note that on your first use of a third-party model, you will be prompted to accept that provider's terms and conditions.</p> Where can I find pre-built examples to test common tasks like summarization? <p>Vertex AI Studio includes a Prompt gallery with predesigned sample prompts for various tasks. To find a summarization example, navigate to the Prompt gallery, select Summarize from the Tasks drop-down menu, and then open a sample prompt like Audio summarization.</p> The sample prompts use a default model like <code>Gemini-2.0-flash-001</code>. Can I switch to a different model? <p>Yes. In the settings panel of the prompt interface, you can see the default model selection. To change it, simply click Switch model and choose a different Gemini model from the available options.</p>"},{"location":"start/quickstarts/quickstart/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>See an introduction to prompt design.</li> <li>Learn about designing multimodal prompts and chat prompts.</li> </ul>"},{"location":"start/quickstarts/try-gen-ai/","title":"Send a text prompt to Gemini using Vertex AI Studio","text":"<p>In this quickstart, you use Vertex AI Studio to send a prompt to the Gemini API without needing to register for a Google Cloud account. Vertex AI Studio provides an interface where you can design, test, and manage prompts for Google's Gemini large language model (LLM).</p> <p>In this quickstart, you will:</p> <ul> <li>Send a freeform text prompt to the Gemini API.</li> <li>View the code used to generate the response.</li> </ul>"},{"location":"start/quickstarts/try-gen-ai/#what-is-a-prompt","title":"\ud83d\udcda What is a prompt?","text":"<p>A prompt is a natural language request that you send to a large language model to elicit a response. A good prompt can contain questions, instructions, contextual information, few-shot examples, and partial input for the model to complete.</p> <p>After the model receives a prompt, it can generate various types of output depending on the model used, including text, code, images, and more.</p>"},{"location":"start/quickstarts/try-gen-ai/#before-you-begin-choose-an-access-option","title":"\u2699\ufe0f Before you begin: Choose an access option","text":"<p>You can try Vertex AI Studio with or without a Google Cloud account. Using a free trial or an existing account provides access to more features. Choose the option that best suits your needs.</p> Without a Google Cloud accountWith a Google Cloud free trialWith an existing Google Cloud account <p>This option is the quickest way to try the Gemini API without any setup or billing information.</p> <ul> <li>Sign-in required: No, but you must accept the Terms of Service.</li> <li>Queries per minute (QPM): 2 QPM for all multimodal models.</li> <li>Features: Access to the prompt designer for freeform text prompts.</li> <li>Limitations: You cannot save prompts, view prompt history, use advanced parameters, or access the API directly.</li> <li>Get started: Go to Vertex AI Studio</li> </ul> <p>This option is ideal for exploring more features of Vertex AI with free credits.</p> <ul> <li>Sign-in required: Yes.</li> <li>Credits offered: Up to $300 for 90 days.</li> <li>Queries per minute (QPM): See quota limits.</li> <li>Features: Access to the prompt gallery, prompt designer, prompt history, and API usage.</li> <li>Limitations: Advanced parameters and model tuning are not available.</li> <li>Get started: Sign up for a free trial</li> </ul> <p>This option provides full access to all Vertex AI features for users with an active Google Cloud project and billing enabled.</p> <ul> <li>Sign-in required: Yes.</li> <li>Billing required: Yes.</li> <li>Queries per minute (QPM): See quota limits.</li> <li>Features: Full access to all features, including the prompt gallery, prompt designer, prompt history, advanced parameters, model tuning, and API usage.</li> <li>Get started: Try Vertex AI Studio in your console</li> </ul> <p>This quickstart follows the instructions for accessing Vertex AI Studio without an account.</p>"},{"location":"start/quickstarts/try-gen-ai/#test-gemini-with-a-text-prompt","title":"\u2699\ufe0f Test Gemini with a text prompt","text":"<p>Vertex AI Studio lets you test text and multimodal prompts using a variety of Gemini models. The following steps show you how to ask Gemini for flower shop name suggestions.</p> <pre><code>flowchart LR\n    A[Start] --&gt; B[Open Vertex AI Studio];\n    B --&gt; C[Enter prompt text in the prompt box];\n    C --&gt; D[Click Submit];\n    D --&gt; E[View the generated response];\n    E --&gt; F{Want to see the code?};\n    F -- Yes --&gt; G[Click Get Code];\n    F -- No --&gt; H[End];\n    G --&gt; H[End];</code></pre> Step 1: Open Vertex AI Studio <p>Click the following link to open the prompt designer in Vertex AI Studio.</p> <p>Open Vertex AI Studio</p> <p>If prompted, accept the Vertex AI Studio Terms of Service.</p> Step 2: Send the prompt <p>In the prompt box, enter the following text:</p> <p><code>What are some possible names for a flower shop that sells bouquets of dried flowers?</code></p> <p>Then, click Submit.</p> Step 3: View the response and code <p>The model's response appears in the Response box.</p> <p>To view the API code used to generate this response, click codeGet code. In the Get code panel, you can select your preferred programming language to view the corresponding code snippet.</p> <p>Note: Opening the sample Python code in a Colab Enterprise notebook is not available when using Vertex AI Studio without a Google Cloud account.</p>"},{"location":"start/quickstarts/try-gen-ai/#related-questions","title":"\ud83d\udcda Related Questions","text":"What are the limitations if I use Vertex AI Studio to test Gemini without a Google Cloud account? <p>If you use Vertex AI Studio without a Google Cloud account, you will face several limitations. You are required to have a regular Google Account to accept the Terms of Service, and your usage is limited to 2 queries per minute (QPM). Additionally, features like prompt history, saving prompts, using advanced parameters, and model tuning are not available. You also cannot use the API directly or view Python code in a Colab Enterprise notebook.</p> Can I see the API code for my prompt even if I'm not using a Google Cloud account? <p>Yes. After you submit a prompt and receive a response in Vertex AI Studio, you can click the Get code button. This will show you the code used to generate the response in your preferred programming language. However, note that you cannot open the Python code sample in a Colab Enterprise notebook when using the service without a Google Cloud account.</p> Do I need to sign up for a Google Cloud account to try Gemini? <p>No, you can test freeform text prompts with the Gemini model in Vertex AI Studio without registering for a Google Cloud account. You will, however, need a standard Google Account to accept the Vertex AI Studio Terms of Service. For access to more features, such as the API, prompt history, and higher quotas, you would need to sign up for a Google Cloud free trial or use an existing Google Cloud account.</p> Why can't I find my prompt history or save the prompts I create? <p>The ability to save prompts and view your prompt history are features that are only available when using Vertex AI Studio with a Google Cloud account (either a free trial or a standard account with billing enabled). These features are not available when you test prompts without a Google Cloud account.</p> What is the basic process for testing a text prompt in Vertex AI Studio? <p>You can test a prompt by following these steps: 1.  Navigate to Vertex AI Studio and click Create prompt. 2.  In the prompt box, type your request, such as, <code>What are some possible names for a flower shop that sells bouquets of dried flowers?</code> 3.  Click Submit to send your prompt to the model. 4.  Review the generated output in the Response box.</p>"},{"location":"start/quickstarts/try-gen-ai/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>See an introduction to prompt design.</li> <li>Learn about designing text prompts and text chat prompts.</li> </ul>"},{"location":"static/vertex-ai/generative-ai/docs/learn/overview/","title":"Generative AI beginner's guide","text":"<p>This beginner's guide introduces you to the core technologies of generative AI and explains how they fit together to power chatbots and applications. Generative AI (also known as genAI or gen AI) is a field of machine learning (ML) that develops and uses ML models for generating new content.</p> <p>Generative AI models are often called large language models (LLMs) because of their large size and ability to understand and generate natural language. However, depending on the data that the models are trained on, these models can understand and generate content from multiple modalities, including text, images, videos, and audio. Models that work with multiple modalities of data are called multimodal models.</p> <p>Google provides the Gemini family of generative AI models designed for multimodal use cases, capable of processing information from multiple modalities, including images, videos, and text.</p>"},{"location":"static/vertex-ai/generative-ai/docs/learn/overview/#the-generative-ai-workflow-on-vertex-ai","title":"\ud83d\udcda The generative AI workflow on Vertex AI","text":"<p>For generative AI models to generate content that's useful in real-world applications, they need to have the following capabilities:</p> <ul> <li> <p>Learn how to perform new tasks: Generative AI models are designed to perform general tasks. If you want a model to perform tasks that are unique to your use case, then you need to be able to customize the model. On Vertex AI, you can customize your model through model tuning.</p> </li> <li> <p>Access external information: Generative AI models are trained on vast amounts of data. However, for these models to be useful, they need to be able to access information outside of their training data. For example, if you want to create a customer service chatbot that's powered by a generative AI model, the model needs to have access to information about the products and services that you offer. In Vertex AI, you use the grounding and function calling features to help the model access external information.</p> </li> <li> <p>Block harmful content: Generative AI models might generate output that you don't expect, including text that's offensive or insensitive. To maintain safety and prevent misuse, the models need safety filters to block prompts and responses that are determined to be potentially harmful. Vertex AI has built-in safety features that promote the responsible use of our generative AI services.</p> </li> </ul> <p>The following diagram shows how these different capabilities work together to generate content that you want:</p> <p></p>"},{"location":"static/vertex-ai/generative-ai/docs/learn/overview/#prompt","title":"Prompt","text":"<p>The generative AI workflow typically starts with prompting. A prompt is a natural language request sent to a generative AI model to elicit a response. Depending on the model, a prompt can contain text, images, videos, audio, documents, and other modalities, or even multiple modalities (multimodal). Creating a prompt to get the desired response from the model is a practice called prompt design. While prompt design is a process of trial and error, there are prompt design principles and strategies that you can use to nudge the model to behave in the desired way. Vertex AI Studio offers a prompt management tool to help you manage your prompts.</p>"},{"location":"static/vertex-ai/generative-ai/docs/learn/overview/#foundation-models","title":"Foundation models","text":"<p>Prompts are sent to a generative AI model for response generation. Vertex AI has a variety of generative AI foundation models that are accessible through a managed API, including the following:</p> <ul> <li>Gemini API: For advanced reasoning, multiturn chat, code generation, and multimodal prompts.</li> <li>Imagen API: For image generation, image editing, and visual captioning.</li> <li>MedLM: For medical question answering and summarization. (Deprecated)</li> </ul> <p>The models differ in size, modality, and cost. You can explore Google models, as well as open models and models from Google partners, in Model Garden.</p>"},{"location":"static/vertex-ai/generative-ai/docs/learn/overview/#model-customization","title":"Model customization","text":"<p>You can customize the default behavior of Google's foundation models so that they consistently generate the desired results without using complex prompts. This customization process is called model tuning. Model tuning helps you reduce the cost and latency of your requests by allowing you to simplify your prompts. Vertex AI also offers model evaluation tools to help you evaluate the performance of your tuned model. After your tuned model is production-ready, you can deploy it to an endpoint and monitor its performance, similar to standard MLOps workflows.</p>"},{"location":"static/vertex-ai/generative-ai/docs/learn/overview/#access-external-information","title":"Access external information","text":"<p>Vertex AI offers multiple ways to give the model access to external APIs and real-time information.</p> Feature Primary Use Case How It Works Grounding Reduce model hallucinations. Connects model responses to a source of truth, such as your own data or web search, to make responses more factual. Retrieval-Augmented Generation (RAG) Provide knowledge from external sources. Connects models to external knowledge sources, such as documents and databases, to generate more accurate and informative responses. Function calling Interact with external systems. Lets the model interact with external APIs to get real-time information and perform real-world tasks."},{"location":"static/vertex-ai/generative-ai/docs/learn/overview/#citation-check","title":"Citation check","text":"<p>After the response is generated, Vertex AI checks whether citations need to be included with the response. If a significant amount of the text in the response comes from a particular source, that source is added to the citation metadata in the response.</p>"},{"location":"static/vertex-ai/generative-ai/docs/learn/overview/#responsible-ai-and-safety","title":"Responsible AI and safety","text":"<p>The last layer of checks that the prompt and response go through before being returned is the safety filters. Vertex AI checks both the prompt and response for how much the prompt or response belongs to a safety category. If the threshold is exceeded for one or more categories, the response is blocked and Vertex AI returns a fallback response.</p>"},{"location":"static/vertex-ai/generative-ai/docs/learn/overview/#response","title":"Response","text":"<p>If the prompt and response pass the safety filter checks, the response is returned. Typically, the response is returned all at once. However, with Vertex AI you can also receive responses progressively as it generates by enabling streaming.</p>"},{"location":"static/vertex-ai/generative-ai/docs/learn/overview/#get-started","title":"\u2699\ufe0f Get started","text":"<p>Try one of these quickstarts to get started with generative AI on Vertex AI.</p> <ul> <li> <p>Generate text using the Vertex AI Gemini API     Use the SDK to send requests to the Vertex AI Gemini API.</p> </li> <li> <p>Send prompts to Gemini using the Vertex AI Studio Prompt Gallery     Test prompts with no setup required.</p> </li> <li> <p>Generate an image and verify its watermark using Imagen     Create a watermarked image using Imagen on Vertex AI.</p> </li> </ul>"},{"location":"static/vertex-ai/generative-ai/docs/multimodal/test/test/","title":"Test","text":"<p>test.md</p>"}]}