{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Google Cloud Documentation","text":""},{"location":"gemini/migrate_to_gemini2/","title":"Migrate your application to Gemini 2 with the Gemini API in Vertex AI","text":"<p>This guide shows how to migrate generative AI applications from Gemini 1.x and PaLM models to Gemini 2 models.</p>"},{"location":"gemini/migrate_to_gemini2/#why-migrate-to-gemini-2","title":"\ud83d\udcda Why migrate to Gemini 2?","text":"<p>Gemini 2 delivers significant performance improvements over Gemini 1.x and PaLM models, along with new capabilities. Additionally, each model version has its own version support and availability timeline.</p> <p>Upgrading most generative AI applications to Gemini 2 shouldn't require significant reengineering of prompts or code. However, some applications might require prompt changes, which are difficult to predict without testing. Therefore, we recommend testing your prompts with Gemini 2 before completing the migration.</p> <p>Significant code changes are only needed to address certain breaking changes or to use new Gemini 2 capabilities.</p>"},{"location":"gemini/migrate_to_gemini2/#which-gemini-2-model-should-i-migrate-to","title":"\ud83d\udcda Which Gemini 2 model should I migrate to?","text":"<p>When choosing a Gemini 2 model, consider the features your application requires and the cost of those features.</p> <p>For an overview of Gemini 2 model features, see Gemini 2. For an overview of all Google models, see Google models.</p> <p>The following table provides a comparison of Gemini 1.x and Gemini 2 models.</p> Feature Gemini 1.5 Pro Gemini 1.5 Flash Gemini 2.0 Flash Gemini 2.0 Flash-Lite Gemini 2.5 Pro Gemini 2.5 Flash Input modalities text, documents, image, video, audio text, documents, image, video, audio text, documents, image, video, audio text, documents, image, video, audio text, documents, image, video, audio text, documents, image, video, audio Output modalities text text text text text text Context window, total token limit 2,097,152 1,048,576 1,048,576 1,048,576 1,048,576 1,048,576 Output context length 8,192 8,192 8,192 8,192 64,192 64,192 Grounding with Search Yes Yes Yes No Yes Yes Function calling Yes Yes Yes Yes Yes Yes Code execution No No Yes No Yes Yes Context caching Yes Yes Yes No Yes Yes Batch prediction Yes Yes Yes Yes Yes Yes Live API No No No No No No Latency Most capable in 1.5 family Fastest in 1.5 family Fast + good cost efficiency Fast + most cost efficient Slower than Flash, but good cost efficiency Fast + most cost efficient Fine-tuning Yes Yes Yes Yes Yes Yes Recommended SDK Vertex AI SDK Vertex AI SDK Gen AI SDK Gen AI SDK Gen AI SDK Gen AI SDK Pricing units Character Character Token Token Token Token"},{"location":"gemini/migrate_to_gemini2/#migration-process","title":"\u2699\ufe0f Migration Process","text":"<p>This section outlines the end-to-end process for migrating your application to a Gemini 2 model.</p> <pre><code>flowchart TD\n    subgraph Preparation\n        A[Start] --&gt; B[Step 1: Complete prerequisites];\n        B --&gt; C[Step 2: Document evaluation requirements];\n    end\n\n    subgraph Implementation &amp; Testing\n        C --&gt; D[Step 3: Upgrade code];\n        D --&gt; E[Step 4: Perform offline evaluation &amp; tune];\n        E --&gt; F{Performance drop?};\n        F -- Yes --&gt; G[Tune prompts &amp; hyperparameters];\n        G --&gt; E;\n        F -- No --&gt; H[Step 5: Perform load &amp; online testing];\n    end\n\n    subgraph Deployment\n        H --&gt; I{Online results match offline?};\n        I -- No --&gt; C;\n        I -- Yes --&gt; J[Step 6: Deploy to production];\n        J --&gt; K[Finish];\n    end</code></pre> Step 1: Complete prerequisites <p>For a seamless Gemini 2 migration, address the following concerns before you begin the migration process.</p> <ul> <li>Model retirement awareness: Note the model version support and availability timelines for older Gemini models, and make sure your migration is completed before the model you're using is retired.</li> <li>InfoSec, governance, and regulatory approvals: Proactively request the approvals you need for Gemini 2 from your information security (InfoSec), risk, and compliance stakeholders. Make sure that you cover domain-specific risk and compliance constraints, especially in heavily regulated industries such as healthcare and financial services. Note that Gemini security controls differ among Gemini 2 models.</li> <li>Location availability: See the Generative AI on Google Cloud models and partner model availability documentation, and make sure your chosen Gemini 2 model is available in the regions where you need it, or consider switching to the global endpoint.</li> <li>Modality and tokenization-based pricing differences: Check Gemini 2 pricing for all the modalities (text, code, images, speech) in your application. For more information, see generative AI pricing page. Note that Gemini 2 text input and output is priced per token, while Gemini 1 text input and output is priced per character.</li> <li>Provisioned Throughput: If needed, purchase additional Provisioned Throughput for Gemini 2 or change existing Provisioned Throughput orders.</li> <li>Supervised fine-tuning: If your Gemini application uses supervised fine-tuning, submit a new tuning job with Gemini 2. We recommend that you start with the default tuning hyperparameters instead of reusing the hyperparameter values that you used with previous Gemini versions. The tuning service has been optimized for Gemini 2. Therefore, reusing previous hyperparameter values might not yield the best results.</li> <li>Regression testing: There are three main types of regression tests involved when upgrading to Gemini 2 models:<ul> <li>Code regression tests: Regression testing from a software engineering and DevOps perspective. This type of regression test is always required.</li> <li>Model performance regression tests: Regression testing from a data science or machine learning perspective. This means ensuring that the new Gemini 2 model provides outputs that are at least as high-quality as outputs from the current production model. Model performance regression tests are just model evaluations done as part of a change to a system or to the underlying model. Model performance regression testing further breaks down into:<ul> <li>Offline model performance testing: Assessing the quality of model outputs in a dedicated experimentation environment based on various model output quality metrics.</li> <li>Online model performance testing: Assessing the quality of model outputs in a live online deployment based on implicit or explicit user feedback.</li> </ul> </li> <li>Load testing: Assessing how the application handles high volumes of inference requests. This type of regression test is required for applications that require Provisioned Throughput.</li> </ul> </li> </ul> Step 2: Document evaluation and testing requirements <ol> <li>Prepare to repeat any relevant evaluations from when you originally built your application, along with any relevant evaluations you have done since then.</li> <li>If you feel your existing evaluations don't appropriately cover or measure the breadth of tasks that your application performs, you should design and prepare additional evaluations.</li> <li>If your application involves RAG, tool use, complex agentic workflows, or prompt chains, make sure that your existing evaluation data allows for assessing each component independently. If not, gather input-output examples for each component.</li> <li>If your application is especially high-impact, or if it's part of a larger user-facing real-time system, you should include online evaluation.</li> </ol> Step 3: Upgrade your code <p>This step involves choosing an SDK, changing your code to call a Gemini 2 model, and addressing any breaking changes.</p> Step 4: Perform offline evaluation and tune Step 5: Perform load and online testing Step 6: Deploy to production <p>Once your evaluation shows that Gemini 2 meets or exceeds the performance of an older model, turn down the existing version of your application in favor of the Gemini 2 version. Follow your organization's existing procedures for production rollout.</p> <p>If you're using Provisioned Throughput, change your Provisioned Throughput order to your chosen Gemini 2 model. If you're rolling out your application incrementally, use short-term Provisioned Throughput to meet throughput requirements for two different Gemini models.</p>"},{"location":"gemini/migrate_to_gemini2/#choose-and-configure-your-sdk","title":"Choose and configure your SDK","text":"<p>When migrating your application, you can continue using the Vertex AI SDK for Python or upgrade to the Google Gen AI SDK.</p> Feature/Reason Vertex AI SDK Google Gen AI SDK Primary Use Case Interacting with a broad range of Vertex AI services, not just generative AI models. Focused specifically on Google's generative AI models, including the latest Gemini 2 features. Gemini 2 Features Supports core Gemini features but does not support all new Gemini 2 capabilities. Required for new Gemini 2 capabilities like code execution. New features are added only to this SDK. Recommendation Suitable if your application only requires capabilities already available in this SDK and you have extensive existing code using it. Recommended for most migrations to access the full power of Gemini 2 and future updates. <p>Choose the SDK that best fits your needs and follow the installation and setup instructions below.</p> Google Gen AI SDK (Recommended)Vertex AI SDK <p>The Gen AI SDK is the recommended choice for accessing the latest Gemini 2 features. If you're new to the Gen AI SDK, see the Getting started with Google Generative AI using the Gen AI SDK notebook.</p> <p>1. Install the SDK</p> <pre><code>pip install --upgrade google-genai\n</code></pre> <p>2. Configure your environment</p> <p>Set the following environment variables to use the Gen AI SDK with Vertex AI.</p> <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n</code></pre> <p>3. Initialize the client and send a request</p> <pre><code>from google import genai\nfrom google.genai.types import HttpOptions\n\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash-preview-05-20\",\n    contents=\"How does AI work?\",\n)\nprint(response.text)\n# Example response:\n# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n#\n# Here's a simplified overview:\n# ...\n</code></pre> <p>To learn more, see the SDK reference documentation.</p> <p>You can continue using the Vertex AI SDK if your application does not require the newest Gemini 2 features. The setup process is the same for all Gemini model versions.</p> <p>1. Install the SDK</p> <pre><code>pip install --upgrade --quiet google-cloud-aiplatform\n</code></pre> <p>2. Initialize the client and send a request</p> <pre><code>import vertexai\nfrom vertexai.generative_models import GenerativeModel\n\n# TODO(developer): Update and un-comment below line\n# PROJECT_ID = \"your-project-id\"\nvertexai.init(project=PROJECT_ID, location=\"us-central1\")\n\nmodel = GenerativeModel(\"gemini-2.0-flash-001\")\n\nresponse = model.generate_content(\n    \"What's a good name for a flower shop that specializes in selling bouquets of dried flowers?\"\n)\n\nprint(response.text)\n# Example response:\n# **Emphasizing the Dried Aspect:**\n# * Everlasting Blooms\n# * Dried &amp; Delightful\n# * The Petal Preserve\n# ...\n</code></pre> <p>For more information, see Introduction to the Vertex AI SDK for Python.</p>"},{"location":"gemini/migrate_to_gemini2/#change-your-gemini-calls","title":"Change your Gemini calls","text":"<p>Change your prediction code to use Gemini 2. At a minimum, this means changing the model endpoint name to a Gemini 2 model where you load your model. After you make your code changes, perform code regression testing to ensure that it runs correctly. This test is only meant to assess whether the code functions, not the quality of model responses.</p>"},{"location":"gemini/migrate_to_gemini2/#address-breaking-code-changes","title":"Address breaking code changes","text":"<ul> <li>Dynamic retrieval: Switch to using Grounding with Google Search. This feature requires using the Gen AI SDK; it's not supported by the Vertex AI SDK.</li> <li>Content filters: Note the default content filter settings, and change your code if it relies on a default that has changed.</li> <li><code>Top-K</code> token sampling parameter: Models after <code>gemini-1.0-pro-vision</code> don't support changing the <code>Top-K</code> parameter.</li> </ul> <p>Focus only on code changes in this step. You may need to make other changes later based on evaluation results, such as adjusting prompts or other token sampling parameters like <code>Top-P</code>.</p>"},{"location":"gemini/migrate_to_gemini2/#offline-evaluation","title":"Offline evaluation","text":"<p>Repeat the evaluation that you did when you originally developed and launched your application, any further offline evaluation you did after launching, and any additional evaluation you identified in step 2. If you don't have an automated way to run your offline evaluations, consider using the Gen AI evaluation service.</p> <p>If your application uses fine-tuning, perform offline evaluation before retuning your model with Gemini 2. Gemini 2's improved output quality may mean that your application no longer requires a fine-tuned model.</p>"},{"location":"gemini/migrate_to_gemini2/#assess-evaluation-results-and-tune","title":"Assess evaluation results and tune","text":"<p>If your offline evaluation shows a drop in performance with Gemini 2, iterate on your application as follows until Gemini performance matches the older model:</p> <ul> <li>Iteratively engineer your prompts to improve performance (\"Hill Climbing\"). If you are new to hill climbing, see the Vertex Gemini hill climbing online training. The Vertex AI prompt optimizer (example notebook) can also help.</li> <li>If your application already relies on fine-tuning, try fine-tuning Gemini 2.</li> <li>If your application is impacted by Dynamic Retrieval and Top-K breaking changes, experiment with changing your prompt and token sampling parameters.</li> </ul>"},{"location":"gemini/migrate_to_gemini2/#load-testing","title":"Load testing","text":"<p>If your application requires a certain minimum throughput, perform load testing to make sure the Gemini 2 version of your application meets your throughput requirements. Load testing should happen before online evaluation. Use your existing load testing instrumentation to perform this step.</p> <p>If your application already meets throughput requirements, consider using Provisioned Throughput. You'll need additional short-term Provisioned Throughput to cover load testing while your existing Provisioned Throughput order continues to serve production traffic.</p>"},{"location":"gemini/migrate_to_gemini2/#online-evaluation","title":"Online evaluation","text":"<p>Only proceed to online evaluation if your offline evaluation shows adequate Gemini output quality and your application requires online evaluation. Online evaluation is a special case of online testing. Try to use your organization's existing tools and procedures for online evaluation, such as A/B tests or canary deployments.</p> <p>Online evaluation can also be done by building new feedback and measurement capabilities into your application, for example: *   Adding thumbs-up and thumbs-down buttons next to model outputs. *   Presenting users with outputs from both models and asking them to pick their favorite. *   Tracking how often users override or manually adjust model outputs.</p> <p>If online evaluation results differ significantly from offline evaluation results, your offline evaluation is not capturing key aspects of the live environment. Use the online evaluation findings to devise a new offline evaluation to cover the gap, and then return to Step 2.</p>"},{"location":"gemini/migrate_to_gemini2/#improving-model-performance","title":"\ud83d\udcda Improving model performance","text":"<p>As you complete your migration, use the following tips to maximize Gemini 2 model performance:</p> <ul> <li>Inspect your system instructions, prompts, and few-shot learning examples for any inconsistencies, contradictions, or irrelevant instructions and examples.</li> <li>Test a more powerful model. For example, if you evaluated Gemini 2.0 Flash-Lite, try Gemini 2.0 Flash.</li> <li>Examine any automated evaluation results to make sure they match human judgment, especially results that use a judge model.</li> <li>Fine-tune the Gemini 2 model.</li> <li>Examine evaluation outputs to look for patterns that show specific kinds of failures. Grouping failures into categories gives you more targeted evaluation data, which makes it easier to adjust prompts.</li> <li>Make sure you are independently evaluating different generative AI components.</li> <li>Experiment with adjusting token sampling parameters.</li> </ul>"},{"location":"gemini/migrate_to_gemini2/#getting-help","title":"\ud83d\udd17 Getting help","text":"<p>If you need help, Google Cloud offers support packages to meet your needs, such as 24/7 coverage, phone support, and access to a technical support manager. For more information, see Google Cloud Support.</p>"},{"location":"gemini/migrate_to_gemini2/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Read the list of frequently asked questions.</li> <li>Migrate from the PaLM API to the Gemini API in Vertex AI.</li> </ul>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/","title":"Migrate from the Gemini API on Google AI Studio to the Gemini API in Vertex AI","text":"<p>If you started building with the Gemini API through Google AI Studio, you might find that as your generative AI solutions mature, you need a more comprehensive platform for building, deploying, and managing them end-to-end.</p> <p>This guide explains the benefits of migrating to the Gemini API in Vertex AI and provides a step-by-step process for moving your existing work from Google AI Studio to Google Cloud's Vertex AI platform.</p>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#when-to-choose-the-gemini-api-in-vertex-ai","title":"\ud83d\udcda When to choose the Gemini API in Vertex AI","text":"<p>Google Cloud provides a comprehensive ecosystem of tools to enable developers to harness the power of generative AI, from app development to deployment, hosting, and managing complex data at scale.</p> <p>Vertex AI offers a suite of MLOps tools that streamline the usage, deployment, and monitoring of AI models for efficiency and reliability. Integrations with databases, DevOps tools, logging, monitoring, and IAM provide a holistic approach to managing the entire generative AI lifecycle.</p> <p>Consider migrating to Vertex AI for the following use cases:</p> <ul> <li>Productionize your apps and solutions: Products like Cloud Functions and Cloud Run let you deploy apps with enterprise-grade scale, security, and privacy. For more details, see Security, Privacy, and Cloud Compliance on Google Cloud.</li> <li>Leverage end-to-end MLOps: Use Vertex AI for capabilities ranging from model tuning to vector similarity search and ML pipelines.</li> <li>Build event-driven architectures: Trigger your LLM calls with Cloud Functions or Cloud Run.</li> <li>Monitor application usage: Integrate with Cloud Logging and BigQuery to monitor your app's performance and usage.</li> <li>Store data at scale: Use enterprise-grade services like BigQuery, Cloud Storage, and Cloud SQL.</li> <li>Perform Retrieval-Augmented Generation (RAG): Use your data in BigQuery or Cloud Storage to ground model responses.</li> <li>Create and schedule data pipelines: Use Cloud Scheduler to schedule jobs.</li> <li>Apply LLMs to your data in the cloud: If you store data in Cloud Storage or BigQuery, you can prompt LLMs over that data to extract information, summarize, or ask questions.</li> <li>Manage data governance: Leverage Google Cloud data governance and residency policies to manage your data lifecycle.</li> </ul>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#differences-between-the-apis","title":"\ud83d\udcda Differences between the APIs","text":"<p>The following table summarizes the main differences between the Gemini API on Google AI Studio and the Gemini API in Vertex AI to help you decide which option is right for your use case.</p> Features Gemini API on Google AI Studio Gemini API in Vertex AI Gemini models Gemini 2.0 Flash, Gemini 2.0 Flash-Lite Gemini 2.0 Flash, Gemini 2.0 Flash-Lite Sign up Google account Google Cloud account (with terms agreement and billing) Authentication API key Google Cloud service account User interface playground Google AI Studio Vertex AI Studio API &amp; SDK Server and mobile/web client SDKs <ul><li>Server: Python, Node.js, Go, Dart, ABAP</li><li>Mobile/Web client: Android (Kotlin/Java), Swift, Web, Flutter</li></ul> Server and mobile/web client SDKs <ul><li>Server: Python, Node.js, Go, Java, ABAP</li><li>Mobile/Web client (via Vertex AI in Firebase): Android (Kotlin/Java), Swift, Web, Flutter</li></ul> No-cost usage of API &amp; SDK Yes, where applicable $300 Google Cloud credit for new users Quota (requests per minute) Varies based on model and pricing plan (see detailed information) Varies based on model and region (see detailed information) Enterprise support No Customer encryption keyVirtual private cloudData residencyAccess transparencyScalable infrastructure for application hostingDatabases and data storage MLOps No Full MLOps on Vertex AI (examples: model evaluation, Model Monitoring, Model Registry)"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#migrate-to-the-gemini-api-in-vertex-ai","title":"\u2699\ufe0f Migrate to the Gemini API in Vertex AI","text":"<p>This section shows how to migrate from the Gemini API on Google AI Studio to the Gemini API in Vertex AI.</p>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#migration-workflow","title":"Migration workflow","text":"<p>The following diagram shows the high-level workflow for migrating to Vertex AI.</p> <pre><code>flowchart TD\n    A[Start] --&gt; B{Have a Google Cloud Project?}\n    B -- Yes --&gt; C[Enable Vertex AI API]\n    B -- No --&gt; D[Create Google Cloud Account &amp; Project]\n    D --&gt; C\n    C --&gt; E[Migrate Code to use Vertex AI SDK]\n    E --&gt; F[Migrate Prompts from Google Drive to Vertex AI Studio]\n    F --&gt; G[Upload Training Data to Cloud Storage]\n    G --&gt; H[Clean up: Delete old Gemini API Key]\n    H --&gt; I[Finish]</code></pre>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#before-you-begin","title":"Before you begin","text":"<p>Consider the following when migrating:</p> <ul> <li>You can use your existing Google Cloud project (the same one you used to generate your Gemini API key) or you can create a new Google Cloud project.</li> <li>Supported regions might differ. See the list of supported regions for generative AI on Google Cloud.</li> <li>Any models you created in Google AI Studio need to be retrained in Vertex AI.</li> </ul>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#migration-steps","title":"Migration steps","text":"Step 1: Set up your Google Cloud project and Vertex AI <p>The process you follow depends on whether you already have a Google Cloud account.</p> <p>Note: Google AI Studio and its Gemini API are available only in specific regions and languages. If you aren't located in a supported region, you can't start using the Gemini API in Vertex AI.</p> New to Google CloudAlready use Google Cloud <ol> <li>Sign in to Google AI Studio.</li> <li>At the bottom of the left navigation pane, click Build with Vertex AI on Google Cloud. The Create an account to get started with Google Cloud page opens.</li> <li>Click Agree &amp; Continue. The Let's confirm your identity page appears.</li> <li>Click Start Free. The Get Started with Vertex AI studio dialog appears.</li> <li>To enable the APIs required to run Vertex AI, click Agree &amp; Continue. The Vertex AI console appears.</li> </ol> <ol> <li>Sign in to Google AI Studio.</li> <li>At the bottom of the left navigation pane, click Build with Vertex AI on Google Cloud. The Try Vertex AI and Google Cloud for free page opens.</li> <li>Click Agree &amp; Continue. The Get Started with Vertex AI studio dialog appears.</li> <li>To enable the APIs required to run Vertex AI, click Agree &amp; Continue. The Vertex AI console appears.</li> </ol> Step 2: Migrate your Python code <p>The following sections show code snippets to help you migrate your Python code to use the Gemini API in Vertex AI.</p> Step 3: Migrate your prompts to Vertex AI Studio <p>Your Google AI Studio prompt data is saved in a Google Drive folder. This section shows how to migrate your prompts to Vertex AI Studio.</p> <ol> <li>Open Google Drive.</li> <li>Navigate to the AI_Studio folder where the prompts are stored.     </li> <li> <p>Download your prompts from Google Drive to a local directory.</p> <p>Note: Prompts downloaded from Google Drive are in the text (<code>.txt</code>) format. Before you upload them to Vertex AI Studio, convert them to JSON files by changing the file extension from <code>.txt</code> to <code>.json</code>.</p> </li> <li> <p>Open Vertex AI Studio in the Google Cloud console.</p> </li> <li>In the Vertex AI menu, click Prompt management.</li> <li>Click Import prompt.</li> <li>In the Prompt file field, click Browse and select a prompt from your local directory. To upload prompts in bulk, you must manually combine your prompts into a single JSON file.</li> <li>Click Upload. The prompts are uploaded to the My Prompts tab.</li> </ol> Step 4: Upload training data to Vertex AI <p>To migrate your training data to Vertex AI, you need to upload your data to a Cloud Storage bucket. For more information, see Introduction to tuning.</p>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#sdk-setup-and-authentication","title":"SDK setup and authentication","text":"<p>On Vertex AI, you don't need an API key. Instead, access is managed using IAM, which controls permissions for a user, group, or service account.</p> <p>While there are many ways to authenticate, the easiest method for a development environment is to install the Google Cloud CLI and then use your user credentials to sign in to the CLI.</p> <p>To make inference calls to Vertex AI, you must also ensure that your user or service account has the Vertex AI User role.</p>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#code-examples","title":"Code examples","text":"Install the clientGenerate text from textGenerate text from text and imageMulti-turn chat <p>Gemini API on Google AI Studio</p> <pre><code># To install the Python SDK, use this CLI command:\n# pip install google-generativeai\n\nimport google.generativeai as genai\nfrom google.generativeai import GenerativeModel\n\nAPI_KEY=\"API_KEY\"\ngenai.configure(api_key=API_KEY)\n</code></pre> <p>Gemini API in Vertex AI</p> <pre><code># To install the Python SDK, use this CLI command:\n# pip install google-genai\n\nfrom google import genai\n\nPROJECT_ID = \"PROJECT_ID\"\nLOCATION = \"LOCATION\"  # e.g. us-central1\nclient = genai.Client(project=PROJECT_ID, location=LOCATION, vertexai=True)\n</code></pre> <p>Gemini API on Google AI Studio</p> <pre><code>model = GenerativeModel(\"gemini-2.0-flash\")\n\nresponse = model.generate_content(\"The opposite of hot is\")\nprint(response.text) #  The opposite of hot is cold.\n</code></pre> <p>Gemini API in Vertex AI</p> <pre><code>from google import genai\nfrom google.genai.types import HttpOptions\n\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash-preview-05-20\",\n    contents=\"How does AI work?\",\n)\nprint(response.text)\n# Example response:\n# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n#\n# Here's a simplified overview:\n# ...\n</code></pre> <p>Gemini API on Google AI Studio</p> <pre><code>import PIL.Image\n\nmultimodal_model = GenerativeModel(\"gemini-2.0-flash\")\n\nimage = PIL.Image.open(\"image.jpg\")\n\nresponse = multimodal_model.generate_content([\"What is this picture?\", image])\nprint(response.text) # A cat is shown in this picture.\n</code></pre> <p>Gemini API in Vertex AI</p> <pre><code>from google import genai\nfrom google.genai.types import HttpOptions, Part\n\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash-preview-05-20\",\n    contents=[\n        \"What is shown in this image?\",\n        Part.from_uri(\n            file_uri=\"gs://cloud-samples-data/generative-ai/image/scones.jpg\",\n            mime_type=\"image/jpeg\",\n        ),\n    ],\n)\nprint(response.text)\n# Example response:\n# The image shows a flat lay of blueberry scones arranged on parchment paper. There are ...\n</code></pre> <p>Gemini API on Google AI Studio</p> <pre><code>model = GenerativeModel(\"gemini-2.0-flash\")\n\nchat = model.start_chat()\n\nprint(chat.send_message(\"How are you?\").text)\nprint(chat.send_message(\"What can you do?\").text)\n</code></pre> <p>Gemini API in Vertex AI</p> <pre><code>from google import genai\nfrom google.genai.types import HttpOptions, ModelContent, Part, UserContent\n\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nchat_session = client.chats.create(\n    model=\"gemini-2.5-flash-preview-05-20\",\n    history=[\n        UserContent(parts=[Part(text=\"Hello\")]),\n        ModelContent(\n            parts=[Part(text=\"Great to meet you. What would you like to know?\")],\n        ),\n    ],\n)\nresponse = chat_session.send_message(\"Tell me a story.\")\nprint(response.text)\n# Example response:\n# Okay, here's a story for you:\n# ...\n</code></pre>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#delete-unused-api-keys","title":"\u2699\ufe0f Delete unused API keys","text":"<p>If you have fully migrated to Vertex AI and no longer need your old Gemini API key, follow security best practices and delete it.</p> <ol> <li>Open the Google Cloud API Credentials page.</li> <li>Find the API key that you want to delete and click the Actions icon.</li> <li>Select Delete API key.</li> <li>In the Delete credential modal, select Delete.</li> </ol> <p>Deleting an API key takes a few minutes to propagate. After propagation completes, any traffic using the deleted API key is rejected.</p> <p>Important: If you delete a key that's still used in production and need to recover it, see <code>gcloud beta services api-keys undelete</code>.</p>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Try a quickstart tutorial using Vertex AI Studio or the Vertex AI API.</li> </ul>"},{"location":"migration/openai/Authenticate/","title":"Authenticate","text":"<p>Preview</p> <p>This product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the Service Specific Terms. Pre-GA products and features are available \"as is\" and might have limited support. For more information, see the launch stage descriptions.</p> <p>To use the OpenAI Python libraries with Vertex AI, you first need to install the OpenAI SDK:</p> <pre><code>pip install openai\n</code></pre> <p>To authenticate with the Chat Completions API for Vertex AI, you can either modify your client setup or change your environment configuration to use Google authentication and a Vertex AI endpoint. This guide provides steps for both methods, covering calls to Gemini models and self-deployed Model Garden models.</p> <p>Important: Certain models in Model Garden and supported Hugging Face models must be deployed to a Vertex AI endpoint before they can serve requests. When calling these self-deployed models, you'll need to specify the <code>ENDPOINT_ID</code> in your configuration. You can list your existing Vertex AI endpoints using the <code>gcloud ai endpoints list</code> command.</p> <p>The following diagram shows the workflow for choosing an authentication method:</p> <pre><code>flowchart TD\n  A[Start: Need to Authenticate] --&gt; B{Choose Method};\n  B -- Client Setup --&gt; C[Configure Client in Code];\n  B -- Environment Variables --&gt; D[Set Environment Variables];\n  C --&gt; E[Use API];\n  D --&gt; E;</code></pre>"},{"location":"migration/openai/Authenticate/#choose-your-authentication-method","title":"\u2699\ufe0f Choose your authentication method","text":"<p>You can authenticate by configuring the OpenAI client directly in your code or by setting environment variables. The best method depends on your specific needs and environment. Client setup offers more programmatic control and easier token refresh within applications, while environment variables can be simpler for quick scripts or when using clients in languages other than Python.</p> <p>Comparison of Authentication Methods</p> Feature Client Setup (Python SDK) Environment Variables Primary Use Case Programmatic authentication within Python applications. Quick setup, shell scripts, or language-agnostic clients that read these variables. Credential Management Handles token refresh more easily within the application. Manual or scripted token refresh required for <code>OPENAI_API_KEY</code>. Setup Complexity Requires <code>google-auth</code> library and code integration. Requires gcloud CLI and setting environment variables. Flexibility High, allows dynamic configuration and error handling. Simpler for static configurations. Client SetupEnvironment Variables <p>To programmatically manage authentication and credentials in Python, use the <code>google-auth</code> Python SDK.</p> <p>First, install the necessary libraries:</p> <pre><code>pip install google-auth requests\n</code></pre> <p>Before trying the sample code below, follow the Python setup instructions in the Vertex AI quickstart using client libraries. For more information, see the Vertex AI Python API reference documentation.</p> <p>To authenticate to Vertex AI, set up Application Default Credentials (ADC). For more information, see Set up authentication for a local development environment.</p> <p>The following Python code demonstrates how to configure the OpenAI client:</p> <pre><code>import openai\n\nfrom google.auth import default\nimport google.auth.transport.requests\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n# Note: the credential lives for 1 hour by default (https://cloud.google.com/docs/authentication/token-types#at-lifetime); after expiration, it must be refreshed.\n\n##############################\n# Choose one of the following:\n##############################\n\n# If you are calling a Gemini model, set the ENDPOINT_ID variable to use openapi.\nENDPOINT_ID = \"openapi\"\n\n# If you are calling a self-deployed model from Model Garden, set the\n# ENDPOINT_ID variable and set the client's base URL to use your endpoint.\n# ENDPOINT_ID = \"YOUR_ENDPOINT_ID\"\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{ENDPOINT_ID}\",\n    api_key=credentials.token,\n)\n</code></pre> <p>Access tokens obtained this way are short-lived (typically 1 hour). For longer-running applications, see the section on Refresh your credentials.</p> <p>You can configure authentication by setting environment variables. The OpenAI library automatically reads <code>OPENAI_API_KEY</code> and <code>OPENAI_BASE_URL</code>.</p> <p>First, install the Google Cloud CLI if you haven't already.</p> <p>Set the following common environment variables:</p> <pre><code>export PROJECT_ID=\"PROJECT_ID\"\nexport LOCATION=\"LOCATION\"\nexport OPENAI_API_KEY=\"$(gcloud auth application-default print-access-token)\"\n</code></pre> <p>Next, configure the <code>OPENAI_BASE_URL</code> based on your model type:</p> <ul> <li> <p>For Gemini models:     Set the <code>MODEL_ID</code> (e.g., <code>google/gemini-1.5-flash-001</code>) and use the <code>openapi</code> endpoint. The <code>MODEL_ID</code> will be used in your API calls.     <pre><code>export MODEL_ID=\"MODEL_ID\" # e.g., google/gemini-1.5-flash-001\nexport OPENAI_BASE_URL=\"https://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/openapi\"\n</code></pre></p> </li> <li> <p>For self-deployed Model Garden models:     Set the <code>ENDPOINT_ID</code> for your deployed model.     <pre><code>export ENDPOINT_ID=\"YOUR_ENDPOINT_ID\"\nexport OPENAI_BASE_URL=\"https://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/${ENDPOINT_ID}\"\n</code></pre></p> </li> </ul> <p>After setting the environment variables, initialize the OpenAI client in your code (the client will pick up the variables automatically):</p> <pre><code>client = openai.OpenAI()\n</code></pre> <p>The access token obtained via <code>gcloud auth application-default print-access-token</code> is short-lived (typically 1 hour). You may need to refresh this token and re-export <code>OPENAI_API_KEY</code> periodically for long-running processes. For automated refresh within an application, consider the client setup method or see Refresh your credentials for a Python example.</p>"},{"location":"migration/openai/Authenticate/#refresh-your-credentials","title":"\u2699\ufe0f Refresh your credentials","text":"<p>Access tokens are short-lived (typically 1 hour by default). For applications that run longer, you need to refresh the credentials. The following Python example shows a class that wraps the OpenAI client to automatically refresh Google Cloud credentials when they expire.</p> <p><pre><code>from typing import Any\n\nimport google.auth\nimport google.auth.transport.requests\nimport openai\n\n\nclass OpenAICredentialsRefresher:\n    def __init__(self, **kwargs: Any) -&gt; None:\n        # Set a placeholder key here; it will be replaced upon first use or refresh.\n        self.client = openai.OpenAI(**kwargs, api_key=\"PLACEHOLDER\")\n        self.creds, self.project = google.auth.default(\n            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n        )\n        # Ensure the token is fresh at initialization if credentials are valid\n        if self.creds and self.creds.valid:\n            self.creds.refresh(google.auth.transport.requests.Request())\n            self.client.api_key = self.creds.token\n        elif self.creds: # creds exist but are not valid (e.g. expired and can't refresh without interaction)\n             # Attempt a refresh, which might raise an error if user interaction is needed\n            self.creds.refresh(google.auth.transport.requests.Request())\n            self.client.api_key = self.creds.token\n\n\n    def __getattr__(self, name: str) -&gt; Any:\n        # Refresh credentials if they are invalid or nearing expiration (optional: add proactive refresh)\n        if not self.creds.valid:\n            self.creds.refresh(google.auth.transport.requests.Request())\n\n            if not self.creds.valid:\n                raise RuntimeError(\"Unable to refresh auth credentials.\")\n\n            self.client.api_key = self.creds.token\n        return getattr(self.client, name)\n\n\n# Example Usage:\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Initialize the client wrapper\n# For Gemini models:\n# client = OpenAICredentialsRefresher(\n#     base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n# )\n\n# For self-deployed Model Garden models:\n# ENDPOINT_ID = \"YOUR_ENDPOINT_ID\"\n# client = OpenAICredentialsRefresher(\n#    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{ENDPOINT_ID}\",\n# )\n\n\n# Example API call (uncomment and configure client above)\n# response = client.chat.completions.create(\n#    model=\"google/gemini-1.5-flash-001\", # Or your Model Garden model if using a specific endpoint\n#    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n# )\n#\n# print(response)\n</code></pre> You can also extend the life of your access token if appropriate for your security posture.</p>"},{"location":"migration/openai/Authenticate/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>See examples of calling the Chat Completions API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Inference API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Function Calling API with OpenAI-compatible syntax.</li> <li>Learn more about the Gemini API.</li> <li>Learn more about migrating from Azure OpenAI to the Gemini API.</li> </ul>"},{"location":"migration/openai/Examples/","title":"Examples","text":"<p>Preview</p> <p>This product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the Service Specific Terms. Pre-GA products and features are available \"as is\" and might have limited support. For more information, see the launch stage descriptions.</p> <p>To see an example of using the Chat Completions API, run the \"Call Gemini with the OpenAI Library\" Jupyter notebook in one of the following environments:</p> <p>Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench user-managed notebooks | View on GitHub</p>"},{"location":"migration/openai/Examples/#call-gemini-with-the-chat-completions-api","title":"\ud83d\udcbb Call Gemini with the Chat Completions API","text":"<p>Use these examples when you want to use Google's pre-trained Gemini models directly without managing your own deployments.</p>"},{"location":"migration/openai/Examples/#send-non-streaming-requests","title":"Send non-streaming requests","text":"<p>The following sample shows you how to send non-streaming requests:</p> RESTPython <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\nhttps://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/openapi/chat/completions \\\n-d '{\n  \"model\": \"google/${MODEL_ID}\",\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Write a story about a magic backpack.\"\n  }]\n}'\n</code></pre> <p>Before trying this sample, follow the Python setup instructions in the Vertex AI quickstart using client libraries. For more information, see the Vertex AI Python API reference documentation.</p> <p>To authenticate to Vertex AI, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.</p> <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=\"google/gemini-2.0-flash-001\",\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n)\n\nprint(response)\n</code></pre>"},{"location":"migration/openai/Examples/#send-streaming-requests","title":"Send streaming requests","text":"<p>The following sample shows you how to send streaming requests to a Gemini model by using the Chat Completions API:</p> RESTPython <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\nhttps://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/openapi/chat/completions \\\n-d '{\n  \"model\": \"google/${MODEL_ID}\",\n  \"stream\": true,\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Write a story about a magic backpack.\"\n  }]\n}'\n</code></pre> <p>Before trying this sample, follow the Python setup instructions in the Vertex AI quickstart using client libraries. For more information, see the Vertex AI Python API reference documentation.</p> <p>To authenticate to Vertex AI, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.</p> <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=\"google/gemini-2.0-flash-001\",\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n    stream=True,\n)\nfor chunk in response:\n    print(chunk)\n</code></pre>"},{"location":"migration/openai/Examples/#send-multimodal-requests-text-and-image","title":"Send multimodal requests (text and image)","text":"<p>The following sample shows how to send a prompt and an image to the Gemini API.</p> Python <p>Before trying this sample, follow the Python setup instructions in the Vertex AI quickstart using client libraries. For more information, see the Vertex AI Python API reference documentation.</p> <p>To authenticate to Vertex AI, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.</p> <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=\"google/gemini-2.0-flash-001\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Describe the following image:\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"gs://cloud-samples-data/generative-ai/image/scones.jpg\",\n                },\n            ],\n        }\n    ],\n)\n\nprint(response)\n</code></pre>"},{"location":"migration/openai/Examples/#call-a-self-deployed-model-with-the-chat-completions-api","title":"\ud83d\udcbb Call a self-deployed model with the Chat Completions API","text":"<p>Use these examples when you have deployed your own model (e.g., a fine-tuned Gemma model) on a Vertex AI Endpoint and want to interact with it using the OpenAI-compatible API.</p>"},{"location":"migration/openai/Examples/#send-non-streaming-requests_1","title":"Send non-streaming requests","text":"<p>The following sample shows you how to send non-streaming requests:</p> RESTPython <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\nhttps://aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/global/endpoints/${ENDPOINT}/chat/completions \\\n-d '{\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Write a story about a magic backpack.\"\n  }]\n}'\n</code></pre> <p>Before trying this sample, follow the Python setup instructions in the Vertex AI quickstart using client libraries. For more information, see the Vertex AI Python API reference documentation.</p> <p>To authenticate to Vertex AI, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.</p> <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n# model_id = \"gemma-2-9b-it\"\n# endpoint_id = \"YOUR_ENDPOINT_ID\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{endpoint_id}\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=model_id,\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n)\nprint(response)\n</code></pre>"},{"location":"migration/openai/Examples/#send-streaming-requests_1","title":"Send streaming requests","text":"<p>The following sample shows you how to send streaming requests to a self- deployed model by using the Chat Completions API:</p> RESTPython <pre><code>curl -X POST \\\n    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n    -H \"Content-Type: application/json\" \\\n  https://aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/global/endpoints/${ENDPOINT}/chat/completions \\\n  -d '{\n    \"stream\": true,\n    \"messages\": [{\n      \"role\": \"user\",\n      \"content\": \"Write a story about a magic backpack.\"\n    }]\n  }'\n</code></pre> <p>Before trying this sample, follow the Python setup instructions in the Vertex AI quickstart using client libraries. For more information, see the Vertex AI Python API reference documentation.</p> <p>To authenticate to Vertex AI, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.</p> <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n# model_id = \"gemma-2-9b-it\"\n# endpoint_id = \"YOUR_ENDPOINT_ID\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{endpoint_id}\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=model_id,\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n    stream=True,\n)\nfor chunk in response:\n    print(chunk)\n</code></pre>"},{"location":"migration/openai/Examples/#extra_body-examples","title":"\ud83d\udcbb <code>extra_body</code> examples","text":"<p>You can use either the SDK or the REST API to pass in <code>extra_body</code> for additional Google-specific parameters.</p>"},{"location":"migration/openai/Examples/#add-thought_tag_marker-rest-api","title":"Add <code>thought_tag_marker</code> (REST API)","text":"<p>This example shows the JSON structure for adding <code>thought_tag_marker</code> within <code>extra_body</code>.</p> <pre><code>{\n  ...,\n  \"extra_body\": {\n     \"google\": {\n       ...,\n       \"thought_tag_marker\": \"...\"\n     }\n   }\n}\n</code></pre>"},{"location":"migration/openai/Examples/#add-extra_body-using-the-sdk-python","title":"Add <code>extra_body</code> using the SDK (Python)","text":"<p>This example shows how to pass <code>extra_body</code> using the Python SDK.</p> <pre><code>client.chat.completions.create(\n  ...,\n  extra_body = {\n    'extra_body': { 'google': { ... } }\n  },\n)\n</code></pre>"},{"location":"migration/openai/Examples/#extra_content-examples-rest-api","title":"\ud83d\udcbb <code>extra_content</code> examples (REST API)","text":"<p>You can populate the <code>extra_content</code> field by using the REST API directly to include additional Google-specific content at various levels of your request.</p>"},{"location":"migration/openai/Examples/#extra_content-with-string-content","title":"<code>extra_content</code> with string <code>content</code>","text":"<pre><code>{\n  \"messages\": [\n    { \"role\": \"...\", \"content\": \"...\", \"extra_content\": { \"google\": { ... } } }\n  ]\n}\n</code></pre>"},{"location":"migration/openai/Examples/#per-message-extra_content","title":"Per-message <code>extra_content</code>","text":"<pre><code>{\n  \"messages\": [\n    {\n      \"role\": \"...\",\n      \"content\": [\n        { \"type\": \"...\", ..., \"extra_content\": { \"google\": { ... } } }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"migration/openai/Examples/#per-tool-call-extra_content","title":"Per-tool call <code>extra_content</code>","text":"<pre><code>{\n  \"messages\": [\n    {\n      \"role\": \"...\",\n      \"tool_calls\": [\n        {\n          ...,\n          \"extra_content\": { \"google\": { ... } }\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"migration/openai/Examples/#sample-curl-requests","title":"\ud83d\udcbb Sample <code>curl</code> requests","text":"<p>You can use these <code>curl</code> requests directly, rather than going through the SDK, for specific functionalities.</p>"},{"location":"migration/openai/Examples/#use-thinking_config-with-extra_body","title":"Use <code>thinking_config</code> with <code>extra_body</code>","text":"<pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\n  https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/endpoints/openapi/chat/completions \\\n  -d '{ \\\n    \"model\": \"google/gemini-2.5-flash-preview-04-17\", \\\n    \"messages\": [ \\\n      { \"role\": \"user\", \\\n      \"content\": [ \\\n        { \"type\": \"text\", \\\n          \"text\": \"Are there any primes number of the form n*ceil(log(n))\" \\\n        }] }], \\\n    \"extra_body\": { \\\n      \"google\": { \\\n          \"thinking_config\": { \\\n          \"include_thoughts\": true, \"thinking_budget\": 10000 \\\n        }, \\\n        \"thought_tag_marker\": \"think\" } }, \\\n    \"stream\": true }'\n</code></pre>"},{"location":"migration/openai/Examples/#send-multimodal-request-with-image-data-image_url","title":"Send multimodal request with image data (<code>image_url</code>)","text":"<p>The Chat Completions API supports multimodal input. This example shows how to use <code>image_url</code> to pass in image data.</p> <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\n  https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/us-central1/endpoints/openapi/chat/completions \\\n  -d '{ \\\n    \"model\": \"google/gemini-2.0-flash-001\", \\\n    \"messages\": [{ \"role\": \"user\", \"content\": [ \\\n      { \"type\": \"text\", \"text\": \"Describe this image\" }, \\\n      { \"type\": \"image_url\", \"image_url\": \"gs://cloud-samples-data/generative-ai/image/scones.jpg\" }] }] }'\n</code></pre>"},{"location":"migration/openai/Examples/#send-multimodal-request-with-audio-data-input_audio","title":"Send multimodal request with audio data (<code>input_audio</code>)","text":"<p>This example shows how to use <code>input_audio</code> to pass in audio data.</p> <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\n  https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/us-central1/endpoints/openapi/chat/completions \\\n  -d '{ \\\n    \"model\": \"google/gemini-2.0-flash-001\", \\\n    \"messages\": [ \\\n      { \"role\": \"user\", \\\n        \"content\": [ \\\n          { \"type\": \"text\", \"text\": \"Describe this: \" }, \\\n          { \"type\": \"input_audio\", \"input_audio\": { \\\n            \"format\": \"audio/mp3\", \\\n            \"data\": \"gs://cloud-samples-data/generative-ai/audio/pixel.mp3\" } }] }] }'\n</code></pre>"},{"location":"migration/openai/Examples/#structured-output","title":"\ud83d\udcbb Structured output","text":"<p>You can use the <code>response_format</code> parameter to get structured output from the model.</p>"},{"location":"migration/openai/Examples/#example-using-sdk-python","title":"Example using SDK (Python)","text":"<pre><code>from pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"google/gemini-2.5-flash-preview-04-17\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n    ],\n    response_format=CalendarEvent,\n)\n\nprint(completion.choices[0].message.parsed)\n</code></pre>"},{"location":"migration/openai/Examples/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>See examples of calling the Inference API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Function Calling API with OpenAI-compatible syntax.</li> <li>Learn more about the Gemini API.</li> <li>Learn more about migrating from Azure OpenAI to the Gemini API.</li> </ul>"},{"location":"migration/openai/Overview/","title":"Using OpenAI libraries with Vertex AI","text":"<p>Preview</p> <p>This product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the Service Specific Terms. Pre-GA products and features are available \"as is\" and might have limited support. For more information, see the launch stage descriptions.</p> <p>To see an example of using the Chat Completions API, run the \"Call Gemini with the OpenAI Library\" Jupyter notebook in one of the following environments:</p> <p>Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench user-managed notebooks | View on GitHub</p> <p>The Chat Completions API works as an OpenAI-compatible endpoint, designed to make it easier to interface with Gemini on Vertex AI by using the OpenAI libraries for Python and REST.</p>"},{"location":"migration/openai/Overview/#choosing-your-sdk-openai-libraries-vs-google-gen-ai-sdk","title":"\ud83d\udcda Choosing your SDK: OpenAI Libraries vs. Google Gen AI SDK","text":"<p>The Chat Completions API for Vertex AI allows you to interact with Gemini models using existing OpenAI SDKs. This approach is beneficial if you have an established codebase using OpenAI's libraries. However, for new projects or to fully leverage all Gemini-specific features, the native Google Generative AI SDK is recommended.</p> <p>Here's a comparison to help you decide:</p> Feature/Aspect OpenAI Libraries with Vertex AI Chat Completions API Google Gen AI SDK Primary Use Case Migrating existing OpenAI applications to Vertex AI with minimal code changes. Comparing OpenAI models with Vertex AI models. New application development on Vertex AI. Access to the full suite of Gemini features and optimizations. Ease of Adoption High for existing OpenAI users. Moderate; requires learning the Google Gen AI SDK. Feature Access Supports common OpenAI parameters and some Gemini-specific features via <code>extra_body</code> or <code>extra_content</code>. Full access to all current and future Gemini features and parameters. Recommendation Suitable for quick migration, comparison, or if deeply invested in OpenAI tooling. Recommended for new projects and for users wanting to maximize Gemini's capabilities. <p>If you're already using the OpenAI libraries, using this Chat Completions API provides a low-effort way to switch between calling OpenAI models and Vertex AI hosted models to compare output, cost, and scalability, without significant code changes.</p> <p>If you aren't already using the OpenAI libraries, or for new development, we recommend that you use the Google Gen AI SDK.</p>"},{"location":"migration/openai/Overview/#supported-models","title":"\ud83d\udd17 Supported models","text":"<p>The Chat Completions API supports both Gemini models and select self-deployed models from Model Garden.</p>"},{"location":"migration/openai/Overview/#gemini-models","title":"Gemini models","text":"<p>The following models provide support for the Chat Completions API:</p> <ul> <li>Gemini 2.5 Pro (Preview)</li> <li>Gemini 2.5 Flash (Preview)</li> <li>Gemini 2.0 Flash</li> <li>Gemini 2.0 Flash-Lite</li> </ul>"},{"location":"migration/openai/Overview/#self-deployed-models-from-model-garden","title":"Self-deployed models from Model Garden","text":"<p>The Hugging Face Text Generation Interface (HF TGI) and Vertex AI Model Garden prebuilt vLLM containers support the Chat Completions API. However, not every model deployed to these containers supports the Chat Completions API. The following table includes the most popular supported models by container:</p> HF TGI Models vLLM Models * <code>gemma-2-9b-it</code>  * <code>gemma-2-27b-it</code>  * <code>Meta-Llama-3.1-8B-Instruct</code>  * <code>Meta-Llama-3-8B-Instruct</code>  * <code>Mistral-7B-Instruct-v0.3</code>  * <code>Mistral-Nemo-Instruct-2407</code> * Gemma  * Llama 2  * Llama 3  * Mistral-7B  * Mistral Nemo"},{"location":"migration/openai/Overview/#supported-parameters","title":"\ud83d\udd17 Supported parameters","text":"<p>For Google models, the Chat Completions API supports the following OpenAI parameters. For a description of each parameter, see OpenAI's documentation on Creating chat completions. Parameter support for third-party models varies by model. To see which parameters are supported, consult the model's documentation.</p> Parameter Description <code>messages</code> * <code>System message</code>  * <code>User message</code>: The <code>text</code> and <code>image_url</code> types are supported. The <code>image_url</code> type supports images stored a Cloud Storage URI or a base64 encoding in the form <code>\"data:&lt;MIME-TYPE&gt;;base64,&lt;BASE64-ENCODED-BYTES&gt;\"</code>. To learn how to create a Cloud Storage bucket and upload a file to it, see Discover object storage. The <code>detail</code> option is not supported.  * <code>Assistant message</code>  * <code>Tool message</code>  * <code>Function message</code>: This field is deprecated, but supported for backwards compatibility. <code>model</code> <code>max_completion_tokens</code> Alias for <code>max_tokens</code>. <code>max_tokens</code> <code>n</code> <code>frequency_penalty</code> <code>presence_penalty</code> <code>reasoning_effort</code> Configures how much time and how many tokens are used on a response.  * <code>low</code>: 1024  * <code>medium</code>: 8192  * <code>high</code>: 24576  Note: As no thoughts are included in the response when using <code>reasoning_effort</code>, only one of <code>reasoning_effort</code> or <code>extra_body.google.thinking_config</code> may be specified. <code>response_format</code> * <code>json_object</code>: Interpreted as passing \"application/json\" to the Gemini API.  * <code>json_schema</code>: Fully recursive schemas are not supported. <code>additional_properties</code> is supported.  * <code>text</code>: Interpreted as passing \"text/plain\" to the Gemini API.  * Any other MIME type is passed as is to the model, such as passing \"application/json\" directly. <code>seed</code> Corresponds to <code>GenerationConfig.seed</code>. <code>stop</code> <code>stream</code> <code>temperature</code> <code>top_p</code> <code>tools</code> * <code>type</code>  * <code>function</code>    * <code>name</code>    * <code>description</code>    * <code>parameters</code>: Specify parameters by using the OpenAPI specification. This differs from the OpenAI parameters field, which is described as a JSON Schema object. To learn about keyword differences between OpenAPI and JSON Schema, see the OpenAPI guide. <code>tool_choice</code> * <code>none</code>  * <code>auto</code>  * <code>required</code>: Corresponds to the mode <code>ANY</code> in the <code>FunctionCallingConfig</code>.  * <code>validated</code>: Corresponds to the mode <code>VALIDATED</code> in the <code>FunctionCallingConfig</code>. This is Google-specific. <code>web_search_options</code> Corresponds to the <code>GoogleSearch</code> tool. No sub-options are supported. <code>function_call</code> This field is deprecated, but supported for backwards compatibility. <code>functions</code> This field is deprecated, but supported for backwards compatibility. <p>If you pass any unsupported parameter, it is ignored.</p>"},{"location":"migration/openai/Overview/#multimodal-input-parameters","title":"Multimodal input parameters","text":"<p>The Chat Completions API supports select multimodal inputs.</p> Parameter Description <code>input_audio</code> * <code>data:</code> Any URI or valid blob format. We support all blob types, including image, audio, and video. Anything supported by <code>GenerateContent</code> is supported (HTTP, Cloud Storage, etc.).  * <code>format:</code> OpenAI supports both <code>wav</code> (audio/wav) and <code>mp3</code> (audio/mp3). Using Gemini, all valid MIME types are supported. <code>image_url</code> * <code>data:</code> Like <code>input_audio</code>, any URI or valid blob format is supported. Note: <code>image_url</code> as a URL defaults to the <code>image/*</code> MIME type, while <code>image_url</code> as blob data can be used for any multimodal input.  * <code>detail:</code> Similar to media resolution, this determines the maximum tokens per image for the request. Note: While OpenAI's field is per-image, Gemini enforces the same detail across the request, and passing multiple detail types in one request will throw an error. <p>In general, the <code>data</code> parameter can be a URI or a combination of MIME type and base64 encoded bytes in the form <code>\"data:&lt;MIME- TYPE&gt;;base64,&lt;BASE64-ENCODED-BYTES&gt;\"</code>. For a full list of MIME types, see <code>GenerateContent</code>. For more information on OpenAI's base64 encoding, see their documentation.</p> <p>For usage, see our multimodal input examples.</p>"},{"location":"migration/openai/Overview/#gemini-specific-parameters","title":"Gemini-specific parameters","text":"<p>There are several features supported by Gemini that are not available in OpenAI models. These features can still be passed in as parameters, but must be contained within an <code>extra_content</code> or <code>extra_body</code> or they will be ignored.</p>"},{"location":"migration/openai/Overview/#extra_body-features","title":"<code>extra_body</code> features","text":"Parameter Description <code>safety_settings</code> This corresponds to Gemini's <code>[SafetySetting](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/SafetySetting)</code>. <code>cached_content</code> This corresponds to Gemini's <code>[GenerateContentRequest](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/projects.locations.publishers.models/generateContent).cached_content</code>. <code>thinking_config</code> This corresponds to Gemini's <code>[GenerationConfig.ThinkingConfig](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/GenerationConfig#ThinkingConfig)</code>. <code>thought_tag_marker</code> Used to separate a model's thoughts from its responses for models with Thinking available.  * If not specified, thoughts are not enclosed in tags.  * If specified, thoughts are enclosed in these tags. Subsequent queries will strip these tags and correctly identify thoughts for context, preserving appropriate context for follow-up interactions."},{"location":"migration/openai/Overview/#extra_part-features","title":"<code>extra_part</code> features","text":"<p><code>extra_part</code> lets you specify additional settings at a per-<code>Part</code> level.</p> Parameter Description <code>extra_content</code> A field for adding Gemini-specific content that shouldn't be ignored. <code>thought</code> This will explicitly mark if a field is a thought (and take precedence over <code>thought_tag_marker</code>). This should be used to specify whether a tool call is part of a thought or not."},{"location":"migration/openai/Overview/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Learn more about authentication and credentialing with the OpenAI-compatible syntax.</li> <li>See examples of calling the Chat Completions API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Inference API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Function Calling API with OpenAI-compatible syntax.</li> <li>Learn more about the Gemini API.</li> <li>Learn more about migrating from Azure OpenAI to the Gemini API.</li> </ul>"}]}