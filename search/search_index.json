{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Google Cloud Documentation","text":""},{"location":"gemini/migrate_to_gemini2/","title":"Migrate your application to Gemini 2 with the Vertex AI Gemini API","text":"<p>This guide shows how to migrate generative AI applications from Gemini 1.x and PaLM models to Gemini 2 models.</p>"},{"location":"gemini/migrate_to_gemini2/#why-migrate-to-gemini-2","title":"\ud83d\udcda Why migrate to Gemini 2?","text":"<p>Gemini 2 delivers significant performance improvements over Gemini 1.x and PaLM models, along with new capabilities. Additionally, each model version has its own version support and availability timeline.</p> <p>Upgrading most generative AI applications to Gemini 2 shouldn't require significant reengineering of prompts or code. But some applications require prompt changes, and these changes are difficult to predict without running a prompt through Gemini 2 first. Therefore, Gemini 2 testing is recommended before migration.</p> <p>Significant code changes are only needed for certain breaking changes, or to use new Gemini 2 capabilities.</p>"},{"location":"gemini/migrate_to_gemini2/#which-gemini-2-model-should-i-migrate-to","title":"\ud83d\udcda Which Gemini 2 model should I migrate to?","text":"<p>As you choose a Gemini 2 model to migrate to, you'll want to consider the features that your application requires, as well as the cost of those features.</p> <p>For an overview of Gemini 2 model features, see Gemini 2. For an overview of all Google models, see Google models.</p> <p>For a comparison of Gemini 1.x and Gemini 2 models, see the following table.</p> Feature Gemini 1.5 Pro Gemini 1.5 Flash Gemini 2.0 Flash Gemini 2.0 Flash-Lite Gemini 2.5 Pro Gemini 2.5 Flash Input modalities text, documents, image, video, audio text, documents, image, video, audio text, documents, image, video, audio text, documents, image, video, audio text, documents, image, video, audio text, documents, image, video, audio Output modalities text text text text text text Context window, total token limit 2,097,152 1,048,576 1,048,576 1,048,576 1,048,576 1,048,576 Output context length 8,192 8,192 8,192 8,192 64,192 64,192 Grounding with Search Yes Yes Yes No Yes Yes Function calling Yes Yes Yes Yes Yes Yes Code execution No No Yes No Yes Yes Context caching Yes Yes Yes No Yes Yes Batch prediction Yes Yes Yes Yes Yes Yes Live API No No No No No No Latency Most capable in 1.5 family Fastest in 1.5 family Fast + good cost efficiency Fast + most cost efficient Slower than Flash, but good cost efficiency Fast + most cost efficient Fine-tuning Yes Yes Yes Yes Yes Yes Recommended SDK Vertex AI SDK Vertex AI SDK Gen AI SDK Gen AI SDK Gen AI SDK Gen AI SDK Pricing units Character Character Token Token Token Token"},{"location":"gemini/migrate_to_gemini2/#before-you-begin","title":"\u2699\ufe0f Before you begin","text":"<p>For a seamless Gemini 2 migration, we recommend that you address the following concerns before you begin the migration process.</p>"},{"location":"gemini/migrate_to_gemini2/#model-retirement-awareness","title":"Model retirement awareness","text":"<p>Note the model version support and availability timelines for older Gemini models, and make sure your migration is completed before the model you're using is retired.</p>"},{"location":"gemini/migrate_to_gemini2/#infosec-governance-and-regulatory-approvals","title":"InfoSec, governance, and regulatory approvals","text":"<p>Proactively request the approvals you need for Gemini 2 from your information security (InfoSec), risk, and compliance stakeholders. Make sure that you cover domain-specific risk and compliance constraints, especially in heavily regulated industries such as healthcare and financial services. Note that Gemini security controls differ among Gemini 2 models.</p>"},{"location":"gemini/migrate_to_gemini2/#location-availability","title":"Location availability","text":"<p>See the Generative AI on Google Cloud models and partner model availability documentation, and make sure your chosen Gemini 2 model is available in the regions where you need it, or consider switching to the global endpoint.</p>"},{"location":"gemini/migrate_to_gemini2/#modality-and-tokenization-based-pricing-differences","title":"Modality and tokenization-based pricing differences","text":"<p>Check Gemini 2 pricing for all the modalities (text, code, images, speech) in your application. For more information, see generative AI pricing page. Note that Gemini 2 text input and output is priced per token, while Gemini 1 text input and output is priced per character.</p>"},{"location":"gemini/migrate_to_gemini2/#provisioned-throughput","title":"Provisioned Throughput","text":"<p>If needed, purchase additional Provisioned Throughput for Gemini 2 or change existing Provisioned Throughput orders.</p>"},{"location":"gemini/migrate_to_gemini2/#supervised-fine-tuning","title":"Supervised fine-tuning","text":"<p>If your Gemini application uses supervised fine-tuning, submit a new tuning job with Gemini 2. We recommend that you start with the default tuning hyperparameters instead of reusing the hyperparameter values that you used with previous Gemini versions. The tuning service has been optimized for Gemini 2. Therefore, reusing previous hyperparameter values might not yield the best results.</p>"},{"location":"gemini/migrate_to_gemini2/#regression-testing","title":"Regression testing","text":"<p>There are three main types of regression tests involved when upgrading to Gemini 2 models: *   Code regression tests: Regression testing from a software engineering and DevOps perspective. This type of regression test is always required. *   Model performance regression tests: Regression testing from a data science or machine learning perspective. This means ensuring that the new Gemini 2 model provides outputs that are at least as high-quality as outputs from the current production model. Model performance regression tests are just model evaluations done as part of a change to a system or to the underlying model. Model performance regression testing further breaks down into:     *   Offline model performance testing: Assessing the quality of model outputs in a dedicated experimentation environment based on various model output quality metrics.     *   Online model performance testing: Assessing the quality of model outputs in a live online deployment based on implicit or explicit user feedback. *   Load testing: Assessing how the application handles high volumes of inference requests. This type of regression test is required for applications that require Provisioned Throughput.</p>"},{"location":"gemini/migrate_to_gemini2/#migration-process","title":"\u2699\ufe0f Migration process","text":"<p>The following diagram shows the high-level workflow for migrating your application to Gemini 2.</p> <pre><code>flowchart TD\n    subgraph Preparation\n        A[1. Document evaluation requirements]\n    end\n    subgraph Implementation\n        B[2. Upgrade &amp; test code]\n    end\n    subgraph Evaluation &amp; Tuning\n        C[3. Perform offline evaluation] --&gt; D{Performance drop?};\n        D -- Yes --&gt; E[4. Assess results &amp; tune prompts];\n        E --&gt; C;\n        D -- No --&gt; F[5. Perform load testing];\n    end\n    subgraph Deployment\n        F --&gt; G[6. Perform online evaluation] --&gt; H[7. Deploy to production];\n    end\n\n    A --&gt; B --&gt; C</code></pre>"},{"location":"gemini/migrate_to_gemini2/#step-1-document-model-evaluation-and-testing-requirements","title":"Step 1: Document model evaluation and testing requirements","text":"<ol> <li>Prepare to repeat any relevant evaluations from when you originally built your application, along with any relevant evaluations you have done since then.</li> <li>If you feel your existing evaluations don't appropriately cover or measure the breadth of tasks that your application performs, you should design and prepare additional evaluations.</li> <li>If your application involves RAG, tool use, complex agentic workflows, or prompt chains, make sure that your existing evaluation data allows for assessing each component independently. If not, gather input-output examples for each component.</li> <li>If your application is especially high-impact, or if it's part of a larger user-facing real-time system, you should include online evaluation.</li> </ol>"},{"location":"gemini/migrate_to_gemini2/#step-2-upgrade-and-test-your-code","title":"Step 2: Upgrade and test your code","text":"<p>If your Gemini 1.x application uses the Vertex AI SDK, consider upgrading to the Gen AI SDK. New Gemini 2 capabilities are only available in the Gen AI SDK. However, there is no need to switch to the Gen AI SDK if your application only requires capabilities that are available in the Vertex AI SDK.</p> <p>If you're new to the Gen AI SDK, see the Getting started with Google Generative AI using the Gen AI SDK notebook.</p>"},{"location":"gemini/migrate_to_gemini2/#gen-ai-sdk","title":"Gen AI SDK","text":"<p>We recommend that you migrate to the Gen AI SDK when upgrading to Gemini 2.0 to access all new features. The setup process is different from the Vertex AI SDK. For more information, visit Google Gen AI SDK.</p> <p>Install</p> <pre><code>pip install --upgrade google-genai\n</code></pre> <p>To learn more, see the SDK reference documentation.</p> <p>Set up</p> <p>Set environment variables to use the Gen AI SDK with Vertex AI:</p> <pre><code># Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexport GOOGLE_CLOUD_LOCATION=global\nexport GOOGLE_GENAI_USE_VERTEXAI=True\n\n\nfrom google import genai\nfrom google.genai.types import HttpOptions\n\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash-preview-05-20\",\n    contents=\"How does AI work?\",\n)\nprint(response.text)\n# Example response:\n# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n#\n# Here's a simplified overview:\n# ...\n</code></pre> <p>Replace <code>GOOGLE_CLOUD_PROJECT</code> with your Google Cloud project ID, and replace <code>GOOGLE_CLOUD_LOCATION</code> with the location of your Google Cloud project (for example, <code>us-central1</code>).</p>"},{"location":"gemini/migrate_to_gemini2/#vertex-ai-sdk","title":"Vertex AI SDK","text":"<p>If you reuse the Vertex AI SDK, the setup process is the same for the 1.0, 1.5, and 2.0 models. For more information, see Introduction to the Vertex AI SDK for Python.</p> <p>Note: The Vertex AI SDK does not support all features of Gemini 2.0. New features will only be added to the Gen AI SDK.</p> <p>Install</p> <pre><code>pip install --upgrade --quiet google-cloud-aiplatform\n</code></pre> <p>Set up</p> <p>The following is a short code sample that uses the Vertex AI SDK for Python:</p> <pre><code>import vertexai\nfrom vertexai.generative_models import GenerativeModel\n\n# TODO(developer): Update and un-comment below line\n# PROJECT_ID = \"your-project-id\"\nvertexai.init(project=PROJECT_ID, location=\"us-central1\")\n\nmodel = GenerativeModel(\"gemini-2.0-flash-001\")\n\nresponse = model.generate_content(\n    \"What's a good name for a flower shop that specializes in selling bouquets of dried flowers?\"\n)\n\nprint(response.text)\n# Example response:\n# **Emphasizing the Dried Aspect:**\n# * Everlasting Blooms\n# * Dried &amp; Delightful\n# * The Petal Preserve\n# ...\n</code></pre> <p>Replace <code>PROJECT_ID</code> with your Google Cloud project ID, and replace <code>LOCATION</code> with the location of your Google Cloud project (for example, <code>us-central1</code>). Then, change the model ID from <code>gemini-1.5-flash-002</code> to <code>gemini-2.0-flash</code>.</p>"},{"location":"gemini/migrate_to_gemini2/#change-your-gemini-calls","title":"Change your Gemini calls","text":"<p>Change your prediction code to use Gemini 2. At a minimum, this means changing the specific model endpoint name to a Gemini 2 model where you load your model. The exact code change will differ depending on how you originally implemented your application, and especially whether you used the Gen AI SDK or the Vertex AI SDK.</p> <p>After you make your code changes, perform code regression testing and other software tests on your code to make sure that it runs. This test is only meant to assess whether the code functions correctly. It's not meant to assess the quality of model responses.</p>"},{"location":"gemini/migrate_to_gemini2/#address-breaking-code-changes","title":"Address breaking code changes","text":"<p>Focus only on code changes in this step. You may need to make other changes, but wait until you start your evaluation, and then consider the following adjustment based on evaluation results:</p> <ul> <li>Dynamic retrieval: Switch to using Grounding with Google Search. This feature requires using the Gen AI SDK; it's not supported by the Vertex AI SDK. If you're switching from dynamic retrieval, you might need to experiment with system instructions to control when Google Search is used (for example, <code>\"Only generate queries for the Google Search tool if the user asks about sports. Don't generate queries for any other topic.\"</code>), but wait until you evaluate before making prompt changes.</li> <li>Content filters: Note the default content filter settings, and change your code if it relies on a default that has changed.</li> <li><code>Top-K</code> token sampling parameter: Models after <code>gemini-1.0-pro-vision</code> don't support changing the <code>Top-K</code> parameter. If you used the <code>Top-K</code> parameter, adjust other token sampling parameters, such as <code>Top-P</code>, to achieve similar results.</li> </ul>"},{"location":"gemini/migrate_to_gemini2/#step-3-perform-offline-evaluation","title":"Step 3: Perform offline evaluation","text":"<p>Repeat the evaluation that you did when you originally developed and launched your application, any further offline evaluation you did after launching, and any additional evaluation you identified in step 1. If you then feel that your evaluation doesn't fully capture the breadth and depth of your application, do further evaluation.</p> <p>If you don't have an automated way to run your offline evaluations, consider using the Gen AI evaluation service.</p> <p>If your application uses fine-tuning, perform offline evaluation before retuning your model with Gemini 2. Gemini 2's improved output quality may mean that your application no longer requires a fine-tuned model.</p>"},{"location":"gemini/migrate_to_gemini2/#step-4-assess-evaluation-results-and-tune-the-gemini-2-prompts-and-hyperparameters","title":"Step 4: Assess evaluation results and tune the Gemini 2 prompts and hyperparameters","text":"<p>If your offline evaluation shows a drop in performance with Gemini 2, iterate on your application as follows until Gemini performance matches the older model:</p> <ul> <li>Iteratively engineer your prompts to improve performance (\"Hill Climbing\"). If you are new to hill climbing, see the Vertex Gemini hill climbing online training. The Vertex AI prompt optimizer (example notebook) can help as well.</li> <li>If your application already relies on fine-tuning, try fine-tuning Gemini 2.</li> <li>If your application is impacted by Dynamic Retrieval and Top-K breaking changes, experiment with changing your prompt and token sampling parameters.</li> </ul>"},{"location":"gemini/migrate_to_gemini2/#step-5-perform-load-testing","title":"Step 5: Perform load testing","text":"<p>If your application requires a certain minimum throughput, perform load testing to make sure the Gemini 2 version of your application meets your throughput requirements.</p> <p>Load testing should happen before online evaluation, because online evaluation requires exposing Gemini 2 to production traffic. Use your existing load testing instrumentation to perform this step.</p> <p>If your application already meets throughput requirements, consider using Provisioned Throughput. You'll need additional short-term Provisioned Throughput to cover load testing while your existing Provisioned Throughput order continues to serve production traffic.</p>"},{"location":"gemini/migrate_to_gemini2/#step-6-perform-online-evaluation","title":"Step 6: Perform online evaluation","text":"<p>Only proceed to online evaluation if your offline evaluation shows adequate Gemini output quality and your application requires online evaluation.</p> <p>Online evaluation is a special case of online testing. Try to use your organization's existing tools and procedures for online evaluation. For example:</p> <ul> <li>If your organization regularly conducts A/B tests, perform an A/B test that evaluates the current implementation of your application compared to the Gemini 2 version.</li> <li>If your organization regularly conducts canary deployments, be sure to do so with Gemini 2 and measure differences in user behavior.</li> </ul> <p>Online evaluation can also be done by building new feedback and measurement capabilities into your application. Different feedback and measurement capabilities are appropriate for different applications. For example:</p> <ul> <li>Adding thumbs-up and thumbs-down buttons next to model outputs and comparing thumbs-up versus thumbs-down rates between an older model and Gemini 2.</li> <li>Presenting users with the older model and Gemini 2 outputs side-by-side and asking for users to pick their favorite.</li> <li>Tracking how often users override or manually adjust older model versus Gemini 2 outputs.</li> </ul> <p>These kinds of feedback mechanisms often require running a Gemini 2 version of your application in parallel to your existing version. This parallel deployment is sometimes called \"shadow mode\" or \"blue-green deployment\".</p> <p>If online evaluation results differ significantly from offline evaluation results, your offline evaluation is not capturing key aspects of the live environment or user experience. Use the online evaluation findings to devise a new offline evaluation to cover the gap the online evaluation exposed, and then return to step 3.</p> <p>If you use Provisioned Throughput, you may need to purchase additional short-term Provisioned Throughput to continue to meet your throughput requirements for users subject to online evaluation.</p>"},{"location":"gemini/migrate_to_gemini2/#step-7-deploy-to-production","title":"Step 7: Deploy to production","text":"<p>Once your evaluation shows that Gemini 2 meets or exceeds performance of an older model, turn down the existing version of your application in favor of the Gemini 2 version. Follow your organization's existing procedures for production rollout.</p> <p>If you're using Provisioned Throughput, change your Provisioned Throughput order to your chosen Gemini 2 model. If you're rolling out your application incrementally, use short-term Provisioned Throughput to meet throughput requirements for two different Gemini models.</p>"},{"location":"gemini/migrate_to_gemini2/#improving-model-performance","title":"\ud83d\udcda Improving model performance","text":"<p>As you complete your migration, use the following tips to maximize Gemini 2 model performance:</p> <ul> <li>Inspect your system instructions, prompts, and few-shot learning examples for any inconsistencies, contradictions, or irrelevant instructions and examples.</li> <li>Test a more powerful model. For example, if you evaluated Gemini 2.0 Flash-Lite, try Gemini 2.0 Flash.</li> <li>Examine any automated evaluation results to make sure they match human judgment, especially results that use a judge model. Make sure your judge model instructions don't contain inconsistencies or ambiguities.</li> <li>One way to improve judge model instructions is to test the instructions with multiple humans in isolation and see if their judgments are consistent. If humans interpret the instructions differently and render different judgments, your judge model instructions are ambiguous.</li> <li>Fine-tune the Gemini 2 model.</li> <li>Examine evaluation outputs to look for patterns that show specific kinds of failures. Grouping together failures into different models, kinds, or categories gives you more targeted evaluation data, which makes it easier to adjust prompts to address these errors.</li> <li>Make sure you are independently evaluating different generative AI components.</li> <li>Experiment with adjusting token sampling parameters.</li> </ul>"},{"location":"gemini/migrate_to_gemini2/#getting-help","title":"\ud83d\udd17 Getting help","text":"<p>If you need help, Google Cloud offers support packages to meet your needs, such as 24/7 coverage, phone support, and access to a technical support manager. For more information, see Google Cloud Support.</p>"},{"location":"gemini/migrate_to_gemini2/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Read the list of frequently asked questions.</li> <li>Migrate from the PaLM API to the Vertex AI Gemini API.</li> </ul>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/","title":"Migrate from the Gemini Developer API to the Gemini API in Vertex AI","text":"<p>If you are new to Gemini, using the quickstarts is the fastest way to get started.</p> <p>However, as your generative AI solutions mature, you may need a platform for building and deploying generative AI applications and solutions end to end. Google Cloud provides a comprehensive ecosystem of tools to enable developers to harness the power of generative AI, from the initial stages of app development to app deployment, app hosting, and managing complex data at scale.</p> <p>Google Cloud's Vertex AI platform offers a suite of MLOps tools that streamline usage, deployment, and monitoring of AI models for efficiency and reliability. Additionally, integrations with databases, DevOps tools, logging, monitoring, and IAM provide a holistic approach to managing the entire generative AI lifecycle.</p>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#benefits-of-using-the-gemini-api-in-vertex-ai","title":"\ud83d\udcda Benefits of using the Gemini API in Vertex AI","text":"<p>Migrating to the Gemini API in Vertex AI allows you to leverage the broader Google Cloud ecosystem. Here are some common use cases and benefits:</p> <ul> <li>Productionize your apps and solutions: Products like Cloud Run functions and Cloud Run let you deploy apps with enterprise-grade scale, security, and privacy. Find more details about security and privacy in the Security, Privacy, and Cloud Compliance on Google Cloud guide.</li> <li>Utilize end-to-end MLOps capabilities: Use Vertex AI for a full suite of MLOps tools, from model tuning to vector similarity search and ML pipelines.</li> <li>Event-driven architecture: Trigger your LLM calls with event-driven architecture using Cloud Run functions or Cloud Run.</li> <li>Monitor application usage: Monitor your app's usage with Cloud Logging and BigQuery.</li> <li>Secure and scalable data storage: Store your data with enterprise-grade security at scale with services like BigQuery, Cloud Storage, and Cloud SQL.</li> <li>Retrieval-Augmented Generation (RAG): Perform RAG using data in the cloud with BigQuery or Cloud Storage.</li> <li>Create and schedule data pipelines: Schedule jobs using Cloud Scheduler.</li> <li>Apply LLMs to your cloud data: If you store data in Cloud Storage or BigQuery, you can prompt LLMs over that data to extract information, summarize, or ask questions.</li> <li>Data governance and residency: Leverage Google Cloud data governance/residency policies to manage your data lifecycle.</li> </ul>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#differences-between-the-gemini-developer-api-and-the-gemini-api-in-vertex-ai","title":"\ud83d\udcda Differences between the Gemini Developer API and the Gemini API in Vertex AI","text":"<p>The following table summarizes the main differences between the Gemini Developer API and the Vertex AI Gemini API to help you decide which option is right for your use case:</p> Features Gemini Developer API Vertex AI Gemini API Gemini models Gemini 2.0 Flash, Gemini 2.0 Flash-Lite Gemini 2.0 Flash, Gemini 2.0 Flash-Lite Sign up Google account Google Cloud account (with terms agreement and billing) Authentication API key Google Cloud service account User interface playground Google AI Studio Vertex AI Studio API &amp; SDK Server and mobile/web client SDKs: Server: Python, Node.js, Go, Dart, ABAP Mobile/Web client: Android (Kotlin/Java), Swift, Web, Flutter Server and mobile/web client SDKs: Server: Python, Node.js, Go, Java, ABAP Mobile/Web client (via Vertex AI in Firebase): Android (Kotlin/Java), Swift, Web, Flutter No-cost usage of API &amp; SDK Yes, where applicable $300 Google Cloud credit for new users Quota (requests per minute) Varies based on model and pricing plan (see detailed information) Varies based on model and region (see detailed information) Enterprise support No Customer-managed encryption keys (CMEK)Virtual Private Cloud (VPC)Data residencyAccess TransparencyScalable infrastructure for application hostingDatabases and data storage MLOps No Full MLOps on Vertex AI (for example, model evaluation, Model Monitoring, and Model Registry)"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#migrate-to-gemini-api-in-vertex-ai","title":"\u2699\ufe0f Migrate to Gemini API in Vertex AI","text":"<p>This section shows how to migrate from the Gemini Developer API to the Gemini API in Vertex AI.</p> <pre><code>flowchart TD\n    A[Start Migration to Vertex AI] --&gt; B[Review Migration Considerations];\n    B --&gt; C{Google Cloud Account Setup};\n    C -- Existing Account --&gt; D[Connect to Vertex AI via Google AI Studio];\n    C -- New Account --&gt; E[Create Google Cloud Account &amp; Connect to Vertex AI via Google AI Studio];\n    D --&gt; F[Access Vertex AI Console];\n    E --&gt; F;\n    F --&gt; G[Migrate Python Code to Vertex AI SDK];\n    G --&gt; H[Migrate Prompts from Google AI Studio to Vertex AI Studio];\n    H --&gt; I[Upload Training Data to Cloud Storage for Vertex AI];\n    I --&gt; J[Migration Steps Complete];</code></pre>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#considerations-when-migrating","title":"Considerations when migrating","text":"<p>Consider the following when migrating:</p> <ul> <li>You can use your existing Google Cloud project (the same one you used to generate your Gemini API key) or you can create a new Google Cloud project.</li> <li>Supported regions might differ between the Gemini Developer API and the Gemini API in Vertex AI. See the list of supported regions for generative AI on Google Cloud.</li> <li>Any models you created in Google AI Studio need to be retrained in Vertex AI.</li> </ul>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#start-using-vertex-ai-studio","title":"Start using Vertex AI Studio","text":"<p>The process you follow to migrate to Gemini API in Vertex AI is different, depending on if you already have a Google Cloud account or you are new to Google Cloud.</p> <p>Note: Google AI Studio and the Gemini Developer API are available only in specific regions and languages. If you aren't located in a supported region, you can't start using the Gemini API in Vertex AI.</p> <p>To learn how to migrate to the Gemini API in Vertex AI, click one of the following tabs, depending on your Google Cloud account status:</p>   ### Already use Google Cloud {.new-tab}  1.  Sign in to [Google AI Studio](https://aistudio.google.com/app/waitlist/97445851). 2.  At the bottom of the left navigation pane, click **Build with Vertex AI on Google Cloud**.      The **Try Vertex AI and Google Cloud for free** page opens. 3.  Click **Agree &amp; Continue**.      The **Get Started with Vertex AI studio** dialog appears. 4.  To enable the APIs required to run Vertex AI, click **Agree &amp; Continue**.      The Vertex AI console appears. To learn how to migrate your data from Google AI Studio, see [Migrate prompts to Vertex AI Studio](#migrate-prompts-to-vertex-ai-studio).  ### New to Google Cloud {.new-tab}  1.  Sign in to [Google AI Studio](https://aistudio.google.com/app/waitlist/97445851). 2.  At the bottom of the left navigation pane, click **Build with Vertex AI on Google Cloud**.      The **Create an account to get started with Google Cloud** page opens. 3.  Click **Agree &amp; Continue**.      The **Let's confirm your identity** page appears. 4.  Click **Start Free**.      The **Get Started with Vertex AI studio** dialog appears. 5.  To enable the APIs required to run Vertex AI, click **Agree &amp; Continue**. 6.  Optional: To learn how to migrate your data from Google AI Studio, see [Migrate prompts to Vertex AI Studio](#migrate-prompts-to-vertex-ai-studio) on this page."},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#python-migrate-to-the-gemini-api-in-vertex-ai","title":"\ud83d\udcbb Python: Migrate to the Gemini API in Vertex AI","text":"<p>The following sections show code snippets to help you migrate your Python code to use the Gemini API in Vertex AI.</p>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#vertex-ai-python-sdk-setup","title":"Vertex AI Python SDK Setup","text":"<p>On Vertex AI, you don't need an API key. Instead, Gemini on Vertex AI is managed using IAM access, which controls permission for a user, a group, or a service account to call the Gemini API through the Vertex AI SDK.</p> <p>While there are many ways to authenticate, the easiest method for authenticating in a development environment is to install the Google Cloud CLI then use your user credentials to sign in to the CLI.</p> <p>To make inference calls to Vertex AI, you must also make sure that your user or service account has the Vertex AI User role.</p>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#code-example-to-install-the-client","title":"Code example to install the client","text":"Gemini Developer API Gemini API in Vertex AI <pre><code># To install the Python SDK, use this CLI command:\n# pip install google-generativeai\n\nimport google.generativeai as genai\nfrom google.generativeai import GenerativeModel\n\nAPI_KEY=\"API_KEY\"\ngenai.configure(api_key=API_KEY)\n</code></pre> <pre><code># To install the Python SDK, use this CLI command:\n# pip install google-genai\n\nfrom google import genai\n\nPROJECT_ID = \"PROJECT_ID\"\nLOCATION = \"LOCATION\"  # e.g. us-central1\nclient = genai.Client(project=PROJECT_ID, location=LOCATION, vertexai=True)\n</code></pre>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#code-example-to-generate-text-from-text-prompt","title":"Code example to generate text from text prompt","text":"Gemini Developer API Gemini API in Vertex AI <pre><code>model = GenerativeModel(\"gemini-2.0-flash\")\n\nresponse = model.generate_content(\"The opposite of hot is\")\nprint(response.text) #  The opposite of hot is cold.\n```                                                                                                                                                  | ```python\nfrom google import genai\nfrom google.genai.types import HttpOptions\n\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash-preview-05-20\",\n    contents=\"How does AI work?\",\n)\nprint(response.text)\n# Example response:\n# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n#\n# Here's a simplified overview:\n# ...\n</code></pre>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#code-example-to-generate-text-from-text-and-image","title":"Code example to generate text from text and image","text":"Gemini Developer API Gemini API in Vertex AI <pre><code>import PIL.Image\n\nmultimodal_model = GenerativeModel(\"gemini-2.0-flash\")\n\nimage = PIL.Image.open(\"image.jpg\")\n\nresponse = multimodal_model.generate_content([\"What is this picture?\", image])\nprint(response.text) # A cat is shown in this picture.\n```                                                                                                                                                  | ```python\nfrom google import genai\nfrom google.genai.types import HttpOptions, Part\n\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash-preview-05-20\",\n    contents=[\n        \"What is shown in this image?\",\n        Part.from_uri(\n            file_uri=\"gs://cloud-samples-data/generative-ai/image/scones.jpg\",\n            mime_type=\"image/jpeg\",\n        ),\n    ],\n)\nprint(response.text)\n# Example response:\n# The image shows a flat lay of blueberry scones arranged on parchment paper. There are ...\n</code></pre>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#code-example-to-generate-multi-turn-chat","title":"Code example to generate multi-turn chat","text":"Gemini Developer API Gemini API in Vertex AI <pre><code>model = GenerativeModel(\"gemini-2.0-flash\")\n\nchat = model.start_chat()\n\nprint(chat.send_message(\"How are you?\").text)\nprint(chat.send_message(\"What can you do?\").text)\n```                                                                                                                                                  | ```python\nfrom google import genai\nfrom google.genai.types import HttpOptions, ModelContent, Part, UserContent\n\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nchat_session = client.chats.create(\n    model=\"gemini-2.5-flash-preview-05-20\",\n    history=[\n        UserContent(parts=[Part(text=\"Hello\")]),\n        ModelContent(\n            parts=[Part(text=\"Great to meet you. What would you like to know?\")],\n        ),\n    ],\n)\nresponse = chat_session.send_message(\"Tell me a story.\")\nprint(response.text)\n# Example response:\n# Okay, here's a story for you:\n# ...\n</code></pre> <pre><code>                                                                                                                                             |\n</code></pre>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#migrate-prompts-to-vertex-ai-studio","title":"Migrate prompts to Vertex AI Studio","text":"<p>Your Google AI Studio prompt data is saved in a Google Drive folder. This section shows how to migrate your prompts to Vertex AI Studio.</p> <ol> <li>Open Google Drive.</li> <li>Navigate to the AI_Studio folder where the prompts are stored.     </li> <li> <p>Download your prompts from Google Drive to a local directory.</p> <p>Note: Prompts downloaded from Google Drive are in the text (<code>txt</code>) format. Before you upload them to Vertex AI Studio, convert them to JSON files. To do this, change the file extension from <code>.txt</code> to <code>.json</code>. 4.  Open Vertex AI Studio in the Google Cloud console. 5.  In the Vertex AI menu, click Prompt management. 6.  Click Import prompt. 7.  In the Prompt file field, click Browse and select a prompt from your local directory.</p> <p>To upload prompts in bulk, you must manually combine your prompts into a single JSON file. 8.  Click Upload.</p> <p>The prompts are uploaded to the My Prompts tab.</p> </li> </ol>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#upload-training-data-to-vertex-ai-studio","title":"Upload training data to Vertex AI Studio","text":"<p>To migrate your training data to Vertex AI, you need to upload your data to a Cloud Storage bucket. For more information, see Introduction to tuning.</p>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#delete-unused-api-keys","title":"\u2699\ufe0f Delete unused API Keys","text":"<p>If you no longer need to use your Gemini API key for the Gemini Developer API, then follow security best practices and delete it.</p> <p>To delete an API key:</p> <ol> <li>Open the Google Cloud API Credentials page.</li> <li>Find the API key that you want to delete and click the Actions icon.</li> <li>Select Delete API key.</li> <li>In the Delete credential modal, select Delete.</li> </ol> <p>Deleting an API key takes a few minutes to propagate. After propagation completes, any traffic using the deleted API key is rejected.</p> <p>Important: If you delete a key that's still used in production and need to recover it, see <code>gcloud beta services api-keys undelete</code>.</p>"},{"location":"migration/migrate_from_the_gemini_developer_API_to_the_Gemini_API_in_vertex_AI/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Try a quickstart tutorial using Vertex AI Studio or the Vertex AI API.</li> </ul>"},{"location":"migration/openai/Authenticate/","title":"Authenticate","text":"<p>Preview</p> <p>This product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the Service Specific Terms. Pre-GA products and features are available \"as is\" and might have limited support. For more information, see the launch stage descriptions.</p> <p>To use the OpenAI Python libraries with Vertex AI, you need to authenticate your requests. This document outlines how to set up authentication using Google credentials and a Vertex AI endpoint.</p> <pre><code>flowchart TD\n    A[Start] --&gt; B[Complete \"Before you begin\" steps];\n    B --&gt; C{Choose Authentication Method};\n    C -- Programmatic Client Setup --&gt; D[Write Python code to get Google Credentials];\n    D --&gt; E[Configure OpenAI Client with Token &amp; Endpoint in code];\n    C -- Environment Variables --&gt; F[Set OPENAI_API_KEY &amp; OPENAI_BASE_URL env vars];\n    F --&gt; G[Initialize Default OpenAI Client in code];\n    E --&gt; H[Make API Call];\n    G --&gt; H;\n    H --&gt; I{Token Expired?};\n    I -- Yes --&gt; J[Use Refresh Credentials Mechanism];\n    J --&gt; H;\n    I -- No --&gt; K[Finish];</code></pre>"},{"location":"migration/openai/Authenticate/#before-you-begin","title":"\u2699\ufe0f Before you begin","text":"<ol> <li>Install the OpenAI SDK: <pre><code>pip install openai\n</code></pre></li> <li>For Programmatic Client Setup (Python):<ul> <li>Install the <code>google-auth</code> library:     <pre><code>pip install google-auth requests\n</code></pre></li> <li>Follow the Python setup instructions in the Vertex AI quickstart using client libraries.</li> <li>To authenticate to Vertex AI, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.</li> </ul> </li> <li>For Environment Variable Setup:<ul> <li>Install the Google Cloud CLI.</li> </ul> </li> <li>For Self-Deployed Models:<ul> <li>Certain models in Model Garden and supported Hugging Face models need to be deployed to a Vertex AI endpoint first before they can serve requests.</li> <li>When calling these self-deployed models, you need to specify the endpoint ID. To list your existing Vertex AI endpoints, use the <code>gcloud ai endpoints list</code> command.</li> </ul> </li> </ol>"},{"location":"migration/openai/Authenticate/#configure-authentication","title":"\u2699\ufe0f Configure Authentication","text":"<p>To authenticate your calls to the Chat Completions API using the OpenAI Python library with Google authentication and a Vertex AI endpoint, you can either modify your client setup programmatically or configure environment variables.</p> <p>This section explains how to set up authentication for: *   Gemini models: Use the <code>openapi</code> Vertex AI endpoint. *   Self-deployed Model Garden models: Use the ID of your deployed Vertex AI endpoint.</p>   ### Programmatic Client Setup {.new-tab}  **Use this method if:** *   You prefer to manage credentials explicitly within your application code. *   You need fine-grained control over the authentication flow. *   Your deployment environment makes it easier to inject credentials via code rather than environment variables.  To programmatically get Google credentials in Python, use the `google-auth` Python SDK.  <pre><code>import openai\n\nfrom google.auth import default\nimport google.auth.transport.requests\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n# Note: the credential lives for 1 hour by default (https://cloud.google.com/docs/authentication/token-types#at-lifetime); after expiration, it must be refreshed.\n\n##############################\n# Choose one of the following:\n##############################\n\n# If you are calling a Gemini model, set the ENDPOINT_ID variable to use openapi.\nENDPOINT_ID = \"openapi\"\n\n# If you are calling a self-deployed model from Model Garden, set the\n# ENDPOINT_ID variable and set the client's base URL to use your endpoint.\n# ENDPOINT_ID = \"YOUR_ENDPOINT_ID\"\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{ENDPOINT_ID}\",\n    api_key=credentials.token,\n)\n</code></pre>  ### Environment Variables {.new-tab}  **Use this method if:** *   You prefer to keep credentials and configuration outside your codebase. *   Your deployment environment (e.g., CI/CD, managed compute) readily supports setting environment variables. *   You want a simpler client initialization in your code.  The OpenAI library can read the `OPENAI_API_KEY` and `OPENAI_BASE_URL` environment variables to change the authentication and endpoint in their default client. Set the following variables:  <pre><code>export PROJECT_ID=PROJECT_ID\nexport LOCATION=LOCATION\nexport OPENAI_API_KEY=\"$(gcloud auth application-default print-access-token)\"\n</code></pre>  **To call a Gemini model**, set the `MODEL_ID` variable (which you'll use in your API call) and use the `openapi` endpoint in your `OPENAI_BASE_URL`: <pre><code>export MODEL_ID=MODEL_ID # e.g., google/gemini-1.0-pro\nexport OPENAI_BASE_URL=\"https://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/openapi\"\n</code></pre>  **To call a self-deployed model from Model Garden**, set the `ENDPOINT_ID` variable and use that in your `OPENAI_BASE_URL` instead: <pre><code>export ENDPOINT_ID=YOUR_ENDPOINT_ID\nexport OPENAI_BASE_URL=\"https://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/${ENDPOINT_ID}\"\n</code></pre>  Next, initialize the client in your Python code: <pre><code>client = openai.OpenAI()\n</code></pre> <p>Note on Access Tokens: The Chat Completions API uses OAuth to authenticate with a short-lived access token. By default, access tokens obtained through Google authentication last for 1 hour. You can extend the life of your access token or, more commonly, refresh your token periodically. See the next section for an example of how to refresh credentials.</p>"},{"location":"migration/openai/Authenticate/#refresh-your-credentials","title":"\u2699\ufe0f Refresh your credentials","text":"<p>The following example shows how to create an OpenAI client wrapper that refreshes your Google credentials automatically as needed.</p>"},{"location":"migration/openai/Authenticate/#python","title":"Python","text":"<pre><code>from typing import Any\n\nimport google.auth\nimport google.auth.transport.requests\nimport openai\n\n\nclass OpenAICredentialsRefresher:\n    def __init__(self, **kwargs: Any) -&gt; None:\n        # Set a placeholder key here\n        self.client = openai.OpenAI(**kwargs, api_key=\"PLACEHOLDER\")\n        self.creds, self.project = google.auth.default(\n            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n        )\n\n    def __getattr__(self, name: str) -&gt; Any:\n        if not self.creds.valid:\n            self.creds.refresh(google.auth.transport.requests.Request())\n\n            if not self.creds.valid:\n                raise RuntimeError(\"Unable to refresh auth\")\n\n            self.client.api_key = self.creds.token\n        return getattr(self.client, name)\n\n\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Example usage:\nclient = OpenAICredentialsRefresher(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n)\n\n# Example API call\n# response = client.chat.completions.create(\n#     model=\"google/gemini-1.0-pro\", # Or your specific model_id for Gemini\n#     messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n# )\n# print(response)\n</code></pre>"},{"location":"migration/openai/Authenticate/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>See examples of calling the Chat Completions API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Inference API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Function Calling API with OpenAI-compatible syntax.</li> <li>Learn more about the Gemini API.</li> <li>Learn more about migrating from Azure OpenAI to the Gemini API.</li> </ul>"},{"location":"migration/openai/Examples/","title":"Examples","text":"<p>Preview</p> <p>This product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the Service Specific Terms. Pre-GA products and features are available \"as is\" and might have limited support. For more information, see the launch stage descriptions.</p> <p>To see an example of using the Chat Completions API, run the \"Call Gemini with the OpenAI Library\" Jupyter notebook.</p> <pre><code>flowchart TD\n    A[Start] --&gt; B{Choose Environment};\n    B -- Colab --&gt; C[Open in Colab];\n    B -- Colab Enterprise --&gt; D[Open in Colab Enterprise];\n    B -- Vertex AI Workbench --&gt; E[Open in Vertex AI Workbench];\n    C --&gt; F[Run Notebook];\n    D --&gt; F;\n    E --&gt; F;\n    F --&gt; G[Observe Chat Completions API Usage];\n    G --&gt; H[End];</code></pre> <p>Choose the environment that best suits your access and resource needs: *   Open in Colab: Open in Colab - For quick, browser-based execution with free resources. *   Open in Colab Enterprise: Open in Colab Enterprise - For a secure, enterprise-ready environment within Google Cloud. *   Open in Vertex AI Workbench: Open in Vertex AI Workbench user-managed notebooks - For a managed JupyterLab environment integrated with Vertex AI services. *   View on GitHub: View on GitHub</p> <p>You can interact with the Chat Completions API using direct REST calls or the Python SDK. *   REST API: Offers language-agnostic access and fine-grained control over requests. Suitable for environments where a Python SDK is not available or desired. *   Python SDK: Provides a convenient, Pythonic interface, simplifying authentication and request formation.</p>"},{"location":"migration/openai/Examples/#call-gemini-models-with-the-chat-completions-api","title":"\ud83d\udcbb Call Gemini models with the Chat Completions API","text":"<p>Use this method to call Google's pre-trained Gemini models available through a standard endpoint.</p>"},{"location":"migration/openai/Examples/#send-non-streaming-requests","title":"Send non-streaming requests","text":"<p>The following samples show you how to send non-streaming requests:</p>   #### REST {.new-tab}  <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\nhttps://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/openapi/chat/completions \\\n-d '{\n  \"model\": \"google/${MODEL_ID}\",\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Write a story about a magic backpack.\"\n  }]\n}'\n</code></pre>   #### Python {.new-tab}  Before trying this sample, follow the Python setup instructions in the [Vertex AI quickstart using client libraries](/vertex-ai/docs/start/client-libraries). For more information, see the [Vertex AI Python API reference documentation](/python/docs/reference/aiplatform/latest).  To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](/docs/authentication/set-up-adc-local-dev-environment).  <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=\"google/gemini-2.0-flash-001\",\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n)\n\nprint(response)\n</code></pre>"},{"location":"migration/openai/Examples/#send-streaming-requests","title":"Send streaming requests","text":"<p>The following samples show you how to send streaming requests to a Gemini model:</p>   #### REST {.new-tab}  <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\nhttps://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/openapi/chat/completions \\\n-d '{\n  \"model\": \"google/${MODEL_ID}\",\n  \"stream\": true,\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Write a story about a magic backpack.\"\n  }]\n}'\n</code></pre>  #### Python {.new-tab}  Before trying this sample, follow the Python setup instructions in the [Vertex AI quickstart using client libraries](/vertex-ai/docs/start/client-libraries). For more information, see the [Vertex AI Python API reference documentation](/python/docs/reference/aiplatform/latest).  To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](/docs/authentication/set-up-adc-local-dev-environment).  <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=\"google/gemini-2.0-flash-001\",\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n    stream=True,\n)\nfor chunk in response:\n    print(chunk)\n</code></pre>"},{"location":"migration/openai/Examples/#send-multimodal-requests-with-image-input","title":"Send multimodal requests with image input","text":"<p>The following sample shows how to send a prompt that includes an image:</p>   #### Python {.new-tab}  Before trying this sample, follow the Python setup instructions in the [Vertex AI quickstart using client libraries](/vertex-ai/docs/start/client-libraries). For more information, see the [Vertex AI Python API reference documentation](/python/docs/reference/aiplatform/latest).  To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](/docs/authentication/set-up-adc-local-dev-environment).  <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/openapi\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=\"google/gemini-2.0-flash-001\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Describe the following image:\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"gs://cloud-samples-data/generative-ai/image/scones.jpg\",\n                },\n            ],\n        }\n    ],\n)\n\nprint(response)\n</code></pre>"},{"location":"migration/openai/Examples/#call-self-deployed-models-with-the-chat-completions-api","title":"\ud83d\udcbb Call self-deployed models with the Chat Completions API","text":"<p>If you have deployed your own models (e.g., fine-tuned versions or open-source models like Gemma) to a Vertex AI Endpoint, use this method to interact with them via the Chat Completions API syntax.</p>"},{"location":"migration/openai/Examples/#send-non-streaming-requests_1","title":"Send non-streaming requests","text":"<p>The following samples show you how to send non-streaming requests to a self-deployed model:</p>   #### REST {.new-tab}  <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\nhttps://aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/global/endpoints/${ENDPOINT}/chat/completions \\\n-d '{\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Write a story about a magic backpack.\"\n  }]\n}'\n</code></pre>  #### Python {.new-tab}  Before trying this sample, follow the Python setup instructions in the [Vertex AI quickstart using client libraries](/vertex-ai/docs/start/client-libraries). For more information, see the [Vertex AI Python API reference documentation](/python/docs/reference/aiplatform/latest).  To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](/docs/authentication/set-up-adc-local-dev-environment).  <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n# model_id = \"gemma-2-9b-it\"\n# endpoint_id = \"YOUR_ENDPOINT_ID\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{endpoint_id}\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=model_id,\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n)\nprint(response)\n</code></pre>"},{"location":"migration/openai/Examples/#send-streaming-requests_1","title":"Send streaming requests","text":"<p>The following samples show you how to send streaming requests to a self-deployed model:</p>   #### REST {.new-tab}  <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\nhttps://aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/global/endpoints/${ENDPOINT}/chat/completions \\\n-d '{\n  \"stream\": true,\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Write a story about a magic backpack.\"\n  }]\n}'\n</code></pre>  #### Python {.new-tab}  Before trying this sample, follow the Python setup instructions in the [Vertex AI quickstart using client libraries](/vertex-ai/docs/start/client-libraries). For more information, see the [Vertex AI Python API reference documentation](/python/docs/reference/aiplatform/latest).  To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](/docs/authentication/set-up-adc-local-dev-environment).  <pre><code>from google.auth import default\nimport google.auth.transport.requests\n\nimport openai\n\n# TODO(developer): Update and un-comment below lines\n# project_id = \"PROJECT_ID\"\n# location = \"us-central1\"\n# model_id = \"gemma-2-9b-it\"\n# endpoint_id = \"YOUR_ENDPOINT_ID\"\n\n# Programmatically get an access token\ncredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncredentials.refresh(google.auth.transport.requests.Request())\n\n# OpenAI Client\nclient = openai.OpenAI(\n    base_url=f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/endpoints/{endpoint_id}\",\n    api_key=credentials.token,\n)\n\nresponse = client.chat.completions.create(\n    model=model_id,\n    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n    stream=True,\n)\nfor chunk in response:\n    print(chunk)\n</code></pre>"},{"location":"migration/openai/Examples/#using-advanced-features","title":"\ud83d\udcda Using advanced features","text":"<p>This section covers how to use Google-specific extensions like <code>extra_body</code> and <code>extra_content</code>.</p>"},{"location":"migration/openai/Examples/#extra_body-examples","title":"<code>extra_body</code> examples","text":"<p>The <code>extra_body</code> parameter allows you to pass additional, provider-specific parameters that are not part of the standard OpenAI Chat Completions API spec. For Google models, this can be used for features like <code>thinking_config</code>.</p> <p>You can use either the SDK or the REST API to pass in <code>extra_body</code>.</p>"},{"location":"migration/openai/Examples/#add-thought_tag_marker-rest-api","title":"Add <code>thought_tag_marker</code> (REST API)","text":"<pre><code>{\n  ...,\n  \"extra_body\": {\n     \"google\": {\n       ...,\n       \"thought_tag_marker\": \"...\"\n     }\n   }\n}\n</code></pre>"},{"location":"migration/openai/Examples/#add-extra_body-using-the-sdk-python","title":"Add <code>extra_body</code> using the SDK (Python)","text":"<pre><code>client.chat.completions.create(\n  ...,\n  extra_body = {\n    'extra_body': { 'google': { ... } }\n  },\n)\n</code></pre>"},{"location":"migration/openai/Examples/#extra_content-examples","title":"<code>extra_content</code> examples","text":"<p>The <code>extra_content</code> parameter allows you to include additional, provider-specific information alongside messages or tool calls. This is useful for passing Google-specific data structures. You can populate this field by using the REST API directly.</p>"},{"location":"migration/openai/Examples/#extra_content-with-string-content","title":"<code>extra_content</code> with string <code>content</code>","text":"<pre><code>{\n  \"messages\": [\n    { \"role\": \"...\", \"content\": \"...\", \"extra_content\": { \"google\": { ... } } }\n  ]\n}\n</code></pre>"},{"location":"migration/openai/Examples/#per-message-extra_content","title":"Per-message <code>extra_content</code>","text":"<pre><code>{\n  \"messages\": [\n    {\n      \"role\": \"...\",\n      \"content\": [\n        { \"type\": \"...\", ..., \"extra_content\": { \"google\": { ... } } }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"migration/openai/Examples/#per-tool-call-extra_content","title":"Per-tool call <code>extra_content</code>","text":"<pre><code>{\n  \"messages\": [\n    {\n      \"role\": \"...\",\n      \"tool_calls\": [\n        {\n          ...,\n          \"extra_content\": { \"google\": { ... } }\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"migration/openai/Examples/#sample-curl-requests","title":"\ud83d\udcbb Sample <code>curl</code> requests","text":"<p>This section provides specific <code>curl</code> examples for common use cases, demonstrating direct REST API interaction.</p>"},{"location":"migration/openai/Examples/#use-thinking_config-with-extra_body","title":"Use <code>thinking_config</code> with <code>extra_body</code>","text":"<pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\n  https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/endpoints/openapi/chat/completions \\\n  -d '{ \\\n    \"model\": \"google/gemini-2.5-flash-preview-04-17\", \\\n    \"messages\": [ \\\n      { \"role\": \"user\", \\\n      \"content\": [ \\\n        { \"type\": \"text\", \\\n          \"text\": \"Are there any primes number of the form n*ceil(log(n))\" \\\n        }] }], \\\n    \"extra_body\": { \\\n      \"google\": { \\\n          \"thinking_config\": { \\\n          \"include_thoughts\": true, \"thinking_budget\": 10000 \\\n        }, \\\n        \"thought_tag_marker\": \"think\" } }, \\\n    \"stream\": true }'\n</code></pre>"},{"location":"migration/openai/Examples/#multimodal-requests","title":"Multimodal requests","text":"<p>The Chat Completions API supports a variety of multimodal input, including both audio and video.</p>"},{"location":"migration/openai/Examples/#use-image_url-to-pass-in-image-data","title":"Use <code>image_url</code> to pass in image data","text":"<pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\n  https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/us-central1/endpoints/openapi/chat/completions \\\n  -d '{ \\\n    \"model\": \"google/gemini-2.0-flash-001\", \\\n    \"messages\": [{ \"role\": \"user\", \"content\": [ \\\n      { \"type\": \"text\", \"text\": \"Describe this image\" }, \\\n      { \"type\": \"image_url\", \"image_url\": \"gs://cloud-samples-data/generative-ai/image/scones.jpg\" }] }] }'\n</code></pre>"},{"location":"migration/openai/Examples/#use-input_audio-to-pass-in-audio-data","title":"Use <code>input_audio</code> to pass in audio data","text":"<pre><code>curl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\n  https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/us-central1/endpoints/openapi/chat/completions \\\n  -d '{ \\\n    \"model\": \"google/gemini-2.0-flash-001\", \\\n    \"messages\": [ \\\n      { \"role\": \"user\", \\\n        \"content\": [ \\\n          { \"type\": \"text\", \"text\": \"Describe this: \" }, \\\n          { \"type\": \"input_audio\", \"input_audio\": { \\\n            \"format\": \"audio/mp3\", \\\n            \"data\": \"gs://cloud-samples-data/generative-ai/audio/pixel.mp3\" } }] }] }'\n</code></pre>"},{"location":"migration/openai/Examples/#structured-output","title":"\ud83d\udcbb Structured output","text":"<p>To ensure the model's output conforms to a specific schema (e.g., JSON), you can use the <code>response_format</code> parameter. This is particularly useful when you need predictable, parsable output for downstream processing.</p>"},{"location":"migration/openai/Examples/#example-using-sdk","title":"Example using SDK","text":"<pre><code>from pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"google/gemini-2.5-flash-preview-04-17\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n    ],\n    response_format=CalendarEvent,\n)\n\nprint(completion.choices[0].message.parsed)\n</code></pre>"},{"location":"migration/openai/Examples/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>See examples of calling the Inference API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Function Calling API with OpenAI-compatible syntax.</li> <li>Learn more about the Gemini API.</li> <li>Learn more about migrating from Azure OpenAI to the Gemini API.</li> </ul>"},{"location":"migration/openai/Overview/","title":"Using OpenAI libraries with Vertex AI","text":"<p>Preview</p> <p>This product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the Service Specific Terms. Pre-GA products and features are available \"as is\" and might have limited support. For more information, see the launch stage descriptions.</p>"},{"location":"migration/openai/Overview/#overview","title":"\ud83d\udcda Overview","text":"<p>The Vertex AI Chat Completions API provides an OpenAI-compatible endpoint for interacting with Google's Gemini models and select models from Model Garden. This API is designed to make it easier for developers already familiar with the OpenAI libraries (Python and REST) to leverage Vertex AI's capabilities.</p> <p>By using this API, you can:</p> <ul> <li>Switch between calling OpenAI models and Vertex AI hosted models with minimal code changes.</li> <li>Compare output, cost, and scalability between different model providers.</li> <li>Integrate Gemini's advanced features into existing OpenAI-based applications.</li> </ul> <p>If you are new to generative AI on Google Cloud or not already using OpenAI libraries, we generally recommend using the Google Gen AI SDK for a more integrated experience with Google's ecosystem.</p>"},{"location":"migration/openai/Overview/#when-to-use-the-chat-completions-api","title":"\u2699\ufe0f When to use the Chat Completions API","text":"<p>Choosing between the OpenAI-compatible Chat Completions API and the native Google Gen AI SDK depends on your existing setup and specific needs.</p>   ### OpenAI-compatible API (Chat Completions) {.new-tab}  **Use this API if:**  *   You have existing applications built using OpenAI's libraries (Python client or REST APIs). *   You want a quick way to test and migrate to Vertex AI models with minimal code changes. *   You need to compare the performance, cost, or features of Gemini models against OpenAI models within your current workflow. *   Your primary goal is to leverage a familiar API structure.  **Pros:** *   Low friction for migration from OpenAI. *   Familiar API structure for OpenAI users. *   Supports both Gemini and select self-deployed models.  **Cons:** *   May not expose all of Gemini's newest or most unique features as directly as the native SDK. *   Gemini-specific parameters require special handling (e.g., `extra_body`, `extra_part`).  ### Google Gen AI SDK {.new-tab}  **Use this SDK if:**  *   You are starting a new project on Google Cloud. *   You want the most direct and idiomatic way to access all features of Gemini and other Vertex AI generative AI models. *   You prefer an SDK tightly integrated with the Google Cloud ecosystem (e.g., authentication, other Google Cloud services). *   You are not heavily invested in the OpenAI API structure.  **Pros:** *   Optimized for Google Cloud and Gemini models. *   Provides access to the full range of model capabilities and features directly. *   Often simpler to use for features unique to Google's models.  **Cons:** *   Requires learning a new SDK if you are coming from an OpenAI background."},{"location":"migration/openai/Overview/#supported-models","title":"\ud83d\udcda Supported models","text":"<p>The Chat Completions API supports both Gemini models and select self-deployed models from Model Garden.</p>"},{"location":"migration/openai/Overview/#gemini-models","title":"Gemini models","text":"<p>The following models provide support for the Chat Completions API:</p> <ul> <li>Gemini 2.5 Pro Preview</li> <li>Gemini 2.5 Flash Preview</li> <li>Gemini 2.0 Flash</li> <li>Gemini 2.0 Flash-Lite</li> </ul>"},{"location":"migration/openai/Overview/#self-deployed-models-from-model-garden","title":"Self-deployed models from Model Garden","text":"<p>The Hugging Face Text Generation Interface (HF TGI) and Vertex AI Model Garden prebuilt vLLM containers support the Chat Completions API. However, not every model deployed to these containers supports the Chat Completions API. The following table includes the most popular supported models by container:</p> HF TGI vLLM <ul><li><code>gemma-2-9b-it</code></li><li><code>gemma-2-27b-it</code></li><li><code>Meta-Llama-3.1-8B-Instruct</code></li><li><code>Meta-Llama-3-8B-Instruct</code></li><li><code>Mistral-7B-Instruct-v0.3</code></li><li><code>Mistral-Nemo-Instruct-2407</code></li></ul> <ul><li>Gemma</li><li>Llama 2</li><li>Llama 3</li><li>Mistral-7B</li><li>Mistral Nemo</li></ul>"},{"location":"migration/openai/Overview/#supported-parameters","title":"\ud83d\udd17 Supported parameters","text":"<p>For Google models, the Chat Completions API supports the following OpenAI parameters. For a description of each parameter, see OpenAI's documentation on Creating chat completions. Parameter support for third-party models varies by model. To see which parameters are supported, consult the model's documentation.</p> Parameter Description <code>messages</code> <ul><li><code>System message</code></li><li><code>User message</code>: The <code>text</code> and <code>image_url</code> types are supported. The <code>image_url</code> type supports images stored a Cloud Storage URI or a base64 encoding in the form <code>\"data:&lt;MIME-TYPE&gt;;base64,&lt;BASE64-ENCODED-BYTES&gt;\"</code>. To learn how to create a Cloud Storage bucket and upload a file to it, see Discover object storage. The <code>detail</code> option is not supported.</li><li><code>Assistant message</code></li><li><code>Tool message</code></li><li><code>Function message</code>: This field is deprecated, but supported for backwards compatibility.</li></ul> <code>model</code> <code>max_completion_tokens</code> Alias for <code>max_tokens</code>. <code>max_tokens</code> <code>n</code> <code>frequency_penalty</code> <code>presence_penalty</code> <code>reasoning_effort</code> Configures how much time and how many tokens are used on a response. <ul><li><code>low</code>: 1024</li><li><code>medium</code>: 8192</li><li><code>high</code>: 24576</li></ul> As no thoughts are included in the response, only one of <code>reasoning_effort</code> or <code>extra_body.google.thinking_config</code> may be specified. <code>response_format</code> <ul><li><code>json_object</code>: Interpreted as passing \"application/json\" to the Gemini API.</li><li><code>json_schema</code>.  Fully recursive schemas are not supported. <code>additional_properties</code> is supported.</li><li><code>text</code>: Interpreted as passing \"text/plain\" to the Gemini API.</li><li>Any other MIME type is passed as is to the model, such as passing \"application/json\" directly.</li></ul> <code>seed</code> Corresponds to <code>GenerationConfig.seed</code>. <code>stop</code> <code>stream</code> <code>temperature</code> <code>top_p</code> <code>tools</code> <ul><li><code>type</code></li><li><code>function</code><ul><li><code>name</code></li><li><code>description</code></li><li><code>parameters</code>: Specify parameters by using the OpenAPI specification. This differs from the OpenAI parameters field, which is described as a JSON Schema object. To learn about keyword differences between OpenAPI and JSON Schema, see the OpenAPI guide.</li></ul></li></ul> <code>tool_choice</code> <ul><li><code>none</code></li><li><code>auto</code></li><li><code>required</code>: Corresponds to the mode <code>ANY</code> in the <code>FunctionCallingConfig</code>.</li><li><code>validated</code>: Corresponds to the mode <code>VALIDATED</code> in the <code>FunctionCallingConfig</code>. This is Google-specific.</li></ul> <code>web_search_options</code> Corresponds to the <code>GoogleSearch</code> tool. No sub-options are supported. <code>function_call</code> This field is deprecated, but supported for backwards compatibility. <code>functions</code> This field is deprecated, but supported for backwards compatibility. <p>If you pass any unsupported parameter, it is ignored.</p>"},{"location":"migration/openai/Overview/#multimodal-input-parameters","title":"Multimodal input parameters","text":"<p>The Chat Completions API supports select multimodal inputs.</p> Parameter Description <code>input_audio</code> <ul><li><code>data:</code> Any URI or valid blob format. We support all blob types, including image, audio, and video. Anything supported by <code>GenerateContent</code> is supported (HTTP, Cloud Storage, etc.).</li><li><code>format:</code> OpenAI supports both <code>wav</code> (audio/wav) and <code>mp3</code> (audio/mp3). Using Gemini, all valid MIME types are supported.</li></ul> <code>image_url</code> <ul><li><code>data:</code> Like <code>input_audio</code>, any URI or valid blob format is supported.Note that <code>image_url</code> as a URL will default to the image/* MIME-type and <code>image_url</code> as blob data can be used as any multimodal input.</li><li><code>detail:</code> Similar to media resolution, this determines the maximum tokens per image for the request. Note that while OpenAI's field is per-image, Gemini enforces the same detail across the request, and passing multiple detail types in one request will throw an error.</li></ul> <p>In general, the <code>data</code> parameter can be a URI or a combination of MIME type and base64 encoded bytes in the form <code>\"data:&lt;MIME-TYPE&gt;;base64,&lt;BASE64-ENCODED-BYTES&gt;\"</code>. For a full list of MIME types, see <code>GenerateContent</code>. For more information on OpenAI's base64 encoding, see their documentation.</p> <p>For usage, see our multimodal input examples.</p>"},{"location":"migration/openai/Overview/#gemini-specific-parameters","title":"Gemini-specific parameters","text":"<p>There are several features supported by Gemini that are not available in standard OpenAI models. These Gemini-specific features are passed using the <code>extra_body</code> field at the request level, or the <code>extra_part</code> field within a message part. Parameters not placed in these designated fields will be ignored.</p>"},{"location":"migration/openai/Overview/#extra_body-features","title":"<code>extra_body</code> features","text":"<p>These parameters are passed within an <code>extra_body</code> JSON object at the top level of your request.</p> Parameter Description <code>safety_settings</code> This corresponds to Gemini's <code>[SafetySetting](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/SafetySetting)</code>. <code>cached_content</code> This corresponds to Gemini's <code>[GenerateContentRequest](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/projects.locations.publishers.models/generateContent).cached_content</code>. <code>thinking_config</code> This corresponds to Gemini's <code>[GenerationConfig.ThinkingConfig](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/GenerationConfig#ThinkingConfig)</code>. <code>thought_tag_marker</code> Used to separate a model's thoughts from its responses for models with Thinking available.If not specified, no tags will be returned around the model's thoughts. If present, subsequent queries will strip the thought tags and mark the thoughts appropriately for context. This helps preserve the appropriate context for subsequent queries."},{"location":"migration/openai/Overview/#extra_part-features","title":"<code>extra_part</code> features","text":"<p>These parameters are specified within an <code>extra_part</code> object inside a message <code>Part</code>. This allows you to apply settings at a per-<code>Part</code> level.</p> Parameter Description <code>extra_content</code> A field for adding Gemini-specific content that shouldn't be ignored. <code>thought</code> This will explicitly mark if a field is a thought (and take precedence over <code>thought_tag_marker</code>). This should be used to specify whether a tool call is part of a thought or not."},{"location":"migration/openai/Overview/#examples-and-tutorials","title":"\ud83d\udcbb Examples and Tutorials","text":"<p>To see an example of using the Chat Completions API, run the \"Call Gemini with the OpenAI Library\" Jupyter notebook in one of the following environments:</p> <p>Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench user-managed notebooks | View on GitHub</p>"},{"location":"migration/openai/Overview/#whats-next","title":"\ud83d\udd17 What's next","text":"<ul> <li>Learn more about authentication and credentialing with the OpenAI-compatible syntax.</li> <li>See examples of calling the Chat Completions API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Inference API with the OpenAI-compatible syntax.</li> <li>See examples of calling the Function Calling API with OpenAI-compatible syntax.</li> <li>Learn more about the Gemini API.</li> <li>Learn more about migrating from Azure OpenAI to the Gemini API.</li> </ul>"}]}